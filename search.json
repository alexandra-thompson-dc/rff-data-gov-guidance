[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Guidance for Researchers",
    "section": "",
    "text": "Home",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-outline",
    "href": "index.html#site-outline",
    "title": "Data Guidance for Researchers",
    "section": "Site Outline",
    "text": "Site Outline\n\nData Management\nSoftware Quality\nVersion Control",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/data-management/index.html",
    "href": "docs/data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "Foundations\nData management encompasses the methods used to collect, store, organize, and use data. Good data management can not only save time and headaches but increase the usefulness of data and enhance the reproducibility of the whole project. Good data management practices provide (Langseth et al. 2015):\nThe Data Management section contains:",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/index.html#foundations",
    "href": "docs/data-management/index.html#foundations",
    "title": "Data Management",
    "section": "",
    "text": "Short-term benefits\n\nSpend less time doing data management and more time doing research.\nEasier to prepare and use data.\nCollaborators can readily understand and use data files.\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous.\nOther researchers can find, understand, and use your data to address broad questions.\nYou get credit for preserving data products and for their use in other products.\n\n\n\n\nStorage and Organization\nData Quality and Preparation",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/index.html#the-data-life-cycle",
    "href": "docs/data-management/index.html#the-data-life-cycle",
    "title": "Data Management",
    "section": "The Data Life Cycle",
    "text": "The Data Life Cycle\nA common axiom among data scientists is the application of the 80/20 rule to effort: 80% of time is spent wrangling (managing and preparing) data, while 20% is spent on analysis. Most activities in the data life cycle come before the analysis phase and are closely tied to data management. There are many different models of the data life cycle, and the relevant model for your individual project will vary. A general data life cycle is depicted below (see also Langseth et al. 2015).\n\n\n\n\n\ngraph TB\n  A(Plan) --&gt; B(Collect)\n  B --&gt; C(Process)\n  C --&gt; D(Explore / Visualize)\n  D --&gt; E(Analyze / Model)\n  E --&gt; F(Archive, Publish, Share)\n  E --&gt; C\n  E --&gt; A\n\n\n\n\n\n\nThe data life cycle is often iterative and nonlinear, and does not always follow the order shown. Your actual analysis workflow may include dead ends or repeated steps. Regardless, it is helpful to plan and discuss your data-oriented research using these common components of the data life cycle:\n\nPlan: Identify data that will be collected and how it will be managed. Create a data management plan.\nCollect: Acquire and store raw data.\n\nAcquire: Retrieve data from the appropriate source.\nDescribe: Document the raw data source, format, variables, measurement units, coded values, and known problems. Create metadata for primary data. Cite secondary data.\nQuality assurance: Inspect the raw data for quality and fit for analysis purpose.\nStore: Store the raw data in the appropriate folder, as determined in the planning stage. Consider access, resilience (backing up), security, and, if relevant, data agreement stipulations. Make raw data files read-only so they cannot be accidentally modified.\n\nProcess: Prepare the data for exploration and analysis.\n\nClean: Preprocess the data to correct errors, standardize missing values, standardize formats, etc.\nTransform: Convert data into appropriate format and spatiotemporal scale (e.g., convert daily values to annual statistics).\nIntegrate: Combine datasets.\n\nExplore: Describe, summarize, and visualize statistics and relationships.\nAnalyze / Model: Develop, refine, and implement analysis and model specifications.\nArchive, publish, and share: Save and publish final data products and documentation.\n\n\n\n\n\nLangseth, M. L., H. S. Henkel, V. B. Hutchison, C.J. Thibodeaux, and L. S. Zolly. 2015. “USGS Data Management Training Modules—Planning for Data Management Part II Using the DMPTool to Create Data Management Plans: U.S. Geological Survey.” https://doi.org/10.5066/F7RJ4GGJ.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/data-storage-organization.html",
    "href": "docs/data-management/data-storage-organization.html",
    "title": "1  Storage & Organization",
    "section": "",
    "text": "1.1 Storage Options at RFF\nStandardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs. A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions. Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.\nThis section provides storage options and suggested practices for organization and preservation.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Storage & Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-storage-organization.html#storage-options-at-rff",
    "href": "docs/data-management/data-storage-organization.html#storage-options-at-rff",
    "title": "1  Storage & Organization",
    "section": "",
    "text": "1.1.1 Summary\n\nAlways create an L: drive folder for your project. Store all data and code here.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share code.\nFor small datasets, GitHub may be used for both data and code storage.\n\nFor both L: drive and OneDrive project folder setup, contact IThelp@rff.org with specifications.\n\n\n\n1.1.2 L: Drive\nThe primary place to store project data and code is the L: drive. All projects should have an L: drive folder, even when working with external collaborators. It is optimized for data-intensive work, has large storage capacity, is regularly backed up, and is more secure than personal drives. An RFF account is required to access the network, which can be accessed from work computers or remotely. Git repositories can be setup within L drive project folders, as described in the Organization section below.\n\nL: Drive Storage Request\nFor both L: drive and OneDrive storage, coordinate with IT to ensure appropriate resources are available when you need them. At the start of a project (or upon major changes to specifications, such as timeline or disk space), email IThelp@rff.org with the following information. Example answers are provided.\n\n\n\n\n\n\n\nField\nResponse\n\n\n\n\nProject name\nSLR-Septic\n\n\nStorage location\nL drive\n\n\nFolder name\nL:/Project-SLR-Septic\n\n\nShort description\nThis project analyzes problems with septic systems in areas subject to sea level rise and potential solutions\n\n\nPrincipal Investigator(s)\nMargaret Walls and Penny Liao\n\n\nRFF collaborators\nSophie Pesek\n\n\nExternal collaborators\nSeveral University of Maryland collaborators but they will not need access to the project folder\n\n\nData types\nR, Stata, GIS datasets\n\n\nSize requested\n80 GB\n\n\nEstimated max. size\n150GB\n\n\nArchival date\nDecember 2026\n\n\nData agreement or sensitive data security considerations\nProprietary data will be in raw data folder and will need to be made read-only\n\n\n\n\n\n\n1.1.3 Working with External Collaborators\nBecause an RFF account is required for access to the L: drive, it may be necessary to incorporate additional storage options into the project. In that case, there are a few options.\n\nOneDrive and GitHub\nThe recommended method for project data and code organization when working with external collaborators is to use a combination of OneDrive and GitHub. If access to data is the main consideration, OneDrive may be ideal. If external collaborators will be contributing to the codebase, using GitHub for code and sufficiently small data (well below 100 MB) can work well.\n\nFor Data\nFolders created on RFF’s OneDrive account have an initial capacity of 5 TB and can be shared with non-RFF staff. This folder can be accessed from any computer logged into a OneDrive account. However, OneDrive is not accessible from RFF lab computers. To accommodate this, use the L: drive project folder for all data storage, and copy only necessary data files to the OneDrive folder (for example, do not transfer over intermediate data files, only inputs and outputs). Because lab computers don’t have access to OneDrive, it might be necessary to copy files using the web browser. If necessary, OneDrive storage capacity can be increased on a project basis (but not the OneDrive accounts of external collaborators).\nExternal collaborators can choose to work directly from the OneDrive folder or use it for file transfer to their own preferred storage. Directory structures should be mirrored across the L: and OneDrive folders to maintain consistency and code functionality. For example, if a file is saved as raw_data/fires/data.shp in the L: drive, it should be shared with external collaborators in a OneDrive folder under the same directory structure.\nOne consideration when using this strategy is that if different copies of data are edited on both the L: drive and OneDrive, it can sometimes be difficult to determine who edited them and reconcile different versions. One way to avoid this problem is to store data on GitHub and use version control when possible. Git version control is most applicable for data that is small (well below 100 MB) and not restricted by licenses or other sensitivities.\n\n\n\n\n\n\nNote\n\n\n\nWhen a function to load data from a OneDrive folder is included in code, if the data is not already synchronized, it can sometimes automatically trigger the OneDrive program to download/sync the data. That may “freeze” the console where the code is running, without an informative message. This can be avoided by manually syncing the OneDrive folder before running code that depends on the data.\n\n\n\n\nCollaborative Coding\nUse GitHub as the primary storage solution. GitHub repositories can be “cloned” to a local computer. This method leverages GitHub’s distributed version control features to streamline collaboration, organization, and resilience by allowing users to “clone” GitHub repositories to a local computer (such as an RFF server folder or personal OneDrive folder), make changes independently, and then upload those changes to a shared workspace that can be accessed through any web browser or any computer with git installed [LINK TO VC SECTION]\nAs with data organization, use the same folder structure for code across shared folders In addition, it is particularly important to allow for easily configurable paths. One way to do this would be to use relative paths [LINK TO SOFTWARE QUALITY SECTION ON RELATIVE FILE PATHS] so that other code users don’t need to modify the absolute paths to be custom to their computer setup, and code can be run “out-of-the-box”.\nConsiderations:\n\nStorage limitations: OneDrive is not suitable to support large data. There are synchronizations issues when working with single files bigger than 10GB.\n\n\n\n\n\n\nNote\n\n\n\nIf external collaborators are “guests” in the OneDrive folder, they are also limited to your account’s storage limits.\n\n\nSecurity: Consider the security implications of storing sensitive data on the cloud and specify access accordingly.\n\n\n\n\nOther Options for Collaborating with External Partners\n\nOther Cloud Storage Options for large data\nAlternatives to OneDrive, such as Azure, Google Bucket, or AWS S3, may be well suited to your project. Especially for short-term storage and file transfer. Dropbox is also an option, but note that it can incur additional costs. For Azure setups, contact IT at IThelp@rff.org.\n\n\nArcGIS Online for Sharing and Exploring Spatial Data\nArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular). In order to access the data and exploratory mapping interface, users need an ArcGIS Online account. Online accounts cost $100 per user and data storage costs vary by data size. Contact RFF’s GIS Coordinator at Thompson@rff.org for more information.\n\n\nEnabling External Collaborator Access to the L: drive\nWhile not recommended, it is possible to enable access to the L: drive for non-RFF staff. Temporary accounts can be requested by contacting IT at IThelp@rff.org.\n\n\nAccessing raw data via Application Program Interfaces (APIs)\nFor large datasets, it is best to include lines in your code for downloading raw data from the web via an API, where available, rather than downloading data to disk and importing it. If an API is not available, having code download data from remote servers via other methods (e.g., curl) is still preferable.\nUsing APIs is ideal for a variety of reasons:\n\nStorage size: An API allows the user/program to access specific data of relevance without needing to find room to store an entire, broader dataset (which is instead hosted on the data provider’s server)\nReproducibility: Using public APIs or other ways of accessing public data on the internet allows for code produced using other best practices to run “out of the box” on any computer/VM with internet access\nUpdates: Code written using API queries can be programmed to retrieve new data periodically without needing to manually update a local dataset\n\nAPIs do require some special considerations:\n\nConsistency: Remote data may change. It is important to use version control and smaller intermediate datasets within a version control framework to ensure that past results can be reproduced consistently\nLongevity: APIs may stop being accessible/maintained. It is not always a safe assumption that data stored remotely will continue to be accessible via an API. However, public government datasets tend to be more reliable due to laws enforcing their accessibility. Even so, API packages and query formats may change, so project code may need to be debugged occasionally to maintain compatibility with an API.\nAccessibility: Not all online data sources have convenient API functions in a programmer’s language of choice. Some have URL formats that allow data to be accessed regardless. The amount of documentation and the level of technical skill required to understand it can vary.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Storage & Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-storage-organization.html#data-organization",
    "href": "docs/data-management/data-storage-organization.html#data-organization",
    "title": "1  Storage & Organization",
    "section": "1.2 Data Organization",
    "text": "1.2 Data Organization\n\n1.2.1 Directory Structure\nRegardless of the specific method deployed, your data project organization should have the following qualities:\n\nRaw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here\n\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L: drive. It has four main folders: data, results, code, and docs. This version illustrates a personal repository folder model of code version control, which operates best on the L: drive. Similar directories with slight changes to the code folder can be employed in other cases.\nproject_name/\n├── data/\n│   ├── raw/\n│   ├── intermediate or int/\n│   ├── clean/ (optional)\n├── results/\n├── docs/\n├── repos/\n│   ├── smith/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   ├── tools/\n│   ├── pesek/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   ├── tools/\n\nData folder\nIn the data folder, raw data is preserved in its own subfolder. The intermediate or int folder contains datasets created during data cleaning and processing. If practical, a clean folder can contain cleaned output datasets and associated READMEs [LINK TO SECTION], but note that it’s often unclear when datasets are truly “clean” until late project stages.\n\n\nResults folder\nThe results folder contains analysis results, model outputs, and all figures.\n\n\nDocs folder\nThe documents folder should contain the data management plan, a link to the GitHub site, and any other version-controlled shared documents (such as LaTeX or Markdown).\n\n\nCode folder\nFor integration with version control, the code folder has one copy of the project git repository for each of the project collaborators, so that each team member can make changes without affecting the working version of the rest of the team. Repositories are synced manually, so that changes can be made independently and then merged to the shared, remote version of the codebase stored on Github. Each individual’s repository folder has subfolders separating types of code. The scripts folder code is the main project workflow. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software [LINK TO SQ SECTION ABOUT MODULAR PROGRAMMING].\n\n\nOther project files\nNote that this template does not include specific folders for notes, literature reviews, presentations, products, project management, etc., because those types of files are not the focus of this guidance. Folders for these documents should exist either with the data folders or in another project folder, such as the shared Teams folder. If other departments or external collaborators should have access to a folder or file, we recommend storing them in Teams. See the RFF Communication Norms guidance for more information [LINK].\n\n\n\n1.2.2 Other Organization Practices\n\nSubfolders\nOrganizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (figures or model_results).\n\nHowever, it’s important to strike a balance. Too many subfolders can complicate navigation and make the project harder to understand. Aim to create subfolders only when they help categorize files meaningfully—like separating raw data from processed data or utility functions from analysis scripts—without over-complicating the structure.\n\n\nMiscellaneous Practices\n\nWhen creating folders, follow the naming conventions outlined in the following section.\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders\n\n\n\n\n1.2.3 Naming Folders, Files and Scripts\nFolder (including subfolder), file, and script names should be consistent and descriptive. Specifically, they should be human readable, machine readable, and compatible with default ordering.\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content.\n\nFor example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\n\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a README [LINK TO SECTION]\n\nMachine readable: Files and folders are easy to search for and filter based on name.\n\nUse lowercase characters (avoid “camelCase” method)\nContain only ASCII characters (letters, numbers, and underscores)\n\nDo not include spaces or special characters (/  : * ? &lt;&gt; &)\nUse hyphens or underscores instead of spaces (the “snake_case” method)\n\n\nCompatible with default ordering: the ability to sort files by name is useful and helps organization, as shown below.\n\nChronological order: If there are temporal measurements, use the ISO 8601 standard for dates (YYYY-MM-DD). Here is an example of temporal data files named for compatibility with default ordering:\n\n2021_01_01_precipitation_mm.csv\n2021_01_02_precipitation_mm.csv\n2021_01_01_temperature_statistics_f.csv\n2021_01_02_temperature_statistics_f.csv\n\n\n\n\n\n\n\nNote\n\n\n\nIt is not recommended to use dates in script file names, except in cases of individual temporal measurements. Instead, leverage version control to save different script versions.\n\n\nLogical order: one way to organize scripts or folders is by using numbered prefixes signaling the processing order. Example of source code files named for compatibility with default ordering:\n\n01_clean_raw_data.R\n02_merge_clean_data.R\n03_descriptive_statistics.R\n04_regressions.R\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the filename prefixes enter the double digits.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Storage & Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-storage-organization.html#file-and-data-formats",
    "href": "docs/data-management/data-storage-organization.html#file-and-data-formats",
    "title": "1  Storage & Organization",
    "section": "1.3 File and Data Formats",
    "text": "1.3 File and Data Formats\n\n1.3.1 File Formats\nThe best file format to use will depend on the type and structure of data. In general, data should be stored and/or archived in open formats. Open formats maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them (UC Santa Barbara’s Standard Operating Procedures section on Data Formats).\n\nKey characteristics of data file formats:\n\nProprietary vs. non-proprietary: Non-proprietary software formats can be easily imported and accessed using open-source software. This enhances their interoperability, or how easily a file format can be used across different software platforms and systems. Formats that are widely supported and compatible with various tools are generally more versatile.\nTabular vs. hierarchical: Tabular data is organized into rows and columns, resembling a table, while hierarchical data is organized in a tree-like structure, with elements nested within others.\nStructured vs. unstructured: Structured formats have well-defined schemas (specified variable names and data types), semi-structured formats are flexible but can be organized with schemas, and unstructured formats are free-form and rely on human interpretation.\nRetention of data types: Some file formats retain metadata about data types (e.g., whether a column is an integer or string), while others lose this information upon saving.\n\n\n\nTabular Formats\nThe best format to use will vary depending on the characteristics of the data being stored. Excel spreadsheets/workbooks (.xlsx) and Stata datasets (.dta) are examples of proprietary data that it is usually best to avoid. Instead, use open-source plain text formats such as comma-separated values (.csv).\n\n\n\n\n\n\nNote\n\n\n\nIn cases where the native format of source data is in a proprietary software format, it is often necessary to use that software to view and edit data. For example, Stata dataset variables may have labels, a kind of embedded metadata that can only be accessed in Stata.\n\n\n\nCharacteristics of tabular formats:\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nRecommended: comma-separated values\n.csv\nOpen-source\nNo\nStructured\n\n\nTab-separated values\n.tsv\nOpen-source\nNo\nStructured\n\n\nPlain text\n.txt\nOpen-source\nNo\nSemi-structured\n\n\nMicrosoft Excel spreadsheet/workbook\n.xls or .xlsx\nProprietary\nYes\nStructured\n\n\nDatabase File\n.dbf\nOpen-source\nYes\nStructured\n\n\nSAS dataset\n.sas7bdat\nProprietary\nYes\nStructured\n\n\nDTA\n.dta\nProprietary\nYes\nStructured\n\n\nFeather\n.feather\nOpen-source\nYes\nStructured\n\n\nSQLite\n.sqlite, .db\nOpen-source\nYes\nStructured\n\n\nRData\n.rdata or .rds\nOpen-source\nYes\nStructured\n\n\n\n\n\nDescriptions of tabular formats:\n\nRecommended: comma-separated values (.csv), delimited text files with a consistent predetermined format. Namely, each row contains the same number of values separated by commas. These are widely used for data exchange and simple data storage.\nTab-separated values (.tsv), files similar to CSV files but with values separated by tabs.\nPlain text (.txt), files which can contain unformatted or formatted (schema) text. Not recommended for storing complex datasets.\nExcel spreadsheets/workbooks (.xls, .xlsx), files designed for use with Microsoft Excel software. XLS is a binary file format compatible only with Excel, both older and newer versions. XLSX was developed more recently. It is XML-based, making it compatible with open-source software such Google Sheets as well as versions of Excel released since 2007. Generally avoid relying on these files for data storage due to complex formatting, data formats, formulas, etc. They also complicate quality assurance. XLS is not version-control friendly and XLSX requires special version-control techniques because it is stored in a compressed state. Excel spreadsheets can easily be exported to CSV files.\nDatabase File (.dbf), files used by dBASE and other database systems to store tabular data. They support a fixed schema and metadata. DBF files cannot store full precision. Avoid creating this type of file.\nSAS Dataset (.sas7bdat), the proprietary file format used by SAS for storing datasets. It supports metadata and variable attributes. Datasets should be converted to open-source formats after processing.\nDTA (.dta), binary files created by the statistical analysis software Stata. Note that they sometimes include metadata (e.g., variable labels) that isn’t automatically loaded when importing into other software (e.g. R using the haven package).\nFeather (.feather), a fast, lightweight binary columnar data format used for data exchange between data analysis languages like R and Python. Optimized for performance and efficiency, especially when working with large tables of data.\nSQLite (.sqlite, .db), files used by the SQLite relational database engine, which supports SQL queries and transactions and is used for lightweight, portable databases.\nRData (.rds, .rdata), files used to store one R object (.rds) or an R environment with several objects (.rdata). Useful if working within an R project for efficiency and organization features, but providing limited interoperability.\n\n\n\n\nHierarchical Formats\nHierarchical data formats are best suited for storing and exchanging complex, nested data structures with parent-child relationships, such as configurations, scientific datasets, or web APIs, where flexibility and the ability to represent variable levels of detail are essential.\n\nCharacteristics of hierarchical formats:\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nHierarchical Data Format version 5 (HDF5)\n.h5, .hdf5\nOpen-source\nYes\nStructured\n\n\nNetwork Common Data Form (NetCDF)\n.nc\nOpen-source\nYes\nStructured\n\n\nJavaScript Object Notation\n.json\nOpen-source\nNo\nSemi-structured\n\n\neXtensible Markup Language\n.xml\nOpen-source\nNo\nSemi-structured\n\n\nYAML\n.yml or .yaml\nOpen-source\nNo\nUnstructured\n\n\n\n\n\nDescriptions of hierarchical formats:\n\nHierarchical Data Format version 5 (.hdf5, .h5), commonly called HDF5, files for storing complex and hierarchical datasets, supporting large data volumes and complex data structures.\nNetwork Common Data Form (.nc), commonly called NetCDF, files designed for array-oriented scientific data. They work especially well for multi-dimensional data like time-series and spatial data.\nJavaScript Object Notation (.json), text-based files used for storing structured data. Often used to transfer data between a server and a web application, as well as when sending and receiving data via an API.\neXtensible Markup Language (.xml), files organizing data hierarchically with customizable tags, making them both machine-readable and human-readable. XML is widely used in web services, data exchange, and configuration files.\nYAML (.yaml or .yml), human-readable files using a data serialization format well suited for configuration files and data exchange. It uses indentation to define structure and supports key-value pairs, lists, and nested data, making it simpler and more concise compared to XML or JSON. “YAML” is a recursive acronym: YAML Ain’t Markup Language.\n\n\n\n\nGeospatial File Formats\nGeospatial data are stored in two main ways. The format of input spatial data typically dictates which geospatial tools can be applied to it.\n\nVector Data\n\nVector data is stored as pairs of coordinates. Points, lines, and polygons are all vector data.\nRecommended open-source vector file formats:\n\nGeopackage (.gpkg, recommended for its advantages over the shapefile format)\nKeyhole markup language (.kml, .kmz)\nGeoJSON (.json, .geojson)\nTables with coordinates (e.g., a CSV file)\n\nCommon proprietary vector file formats:\n\nShapefiles (.shp)\nFeature classes in geodatabases (.gdb)\n\n\n\n\n\n\n\nNote\n\n\n\nA shapefile is actually a collection of several files, including geometry (.shp), projection information (.prj), tabular data (.dbf), and more. Make sure to keep all component files together.\n\n\nAll vector data files should have three critical metadata components:\n\nCoordinate reference system\nExtent: the geographic area covered by the data, represented by four coordinate pairs\nObject type: whether the data consists of points, lines, or polygons\n\n\n\nRaster Data\n \nRaster data formats store values across a regular grid containing cells of equal size, with each cell containing a value. A raster is similar to a pixelated image, except that it’s accompanied by information linking it to a particular geographic location. All cell values within a single raster variable are of the same scalar data type (integer, float, string, etc.). Common examples of raster data are elevation, land cover, and satellite imagery.\nThe recommended general purpose raster file format is GeoTIFF (.tif), as it supports multiple bands, retention of spatial reference metadata, large file sizes, high compression, and use in a variety of languages/software. Other formats may work better for specific use cases. All of the following common formats are open-source:\n\nGeoTIFF (.tif), the most widely used format for raster data\nASCII grid (.asc), plain-text-based files for elevation models and basic raster grids\nNetCDF (.nc) and HDF5 (.hdf5, .h5), both described in Section 1.3.1.3\n\nAvoid saving rasters as proprietary software file formats, including ESRI grid/tile and ERDAS Imagine (.img) files.\nAll raster files should have five critical metadata components:\n\nCoordinate reference system\nExtent: how large the raster is, often represented by the number of rows and columns\nOrigin point: a pair of coordinates pinpointing the bottom left corner of the image\nResolution: cell size\nNo data value: the value that represents when a cell’s data value is missing\n\nFor a more in-depth introduction to spatial data types, see Introduction to Geospatial Concepts: Summary and Setup (datacarpentry.org) and GIS Training (RFF intranet).\n\n\n\nEfficiency Trade-offs\nIn addition to being more accessible, plain text-based formats are often more compatible with version control systems than many proprietary formats due to their human-readable structure. However, when working with large datasets, it’s important to consider efficiency in terms of input/output speed and file size. Data formats can vary significantly in these aspects—while binary formats are typically more efficient in terms of speed and storage, they are less suited for version control.\nFor example, in the R ecosystem, .Rds and .RData are binary file formats that allow for fast and space-efficient data storage. In comparison, large .shp (Shapefile) or .geojson files can take more than 100 times as long to load than an equivalent .Rds or .RData file. Other binary formats, such as .feather or .fst for tabular data, are both fast and lightweight, with the added benefit of being language-agnostic, meaning they can be used across different programming environments.\n\n\nFigures\nIt is helpful to think ahead when generating and saving final figures. Academic journals often accept TIFF and PNG formats, but they frequently have resolution and dimension requirements. Export figures with a minimum resolution of 300 dots per inch (DPI).\nFor RFF communications, however, vector formats such as .svg and .pdf are best because they can be easily modified as needed. Academic journals often accept these formats as well.\nIn addition to saving final figures in appropriate and flexible formats, consider that you may want to be able to share the underlying data with the RFF Communications team so that they and their external design partners can create custom figures for presentation on the website, in the magazine, etc. This means clearly documenting the processing code that created the underlying data / figures, so that output data can be easily identified and shared as needed.\n\n“Sharing the underlying data of any maps and figures is always helpful for the Communications Team!”\n– Elizabeth Wason (Editorial Director, RFF)\n\n\n\n\n1.3.2 Data Types\nIndividual data values are stored in specific data types, or formats. It is important to identify the data types of important variables in raw datasets to understand their precision and to determine whether data type conversion is necessary for your analysis.\nEvery value, or object, has a type.\nTypes control what operations/methods can be performed on a given value.\nThe choice of data type affects storage requirements. Using larger types (e.g., float64 instead of float32, or int64 instead of int32) increases memory usage, which can be significant in large datasets. Conversely, choosing types that are too small can lead to data loss or overflow errors.\nData types can be unwittingly changed, affecting precision and operations. For example, including a string in a numerical column of a CSV will likely cause all the column’s values to be read in as strings.\n\nBasic Types\n\nExamples\n\n\n\n\n\n\n\n\n\nType\nAbbreviation\nDescription\nExamples\n\n\n\n\nInteger\nint\nWhole numbers\n0, 1, 42, -6,2e+30 (scientific notation)\n\n\nFloating point\nfloat\nReal numbers, with or without a fractional component\n3.1, 2.7182818285, -1.5, 0, 43\n\n\nCharacter string\nstr\nText, demarcated by quote marks on either side\n\"Hello, world!\", 'apple', \"#23\", \"'Why?', he said.\"\n\n\n\n\n\nDetails\nNumeric values are a key element of scientific computing:\n\nIntegers represent whole numbers, which can be positive, negative, or zero\n\nSubtypes of integers can be further categorized based on their size (bits) and whether they are signed (can represent negative numbers), for example,\n\nint8: 8-bit signed integer, ranges from -128 to 127\nuint16: 16-bit unsigned integer, ranges from 0 to 65,535\n\nUse cases include counting, indexing, and scenarios where whole numbers are needed (e.g., population counts, item quantities)\nSmaller integer types use less memory but have a limited range, so it is most efficient to use the smallest type with enough room for a given data value, vector, matrix, list, etc.\n\nFloating point numbers represent real numbers, which include all fractions in addition to all integers\n\nLike integers, they can be specified by size (bits), typically 32-bit or 64-bit\n\nfloat32 is “single precision”\nfloat64 is “double precision”\n\nUse cases include scientific calculations, financial modeling, and any scenario requiring precision for fractional values (e.g., temperature measurements, stock prices)\nThe float32 type uses less memory than float64 but with less precision, so it is best to use float32 for large datasets where memory is a constraint and float64 when precision is important\n\n\nData taking the form of letters, words, or other text are used just as widely as numbers:\n\nCharacter strings contain text written between quote marks (either single or double)\n\nUse cases include names, addresses, descriptive text, and categories (e.g., gender, region, brand)\nStrings longer than a few words often take up more memory than numbers, so it’s important to manage string data carefully, especially in large datasets.\n\n\n\n\n\nSpecial Types\nCategorical data can be stored more efficiently using more specific types:\n\nLogical (boolean) data each take one of two possible values (e.g., 0 or 1, true or false)\n\nExamples of use cases include tests (pass/fail), survey responses (yes/no), and signals (on/off)\nVery memory-efficient, requiring little more than one bit per value\nProgramming note: In R, Python, and Julia, the logical or boolean type is considered a distinct type but is often compatible with or represented as integers for underlying storage or arithmetic operations. In Stata, logical values are represented directly as integers, with no separate boolean type.\n\n\nFactors represent data with a finite (usually relatively small) and usually labeled set of possible values, usually referred to as levels or categories\n\nUse cases include non-numeric data that falls into distinct categories, such as colors, months, quality survey responses, or Excel workbook cells with a dropdown list of values\nInternally stored as integers with an associated set of labels or levels\n\nDate/time values can be stored using data types designed for human- and machine-readability\n\nMost programming languages have modules that support specific date formats\nWe recommend using the conventional ISO 8601 format (YYYY-MM-DD)\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as “DD/MM/YYYY” but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect.\n\n\n\n\n\nOther resources\nSee these websites for additional information about data types:\n\nR-focused: R for Data Science: Transform\nPython-focused: Software Carpentry: Data Types and Type Conversion",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Storage & Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-storage-organization.html#sensitive-and-proprietary-data-data-agreements",
    "href": "docs/data-management/data-storage-organization.html#sensitive-and-proprietary-data-data-agreements",
    "title": "1  Storage & Organization",
    "section": "1.4 Sensitive and Proprietary Data, Data Agreements",
    "text": "1.4 Sensitive and Proprietary Data, Data Agreements\nWhen using sensitive or proprietary data in your research project, it’s crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.\n\n1.4.1 Identify and categorize sensitive data.\nDetermine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.\n\nTo determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.\n\n\n\n\n1.4.2 Document data sensitivity and restrictions.\n\nDocument data sensitivity class and details in both the data management plan and a README file in the data folder. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures\n\n\n\n1.4.3 Determine appropriate security and privacy measures.\n\nContact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.\n\n\n\n1.4.4 Data Derivatives: Masking and Aggregation\n\nData derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.\n\n\n\n\n1.4.5 Review before publication or sharing.\nRevisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Storage & Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-quality-preparation.html",
    "href": "docs/data-management/data-quality-preparation.html",
    "title": "2  Quality & Preparation",
    "section": "",
    "text": "2.1 Quality Assurance for Data Integrity\nAlways use and save a scripted program for data processing and analysis. Although it may seem more expeditious to take manual steps, writing code creates a documented and repeatable account of the processing steps taken and will save time and effort in the long-run.\nIf it is impossible to write code for processing steps, create a detailed record of the workflow in a document.\nQuality assurance (QA) is ensuring the accuracy, consistency, and reliability of data. Quality assurance measures should be implemented on both raw data from external sources and your project’s subsequent datasets; for example, after a major processing step such as data merging.\nEight basic quality assurance measures are listed below, some of which were adopted from (Hutchinson et al. 2015).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-quality-preparation.html#quality-assurance-for-data-integrity",
    "href": "docs/data-management/data-quality-preparation.html#quality-assurance-for-data-integrity",
    "title": "2  Quality & Preparation",
    "section": "",
    "text": "Read documentation / metadata that accompanies source datasets. It often comes in separate files (text, pdf, word, xml, etc.).\nAlways keep raw data in its original form. Do not modify raw datasets; save modified versions in the project’s “intermediate” data folder.\nVisually inspect data throughout processing. Visual checks can reveal issues with the data (e.g., repeated values or delimitation errors) that would affect analysis. This habit not only aids debugging processing code but also builds an understanding of the dataset.\nAssure data are delimited and line up in proper columns. Check that data is correctly delimited and parsed when imported into the processing program.\nCheck for missing values in key parameters. Identify any missing or NA values in critical fields that could impact analysis. If there are missing values, identify the type of missingness and discuss solutions (applied example in R).\nIdentify impossible and anomalous values. Anomalies include values that are outside the expected range, logically impossible, outliers, or inconsistently formatted. In addition to checking for errors, identifying outliers can aid in data exploration by flagging rare events, errors, or interesting phenomena that require further investigation.\nPerform and review statistical summaries. Generate summary statistics to understand data distribution and identify inconsistencies or errors. Use these summaries to guide further cleaning, transformation, or data integrity checks.\nVisualize data through maps, boxplots, histograms, etc.\nFollow good software quality practices described in the software quality section of this guidance, such as pseudocoding, code review, and defensive programming.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-quality-preparation.html#tidy-data",
    "href": "docs/data-management/data-quality-preparation.html#tidy-data",
    "title": "2  Quality & Preparation",
    "section": "2.2 Tidy Data",
    "text": "2.2 Tidy Data\nRaw data rarely comes in structures compatible with your team’s analysis needs. Once the raw data has been checked for quality, additional processing may be required to start exploration and analysis.\nTidy data is a framework for data organization that facilitates efficient exploration, wrangling, and analysis (Wickham 2014). The benefits of storing data in the tidy format are:\n\nEasier data exploration and analysis\nSimpler manipulation and transformation of data\nCompatibility with data analysis tools (e.g., R and Python)\nImproved reproducibility of analysis\n\nData in this format are easy to work with, analyze, and combine with other datasets. However, once analyses and data merges start taking place, the structure of newly generated datasets are likely to be more complex and dependent upon the modeling or analysis needs. Discuss the role of tidy data with your team, and if/when in the process project datasets should deviate from the tidy data structure.\n\n2.2.1 The Three Core Principles of Tidy Data\n\nEach variable forms a column: In a tidy dataset, each variable has its own dedicated column. This means all values associated with that variable are listed vertically within that single column. These are also often referred to as fields or attributes.\nEach observation forms a row: Define an observation and emphasize that each row should represent a single data point. In a tidy dataset, each observation occupies a single row in the table. All the information pertaining to that specific observation is listed horizontally across the columns. These are also often referred to as records.\nEach type of observational unit forms a table: In a tidy dataset, data pertaining to different types of observational units should be separated into distinct tables.\n\n\n\n\nimage from https://r4ds.had.co.nz/tidy-data.html#tidy-data-1\n\n\n\n\n2.2.2 Practical applications\n\nR: The R tidyverse is a set of R packages designed to work together within the tidy data framework. It includes dplyr, readr, ggplot2, and other packages useful for wrangling data.\nPython: The Python pandas library is useful for creating and working with tidy data, as it uses data frames and includes functions for cleaning, transforming, and manipulating data.\nJulia: The DataFrames.jl library is useful for working with tabular tidy data, and has many powerful tools for manipulating data. The Tidier.jl framework builds on DataFrames.jl and emulates the R tidyverse.\n\n\n\n2.2.3 Recommended reading and examples\n\nGeneral:\n\nIntroduction to Data Wrangling and Tidying | Codecademy\nA Gentle Introduction to Tidy Data in R | by Arimoro Olayinka | Medium\n\nR focus:\n\nTidy data | tidyr\nData tidying – R for Data Science\nHelpful libraries and functions:\n\ntidyr::separate\njanitor::clean_names\n\n\nPython focus:\n\nTidy Data in Python",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-quality-preparation.html#data-type-conversion-and-standardization",
    "href": "docs/data-management/data-quality-preparation.html#data-type-conversion-and-standardization",
    "title": "2  Quality & Preparation",
    "section": "2.3 Data Type Conversion and Standardization",
    "text": "2.3 Data Type Conversion and Standardization\nData types, which define how data is stored and interpreted, were presented in the previous section. This section introduces data type conversion and standardization in ensuring consistent and meaningful analysis.\nData type conversion, or typecasting, is the process of transforming a value from one data type to another. Converting data types ensures compatibility between different datasets and allows for proper analysis. For example, converting age values from string (“25 years”) to integer (25) enables mathematical operations.\nWarning: Data type conversion can sometimes lead to loss of information, so it’s crucial to understand the implications of conversion before applying it. Examples:\n\nWhen converting the age column from string (“25 years”) to integer (25), information about the unit (years) was lost.\nConverting a float (2.96) to an integer (3) truncates decimals.\nIf a date is formatted as “DD/MM/YYYY” (03/12/2015) but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect (2015-03-12, instead of the accurate 2015-12-03).\n\nProper type conversion ensures data is correctly interpreted and can prevent errors in calculations, data analysis, and visualization.\n\n2.3.1 Key Points for Type Conversion\n\nUnderstand the source and target types: Knowing the data types involved in conversion helps ensure accurate transformations.\nHandle missing or invalid data: Make sure to manage missing or improperly formatted data that could cause errors during conversion.\nTest conversions: Always verify that conversions produce the expected results to avoid downstream errors in your analysis.\nRemember to always use the ISO 8601 standard format for dates (YYYY-MM-DD).\nBe aware that importing, reimporting, or saving files in certain formats can lead to loss or changes in column data types. For instance, saving data in CSV format often results in date columns being interpreted as text upon re-import, or numeric columns losing precision. This issue arises because formats like CSV lack built-in metadata to store data types, meaning they rely on the importing program to infer types, which can cause inconsistencies and data integrity issues over time. In these cases, columns may need to be re-typecast.\n\n\n\n2.3.2 Resources\n\nPlotting and Programming in Python: Data Types and Type Conversion\nR for Data Science - Transform",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-quality-preparation.html#preparation-for-analysis",
    "href": "docs/data-management/data-quality-preparation.html#preparation-for-analysis",
    "title": "2  Quality & Preparation",
    "section": "2.4 Preparation for Analysis",
    "text": "2.4 Preparation for Analysis\nAdditional cleaning and transformation steps (often referred to as “data wrangling”) are often necessary, and are highly variable depending on project needs. Examples include:\n\nfiltering based on criteria\nrestructuring/reshaping/pivoting\nremoving duplicates\ncorrecting errors in the source data (e.g. misspellings)\nmerging\n\nThe approach depends on specific project and data needs. The following resources go into greater detail, with examples:\n\nR\n\nNCEAS Learning Hub’s coreR Course - 7 Cleaning & Wrangling Data (ucsb.edu)\nTransform – R for Data Science (2e) (hadley.nz)\nR for Reproducible Scientific Analysis: Subsetting Data (swcarpentry.github.io)\nR for Reproducible Scientific Analysis: Data Frame Manipulation with dplyr (swcarpentry.github.io)\n\nPython\n\nData Analysis and Visualization in Python for Ecologists: Indexing, Slicing and Subsetting DataFrames in Python (datacarpentry.org)\nData Analysis and Visualization in Python for Ecologists: Combining DataFrames with Pandas (datacarpentry.org)\n\n\n\n\n\n\nHutchinson, V. B., T. E. Burley, M. Y. Chang, T. A. Chatfield, and R. B. Cook. 2015. “USGS Data Management Training Modules—Best Practices for Preparing Science Data to Share: U.S. Geological Survey.” https://doi.org/10.5066/F7RJ4GGJ.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-archival.html",
    "href": "docs/data-management/data-archival.html",
    "title": "3  Archiving",
    "section": "",
    "text": "3.1 How to archive data at RFF\nArchiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to certain team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Archiving</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-archival.html#how-to-archive-data-at-rff",
    "href": "docs/data-management/data-archival.html#how-to-archive-data-at-rff",
    "title": "3  Archiving",
    "section": "",
    "text": "Note\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\n\n\n3.1.1 Finalize data organization\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see section below.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor additional guidance choosing which files to archive, see Best Practice: Decide what data to preserve.\n\n\n\n\n3.1.2 Finalize documentation\n\nFinalize data and software documentation [LINK TO DOCUMENTATION SECTION]\nCreate a text file called README and record the following information:\n\nProject name and description\nProject PI and contact information\nList of staff responsible for data management and code development\nAssociated final product, date of release, and DOI (if applicable)\nLink to published data/code (if applicable)\nLicense associated with final product\nApproximate folder size (e.g., 5 GB)\nList of researchers that should retain folder access\nNature of sensitive or proprietary data, if applicable\nAny other important notes for navigating folder or using data/code\n\n\n\n\n3.1.3 Coordinate with IT to ensure long-term folder access.\nContact IT at IThelp@rff.org to arrange archival storage of the folder. Include the README with the email. IT will help to configure storage and folder access.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Archiving</span>"
    ]
  },
  {
    "objectID": "docs/data-management/data-archival.html#archiving-source-data-and-apis",
    "href": "docs/data-management/data-archival.html#archiving-source-data-and-apis",
    "title": "3  Archiving",
    "section": "3.2 Archiving Source Data and APIs",
    "text": "3.2 Archiving Source Data and APIs\nIf you used an API to access source data, the best course for archiving source data will vary based on project needs, dataset size, and nature of the data. Some options are:\n\nDownload to folder: During the archival phase, download the source data in its current state and save it to the project folder to be archived.\nDocument: Document the dataset version and access date. While not ideal for reproducibility, this is suitable in cases where the source data is large, reliable, and not likely to be modified.\nDownload to cloud: Use a repository service, such as Zenodo, to store source data as it existed when archiving the project. Source data accessed through an API can be downloaded directly to a Zenodo repository, without having to save files locally. This can be done through R (zen4R) or python.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Archiving</span>"
    ]
  }
]