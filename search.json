[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "TODO: Add a description of the website here.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/version-control/vc1.html",
    "href": "docs/version-control/vc1.html",
    "title": "Version Control Title",
    "section": "",
    "text": "Version Control Title",
    "crumbs": [
      "Version Control",
      "Version Control Title"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/index.html",
    "href": "docs/data-management/storage-organization/index.html",
    "title": "Data Storage and Organization",
    "section": "",
    "text": "Standardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs. A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions. Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.\nThis section provides storage options and suggested practices for organization and preservation.\n\nStorage Options at RFF\nData Organization\nFile Formats\nSensitive and Proprietary Data, Data Agreements",
    "crumbs": [
      "Data Management",
      "Storage and Organization"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/sensitive-data.html",
    "href": "docs/data-management/storage-organization/sensitive-data.html",
    "title": "Sensitive and Proprietary Data, Data Agreements",
    "section": "",
    "text": "When using sensitive or proprietary data in your research project, it’s crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.\n\n\nDetermine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.\n\nTo determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.\n\n\n\n\n\n\nDocument data sensitivity class and details in both the data management plan and a README file in the data folder. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures\n\n\n\n\n\nContact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.\n\n\n\n\n\nData derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.\n\n\n\n\n\nRevisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Sensitive and Proprietary Data, Data Agreements"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/sensitive-data.html#identify-and-categorize-sensitive-data.",
    "href": "docs/data-management/storage-organization/sensitive-data.html#identify-and-categorize-sensitive-data.",
    "title": "Sensitive and Proprietary Data, Data Agreements",
    "section": "",
    "text": "Determine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.\n\nTo determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Sensitive and Proprietary Data, Data Agreements"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/sensitive-data.html#document-data-sensitivity-and-restrictions.",
    "href": "docs/data-management/storage-organization/sensitive-data.html#document-data-sensitivity-and-restrictions.",
    "title": "Sensitive and Proprietary Data, Data Agreements",
    "section": "",
    "text": "Document data sensitivity class and details in both the data management plan and a README file in the data folder. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Sensitive and Proprietary Data, Data Agreements"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/sensitive-data.html#determine-appropriate-security-and-privacy-measures.",
    "href": "docs/data-management/storage-organization/sensitive-data.html#determine-appropriate-security-and-privacy-measures.",
    "title": "Sensitive and Proprietary Data, Data Agreements",
    "section": "",
    "text": "Contact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Sensitive and Proprietary Data, Data Agreements"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/sensitive-data.html#data-derivatives-masking-and-aggregation",
    "href": "docs/data-management/storage-organization/sensitive-data.html#data-derivatives-masking-and-aggregation",
    "title": "Sensitive and Proprietary Data, Data Agreements",
    "section": "",
    "text": "Data derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Sensitive and Proprietary Data, Data Agreements"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/sensitive-data.html#review-before-publication-or-sharing.",
    "href": "docs/data-management/storage-organization/sensitive-data.html#review-before-publication-or-sharing.",
    "title": "Sensitive and Proprietary Data, Data Agreements",
    "section": "",
    "text": "Revisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Sensitive and Proprietary Data, Data Agreements"
    ]
  },
  {
    "objectID": "docs/software/software1.html",
    "href": "docs/software/software1.html",
    "title": "Software Title 1",
    "section": "",
    "text": "bulleted\nlist\nalex testing\nEthan editing\n\ncode block\n\n\n\n\nI can also italicize and bold things like this.",
    "crumbs": [
      "Software",
      "Software Title 1"
    ]
  },
  {
    "objectID": "docs/software/software1.html#subheading",
    "href": "docs/software/software1.html#subheading",
    "title": "Software Title 1",
    "section": "",
    "text": "I can also italicize and bold things like this.",
    "crumbs": [
      "Software",
      "Software Title 1"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/organization.html",
    "href": "docs/data-management/storage-organization/organization.html",
    "title": "Data Organization",
    "section": "",
    "text": "Regardless of the specific method deployed, your data project organization should have the following qualities:\n\nRaw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here\n\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L: drive. It has four main folders: data, results, code, and docs. This version illustrates a personal repository folder model of code version control, which operates best on the L: drive. Similar directories with slight changes to the code folder can be employed in other cases.\nproject_name/\n├── data/\n│   ├── raw/\n│   ├── intermediate or int/\n│   ├── clean/ (optional)\n├── results/\n├── docs/\n├── repos/\n│   ├── smith/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   ├── tools/\n│   ├── pesek/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   ├── tools/\n\n\nIn the data folder, raw data is preserved in its own subfolder. The intermediate or int folder contains datasets created during data cleaning and processing. If practical, a clean folder can contain cleaned output datasets and associated READMEs [LINK TO SECTION], but note that it’s often unclear when datasets are truly “clean” until late project stages.\n\n\n\nThe results folder contains analysis results, model outputs, and all figures.\n\n\n\nThe documents folder should contain the data management plan, a link to the GitHub site, and any other version-controlled shared documents (such as LaTeX or Markdown).\n\n\n\nFor integration with version control, the code folder has one copy of the project git repository for each of the project collaborators, so that each team member can make changes without affecting the working version of the rest of the team. Repositories are synced manually, so that changes can be made independently and then merged to the shared, remote version of the codebase stored on Github. Each individual’s repository folder has subfolders separating types of code. The scripts folder code is the main project workflow. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software [LINK TO SQ SECTION ABOUT MODULAR PROGRAMMING].\n\n\n\nNote that this template does not include specific folders for notes, literature reviews, presentations, products, project management, etc., because those types of files are not the focus of this guidance. Folders for these documents should exist either with the data folders or in another project folder, such as the shared Teams folder. If other departments or external collaborators should have access to a folder or file, we recommend storing them in Teams. See the RFF Communication Norms guidance for more information [LINK].\n\n\n\n\n\n\nOrganizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (figures or model_results).\n\nHowever, it’s important to strike a balance. Too many subfolders can complicate navigation and make the project harder to understand. Aim to create subfolders only when they help categorize files meaningfully—like separating raw data from processed data or utility functions from analysis scripts—without over-complicating the structure.\n\n\n\n\nWhen creating folders, follow the naming conventions outlined in the following section.\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders\n\n\n\n\n\nFolder (including subfolder), file, and script names should be consistent and descriptive. Specifically, they should be human readable, machine readable, and compatible with default ordering.\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content.\n\nFor example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\n\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a README [LINK TO SECTION]\n\nMachine readable: Files and folders are easy to search for and filter based on name.\n\nUse lowercase characters (avoid “camelCase” method)\nContain only ASCII characters (letters, numbers, and underscores)\n\nDo not include spaces or special characters (/  : * ? &lt;&gt; &)\nUse hyphens or underscores instead of spaces (the “snake_case” method)\n\n\nCompatible with default ordering: the ability to sort files by name is useful and helps organization, as shown below.\n\nChronological order: If there are temporal measurements, use the ISO 8601 standard for dates (YYYY-MM-DD). Here is an example of temporal data files named for compatibility with default ordering:\n\n2021_01_01_precipitation_mm.csv\n2021_01_02_precipitation_mm.csv\n2021_01_01_temperature_statistics_f.csv\n2021_01_02_temperature_statistics_f.csv\n\n\n\n\n\n\n\nNote\n\n\n\nIt is not recommended to use dates in script file names, except in cases of individual temporal measurements. Instead, leverage version control to save different script versions.\n\n\nLogical order: one way to organize scripts or folders is by using numbered prefixes signaling the processing order. Example of source code files named for compatibility with default ordering:\n\n01_clean_raw_data.R\n02_merge_clean_data.R\n03_descriptive_statistics.R\n04_regressions.R\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the filename prefixes enter the double digits.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Data Organization"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/organization.html#directory-structure",
    "href": "docs/data-management/storage-organization/organization.html#directory-structure",
    "title": "Data Organization",
    "section": "",
    "text": "Regardless of the specific method deployed, your data project organization should have the following qualities:\n\nRaw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here\n\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L: drive. It has four main folders: data, results, code, and docs. This version illustrates a personal repository folder model of code version control, which operates best on the L: drive. Similar directories with slight changes to the code folder can be employed in other cases.\nproject_name/\n├── data/\n│   ├── raw/\n│   ├── intermediate or int/\n│   ├── clean/ (optional)\n├── results/\n├── docs/\n├── repos/\n│   ├── smith/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   ├── tools/\n│   ├── pesek/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   ├── tools/\n\n\nIn the data folder, raw data is preserved in its own subfolder. The intermediate or int folder contains datasets created during data cleaning and processing. If practical, a clean folder can contain cleaned output datasets and associated READMEs [LINK TO SECTION], but note that it’s often unclear when datasets are truly “clean” until late project stages.\n\n\n\nThe results folder contains analysis results, model outputs, and all figures.\n\n\n\nThe documents folder should contain the data management plan, a link to the GitHub site, and any other version-controlled shared documents (such as LaTeX or Markdown).\n\n\n\nFor integration with version control, the code folder has one copy of the project git repository for each of the project collaborators, so that each team member can make changes without affecting the working version of the rest of the team. Repositories are synced manually, so that changes can be made independently and then merged to the shared, remote version of the codebase stored on Github. Each individual’s repository folder has subfolders separating types of code. The scripts folder code is the main project workflow. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software [LINK TO SQ SECTION ABOUT MODULAR PROGRAMMING].\n\n\n\nNote that this template does not include specific folders for notes, literature reviews, presentations, products, project management, etc., because those types of files are not the focus of this guidance. Folders for these documents should exist either with the data folders or in another project folder, such as the shared Teams folder. If other departments or external collaborators should have access to a folder or file, we recommend storing them in Teams. See the RFF Communication Norms guidance for more information [LINK].",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Data Organization"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/organization.html#other-organization-practices",
    "href": "docs/data-management/storage-organization/organization.html#other-organization-practices",
    "title": "Data Organization",
    "section": "",
    "text": "Organizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (figures or model_results).\n\nHowever, it’s important to strike a balance. Too many subfolders can complicate navigation and make the project harder to understand. Aim to create subfolders only when they help categorize files meaningfully—like separating raw data from processed data or utility functions from analysis scripts—without over-complicating the structure.\n\n\n\n\nWhen creating folders, follow the naming conventions outlined in the following section.\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Data Organization"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/organization.html#naming-folders-files-and-scripts",
    "href": "docs/data-management/storage-organization/organization.html#naming-folders-files-and-scripts",
    "title": "Data Organization",
    "section": "",
    "text": "Folder (including subfolder), file, and script names should be consistent and descriptive. Specifically, they should be human readable, machine readable, and compatible with default ordering.\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content.\n\nFor example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\n\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a README [LINK TO SECTION]\n\nMachine readable: Files and folders are easy to search for and filter based on name.\n\nUse lowercase characters (avoid “camelCase” method)\nContain only ASCII characters (letters, numbers, and underscores)\n\nDo not include spaces or special characters (/  : * ? &lt;&gt; &)\nUse hyphens or underscores instead of spaces (the “snake_case” method)\n\n\nCompatible with default ordering: the ability to sort files by name is useful and helps organization, as shown below.\n\nChronological order: If there are temporal measurements, use the ISO 8601 standard for dates (YYYY-MM-DD). Here is an example of temporal data files named for compatibility with default ordering:\n\n2021_01_01_precipitation_mm.csv\n2021_01_02_precipitation_mm.csv\n2021_01_01_temperature_statistics_f.csv\n2021_01_02_temperature_statistics_f.csv\n\n\n\n\n\n\n\nNote\n\n\n\nIt is not recommended to use dates in script file names, except in cases of individual temporal measurements. Instead, leverage version control to save different script versions.\n\n\nLogical order: one way to organize scripts or folders is by using numbered prefixes signaling the processing order. Example of source code files named for compatibility with default ordering:\n\n01_clean_raw_data.R\n02_merge_clean_data.R\n03_descriptive_statistics.R\n04_regressions.R\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the filename prefixes enter the double digits.",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Data Organization"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/file-formats.html",
    "href": "docs/data-management/storage-organization/file-formats.html",
    "title": "File Formats",
    "section": "",
    "text": "File Formats",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "File Formats"
    ]
  },
  {
    "objectID": "docs/data-management/storage-organization/storage-options.html",
    "href": "docs/data-management/storage-organization/storage-options.html",
    "title": "Storage Options at RFF",
    "section": "",
    "text": "Storage Options at RFF",
    "crumbs": [
      "Data Management",
      "Storage and Organization",
      "Storage Options at RFF"
    ]
  },
  {
    "objectID": "docs/version-control/vc2.html",
    "href": "docs/version-control/vc2.html",
    "title": "Version Control Title",
    "section": "",
    "text": "Version Control Title",
    "crumbs": [
      "Version Control",
      "Version Control Title"
    ]
  }
]