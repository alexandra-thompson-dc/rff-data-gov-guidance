[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Guidance for Researchers",
    "section": "",
    "text": "Home",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-this-resource",
    "href": "index.html#about-this-resource",
    "title": "Data Guidance for Researchers",
    "section": "About this resource",
    "text": "About this resource\nThis guidance is designed to help RFF research teams:\n\nSave time through clear data practices, templates, and reusable workflows\nIncrease flexibility for collaboration, future reuse, and reproducibility\nReduce risk by supporting consistent and transparent workflows\n\nIt includes practical support to:\n\nBuild foundational skills and concepts\nPlan and manage data-driven research from start to finish\nNavigate RFF-specific systems like storage and access\nSet up new projects effectively\nImprove existing workflows",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-outline",
    "href": "index.html#site-outline",
    "title": "Data Guidance for Researchers",
    "section": "Site Outline",
    "text": "Site Outline\n\nFoundations\nData Management\nSoftware Quality\nVersion Control",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-and-feedback",
    "href": "index.html#questions-and-feedback",
    "title": "Data Guidance for Researchers",
    "section": "Questions and feedback",
    "text": "Questions and feedback\nThis is a living resource — it will continue to evolve as needs shift and feedback is incorporated.\nTo submit questions, bugs (e.g., broken hyperlinks), suggestions, or feedback on this guidance, click Report an issue on the right-hand side of this page and submit an issue to the repository. Note that your comment will be publicly visible.\nTo submit a question or comment over email, reach out to the Data Governance Working Group.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-authors",
    "href": "index.html#site-authors",
    "title": "Data Guidance for Researchers",
    "section": "Site authors",
    "text": "Site authors\nMembers of the RFF Data Governance Working Group\n\nAris Awang\nPenny Liao\nEthan Russell\nJohn Valdez\nMatthew Wibbenmeyer\nJordan Wingenroth\nAlexandra Thompson",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/foundations/index.html",
    "href": "docs/foundations/index.html",
    "title": "Foundations",
    "section": "",
    "text": "Introduction\nGood data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project. Good data practices provide (Langseth et al. (2015)):",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#introduction",
    "href": "docs/foundations/index.html#introduction",
    "title": "Foundations",
    "section": "",
    "text": "Short-term benefits\n\nAllow you to spend less time doing data management and more time doing research\nMake it easier to prepare and use data\nEnsure that collaborators can readily understand and use data files\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous\nAllow other researchers to find, understand, and use your data to address broad questions\nEnsure that you get credit for data products and for their use in other products",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#the-data-life-cycle",
    "href": "docs/foundations/index.html#the-data-life-cycle",
    "title": "Foundations",
    "section": "The data life cycle",
    "text": "The data life cycle\nA common axiom among data scientists is the application of the 80/20 rule to effort: 80% of time is spent wrangling (managing and preparing) data, while 20% is spent on analysis. Most activities in the data life cycle come before the analysis phase and are closely tied to data management. There are many different models of the data life cycle, and the relevant model for your individual project will vary. A general data life cycle is depicted below (see also Langseth et al. 2015).\n\n\n\n\n\ngraph LR\nA(Plan) --&gt; B(Collect)\nB --&gt; C(Process)\nC --&gt; D(Explore / Visualize)\nD --&gt; E(Analyze / Model)\nE --&gt; F(Archive, Publish, Share)\nE --&gt; C\nE --&gt; A\n\n\n\n\n\n\nThe data life cycle is often iterative and nonlinear, and does not always follow the order shown. Your actual analysis workflow may include dead ends or repeated steps. Regardless, it is helpful to plan and discuss your data-oriented research using these common components of the data life cycle:\n\nStep 1: Plan Identify data that will be collected and how it will be managed. Create a data management plan.\nStep 2: Collect Acquire and store raw data.\n\na. Acquire Retrieve data from the appropriate source.\nb. Describe Document the raw data source, format, variables, measurement units, coded values, and known problems.\nc. Quality assurance Inspect the raw data for quality and fit for analysis purpose.\nd. Store Store the raw data in the appropriate folder, as determined in the planning stage. Consider access, resilience (backing up), security, and, if relevant, data agreement stipulations. Make raw data files read-only so they cannot be accidentally modified.\n\nStep 3: Process Prepare the data for exploration and analysis.\n\na. Clean Preprocess the data to correct errors, standardize missing values, standardize formats, etc.\nb. Transform Convert data into appropriate format and spatiotemporal scale (e.g., convert daily values to annual statistics).\nc. Integrate Combine datasets.\n\nStep 4: Explore Describe, summarize, and visualize statistics and relationships.\nStep 5: Analyze / Model Develop, refine, and implement analysis and model specifications.\nStep 6: Archive, Publish, and Share Finalize documentation (project-level README and metadata). Dispose and/or archive data. Publish final data products and documentation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#sec-dmp",
    "href": "docs/foundations/index.html#sec-dmp",
    "title": "Foundations",
    "section": "The first step: Data management planning for reproducibility",
    "text": "The first step: Data management planning for reproducibility\nData management planning, a form of preliminary documentation, is the process of thinking ahead about how your team will access, use, create, modify, store, share, and describe data related to your research project. Data management plans (DMPs) enhance collaboration by establishing baseline expectations, make projects resilient to turnover, and save time in the long run. In addition, many funders require data management plans be submitted with grant proposals, so thinking about these issues early can facilitate the proposal process.\nThis guidance resource provides general instructions for data practices and addresses many of the core questions that are part of the DMP process. At a minimum, the questions below should be reviewed at the start of a project. Associated guidance is linked.\n\nWhere will data/code be stored and how will it be organized?\nHow will the team use Microsoft Teams for file storage and communication? What types of files will be stored in the Teams folder versus the project’s L:/ drive folder? What will be communicated over Teams chat versus email or GitHub?\nWho will be responsible for disposing / archiving data?\nWho will be responsible for publishing data/code and attaching appropriate documentation and use licenses?\nWhat will the version control / git / GitHub workflow be?\nWhat coding software and main libraries will be used?\nHow will code be reviewed for quality?\nWhat are expectations around data quality and code quality?\nHow will data sources, code, and major methodological decisions be documented?\nHas the appropriate budget been allocated to implement this DMP?\n\nCertain projects may require additional considerations. For development of a more thorough DMP, refer to the UCSB NCEAS data management planning section.\n\n\n\n\nLangseth, M. L., H. S. Henkel, V. B. Hutchison, C.J. Thibodeaux, and L. S. Zolly. 2015. “USGS Data Management Training Modules—Planning for Data Management Part II Using the DMPTool to Create Data Management Plans: U.S. Geological Survey.” https://doi.org/10.5066/F7RJ4GGJ.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/data-management/index.html",
    "href": "docs/data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "Data management encompasses the methods used to collect, store, organize, and use data.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html",
    "href": "docs/data-management/storage-options.html",
    "title": "2  Storage Options",
    "section": "",
    "text": "2.1 Summary",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#summary",
    "href": "docs/data-management/storage-options.html#summary",
    "title": "2  Storage Options",
    "section": "",
    "text": "Store data and code for RFF projects in a project-specific L:/ drive folder.\n\nCreate and configure your new project folder\nOrganize your project folder to enable version control\n\nUse GitHub to share and version control code.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share and review code.\nFor small datasets, GitHub may be used for both data and code storage.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#internal-rff-projects",
    "href": "docs/data-management/storage-options.html#internal-rff-projects",
    "title": "2  Storage Options",
    "section": "2.2 Internal RFF Projects",
    "text": "2.2 Internal RFF Projects\n\n2.2.1 Data: L:/ drive\nThe L:/ drive is the primary location for storing project data and code. All internal projects should have a designated L:/ drive folder—even when working with external collaborators.\nThe L:/ drive is optimized for data-intensive workflows. It offers:\n\nHigh storage capacity\nRegular backups\nEnhanced security compared to personal drives\n\nAccess to the L:/ drive requires an RFF network account.\nSee instructions for setting up a project L:/ drive folder.\n\n\n2.2.2 Code: L:/ drive and GitHub\nProject code should be stored in the project’s L:/ drive folder alongside data. To support collaboration and reproducibility, also enable version control for directories containing scripts (e.g., .R, .py, .do files) and create a remote GitHub repository.\nGitHub’s distributed version control system allows team members to:\n\nWork on scripts independently without disrupting others\nTrack changes with clear commit messages\nReconcile and sync updates across folders\n\nFor example, you can revise a script locally and commit changes, with documentation, when they’re ready—without interfering with your colleague’s workflow.\nInstructions for setting up the version control system are forthcoming. In the meantime, see available external resources.\nIn order for your project to be enabled for version control in the future, each team member who will be viewing, running, or editing code should have their own script folder. Specific instructions are in the Organization section.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#collab",
    "href": "docs/data-management/storage-options.html#collab",
    "title": "2  Storage Options",
    "section": "2.3 Solutions for collaborative projects",
    "text": "2.3 Solutions for collaborative projects\n\n2.3.1 Sharing code\nThe GitHub repository method for storing and sharing scripts is ideal for collaborating with people who do not have access to the RFF network and L:/ drive. If they are made collaborators on the GitHub repository, they’re able to clone a copy of the repository codebase to their local folder directly from the web browser. (See external resources provided.)\n\n\n2.3.2 Sharing data\nIn addition to storing data and code on the L:/ drive project folder, it may be necessary to store and share files in other ways when collaborating with team members without RFF network access. In that case, there are a few options.\n\nOneDrive\nOneDrive can be used to share data with external collaborators. However, the L:/ drive should remain the primary storage location for project data, due to its advantages in access from RFF lab computers, computing capacity, storage space, and data security.\nStorage considerations\n\nFolders created on RFF’s institutional OneDrive accounts have an initial capacity of 5 TB and can be shared with non-RFF staff. If needed, storage capacity can be increased for RFF project folders (but not for individual external users’ accounts).\nIf external collaborators are “guests” in the OneDrive folder, they are also limited to your account’s storage limits.\nOneDrive is not suitable to support large data. There are synchronizations issues when working with single files bigger than 10GB.\nWhile it’s possible to load data from a OneDrive folder into a script, it is not recommended because a) the L:/ drive should be used as the primary storage location for authoritative datasets and b) loading from OneDrive can automatically trigger the OneDrive program to download/sync the data, causing hangups. Instead, copy new data from the OneDrive folder to the project L:/ drive folder.\nOneDrive files are only accessible from the web browser on RFF lab computers. The ability to sync OneDrive folders to the local drive on lab computers is unavailable at this time. The L:/ drive should be used to accessed data from lab computers.\n\nWhen using OneDrive for sharing:\n\nConsider the security implications of storing sensitive data on the cloud and specify access accordingly.\nCopy only necessary data files to the OneDrive folder (e.g., inputs and outputs, not intermediate processing files).\nExternal collaborators can either work directly from the shared OneDrive folder or download files to their own preferred storage.\nExternal collaborators can be given OneDrive folder access as Editors (ability to create, edit, and delete files) or Viewers (read-only access). How to set up collaboration in OneDrive.\nMirror (replicate) the directory structure between the L:/ and OneDrive folders to maintain clarity and consistency. This helps collaborators understand where each file fits within the overall project structure. For example: If a file is stored at raw_data/fires/data.shp on the L:/ drive, it should appear in the OneDrive folder under the same path: OneDrive/.../raw_data/fires/data.shp.\nBe mindful of who is making changes and consider using version control or clear file-naming conventions to track edits to data files. When files exist in both locations (L:/ drive and OneDrive), edits made separately on the L:/ drive and OneDrive can lead to version conflicts.\n\n\n\n\n2.3.3 GitHub\nWhile we recommend only using GitHub to version control script files, experienced users can also use it to share and version control small data files (well below 100 MB). This is also discussed in the Organization section.\nConsider security, sensitivity, and license restrictions when hosting data on GitHub.\nFiles larger than 100 MB should be shared using other tools.\n\n\n2.3.4 Accessing raw data via Application Program Interfaces (APIs)\nSome data sources allow access to data via APIs, such as the US Census and the USDA National Agricultural Statistics Service. APIs enable users or programs to download relevant subsets of data directly into memory, eliminating the need to download and store entire datasets on disk. This approach not only reduces storage requirements but also improves efficiency by allowing applications to process relevant data immediately without managing large raw files.\nWriting code that uses an API is especially advantageous for working with large datasets, because it often allows for subsets of the data to be loaded (e.g., part of a state map covering one city), which can reduce the need for local disk space. If an API is not available, having code download data from remote servers via other methods (e.g., curl) can also be a good option.\nUsing APIs is ideal for a variety of reasons:\n\nStorage size: An API allows the user/program to access specific data of relevance without needing to find room to store an entire, broader dataset (which is instead hosted on the data provider’s server)\nReproducibility advantage: Using public APIs or other ways of accessing public data on the internet allows for code produced using other best practices to run “out of the box” on any computer/VM with internet access\nFresh data: Code that queries an API can automatically retrieve the most current data each time it runs, eliminating the need to manually update local files.\n\nAPIs do require some special considerations:\n\nReproducibility risk: Because API data can change over time, it’s important to save a local copy of the queried data at key stages (e.g., before analysis or publication). This helps ensure results can be reproduced later, even if the live API data has changed.\nLongevity: APIs may stop being accessible/maintained. It is not always a safe assumption that data stored remotely will continue to be accessible via an API. API packages and query formats may also change, so project code may need to be debugged occasionally to maintain compatibility with an API.\nAccessibility: Not all online data sources have convenient API functions in a programmer’s language of choice. Some have URL formats that allow data to be accessed regardless. The amount of documentation and the level of technical skill required to understand and use an API can vary.\n\n\nPublishing/Archiving data sourced from APIs\nIf you use an API to access source data, the best course for publishing or archiving source data will vary based on project needs, dataset size, and nature of the data. Some options are:\n\nDownload to folder: During the archival phase, download the source data in its current state and save it to the project folder to be archived.\nDocument: Document the dataset version and access date in lieu of providing source data. While not ideal for reproducibility, this is suitable in cases where the source data is large, reliable, and not likely to be modified.\nDownload to cloud: Use a repository service, such as Zenodo, to store source data as it existed when archiving the project. Source data accessed through an API can be downloaded directly to a Zenodo repository, without having to save files locally. This can be done through R (zen4R) or python.\n\n\n\n\n2.3.5 Other cloud storage options for large data\nAlternatives to OneDrive, such as Azure, Google Bucket, AWS S3, or Dropbox may be well suited to your project, especially for short-term storage and file transfer. However, note that these storage options may incur additional costs, depending on data size (even the “free tiers” of these services may incur pay-as-you-go costs). For Azure setups, contact IT at IThelp@rff.org.\n\n\n2.3.6 ArcGIS Online for sharing and exploring spatial data\nArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular). In order to access the data and exploratory mapping interface, users need an ArcGIS Online account. Online accounts cost $100 per user and data storage costs vary by data size. Contact RFF’s GIS Coordinator at Thompson@rff.org for more information.\n\n\n2.3.7 Enabling external collaborator access to the L:/ drive\nWhile not recommended, it is possible to enable access to the L:/ drive for non-RFF staff. Temporary accounts can be requested by contacting IT at IThelp@rff.org.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html",
    "href": "docs/data-management/organization.html",
    "title": "3  Organization",
    "section": "",
    "text": "3.1 Directory structure\nStandardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs. A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions. Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.\nRegardless of the specific method deployed, your data project organization should have the following qualities:\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here.\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L:/ drive. This version illustrates a personal repository folder model of code version control, which operates best on the L:/ drive.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#directory-structure",
    "href": "docs/data-management/organization.html#directory-structure",
    "title": "3  Organization",
    "section": "",
    "text": "Raw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\n\n\nproject_name/\n├── data/\n│   ├── raw/\n│   ├── intermediate/\n│   ├── clean/ (optional)\n├── results*/\n├── repos/\n│   ├── edgar/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n│   ├── caudle/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n\n3.1.1 Data folder\n\nThe data folder contains all project data sets.\nRaw data is preserved in its own subfolder.\nThe intermediate folder contains datasets created during data cleaning and processing.\nIf practical, a clean folder can contain cleaned output datasets, but note that it’s often unclear when datasets are truly “clean” until late project stages.\n\n\n\n\n\n\n\nNote\n\n\n\nFor projects with small datasets, this folder can be version controlled, with larger files ignored using the .gitignore file. In this case, this folder would be a subfolder of individuals’ repository folders.\n\n\n\n\n3.1.2 Results folder\n\nThe results folder contains analysis results, and model outputs. For example, it can be used to store tables, figures, and model estimates.\n*For internal RFF projects, the results folder can be stored in the main directory and not within repositories.\n**For projects with external collaborators, it may be useful to store the results folder within repositories. These files are generally smaller than 100 MB and can be stored in a main repository using GitHub; however, some formats (such as SVG) can be quite large. The .gitignore file can be configured to ignore certain file types (such as SVG files, when both PNG and SVG file formats are generated).\n\n\n\n3.1.3 Repos folder\n\nThis directory structure is based on personal repositories. The repos, or repositories, directory contains version-controlled files. To allow individuals to work on version controlled files without interfering with others’ versions, each researcher should have their own folder that’s linked to the GitHub remote repository. Team members can then clone the project’s Git repository into their respective folders and work exclusively within their own copies. Changes can be synced and reconciled using GitHub, preventing simultaneous edits of the same file and ensuring effective version control.\nIndividual folders (e.g. Edgar, Caudle) should have mirrored (the same) directory structures.\n\n\n\n3.1.4 Scripts folders\n\nThe scripts folder contains all code files (e.g. .R, .py, .do) for the project. These scripts provide explicit instructions for processing data and performing analyses. It should be possible to reproduce the entirety of the project’s processed data sets and results using only these scripts and the raw data.\nThe scripts folder can be parsed into subfolders containing scripts that process raw data (processing), analyze processed data (analysis), and tools. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software.\n\n\n\n3.1.5 Docs folder\nThe documents folder should contain any version-controlled shared documents (e.g. LaTeX, Markdown, Overleaf).\n\n\n3.1.6 Other project files\nThis template does not include specific folders for meeting notes, literature reviews, presentations, project management, etc., because those types of files are not the focus of this guidance.\nWe recommend storing these types of files in the project-level Microsoft Teams folder. See the RFF Communication Norms and Guidance for more information.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#other-organization-practices",
    "href": "docs/data-management/organization.html#other-organization-practices",
    "title": "3  Organization",
    "section": "3.2 Other organization practices",
    "text": "3.2 Other organization practices\n\n3.2.1 Subfolders\nOrganizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (figures or model_results).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "href": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "title": "3  Organization",
    "section": "3.3 Naming folders, files and scripts",
    "text": "3.3 Naming folders, files and scripts\nWhen creating folders:\n\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders.\n\nWhen creating data or script files, make them:\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content. For example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you must create different versions of files, make sure to document the distinction in a README file.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a project-level README file.\n\nMachine readable:\n\nUse only ASCII characters (letters, numbers, and underscores).\nDo not include spaces or special characters (/  : * ? &lt;&gt; &).\nUse hyphens or underscores instead of spaces (the “snake_case” method).\nFiles and folders should be easy to search and filter based on name using structured file names. The ability to sort and read files by name is useful and helps organization but requires specific conventions. See examples in the table below, keeping in mind that:\n\nIt is not recommended to use dates in script file names to denote when a script was created or modified. Instead, leverage version control to save and document different script versions.\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the file name prefixes enter the double digits.\n\n\n\n\n\n\n\n\n\n\n\nPractice for structured file names\nDescription\nExample\n\n\n\n\nID-based names\nStructure file paths and names in a way that makes them easy to access programmatically—e.g., enabling batch loading or iteration across identifiers like years/dates, counties, or scenarios. For example, storing county-level data as shown would allow data to be read into memory by simply looping over FIPS codes as they appear in the directory file names.\n53019_data.csv53033_data.csv53061_data.csv\n\n\nChronological order\nUse ISO 8601 format for date-based files: YYYY-MM-DD. This ensures dates sort correctly by default.\n2021_01_01_precipitation_mm.csv2021_01_02_precipitation_mm.csv2021_01_01_temperature_statistics_f.csv2021_01_02_temperature_statistics_f.csv\n\n\nLogical processing order\nFor scripts or folders that must run in sequence, use numeric prefixes to indicate the intended order of execution.\n01_clean_raw_data.R02_merge_clean_data.R03_descriptive_statistics.R04_regressions.R",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html",
    "href": "docs/data-management/documentation.html",
    "title": "4  Documentation",
    "section": "",
    "text": "4.1 Introduction\n“If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point” (Stoudt, Vásquez, and Martinez (2021)).\nQuality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts.\nWithout documentation, projects lose their usefulness over time, as illustrated below (see also Michener et al. 1997).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#introduction",
    "href": "docs/data-management/documentation.html#introduction",
    "title": "4  Documentation",
    "section": "",
    "text": "Image from Michener et al. 1997",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#types-of-documentation",
    "href": "docs/data-management/documentation.html#types-of-documentation",
    "title": "4  Documentation",
    "section": "4.2 Types of documentation",
    "text": "4.2 Types of documentation\nThis summary can help guide team conversations around documentation strategies.\n\n4.2.1 Preliminary\nPreliminary documentation refers to early-stage descriptions created during the planning phase. This often includes a data management plan (DMP), which outlines data collection, organization, storage, and sharing strategies.\n\n\n4.2.2 Process\nProcess documentation involves capturing step-by-step procedures and workflows used to collect, process, analyze, and model data, including:\n\nDocumenting raw data sources\nDocumenting methods with code comments\nDocumenting methods with version control tools\n\n\n\n4.2.3 Product\nProduct documentation provides information to accompany code and data of completed projects, ensuring usability and transparency. This documentation can be a part of or accompanying written products.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#core-documentation",
    "href": "docs/data-management/documentation.html#core-documentation",
    "title": "4  Documentation",
    "section": "4.3 Core documentation",
    "text": "4.3 Core documentation\n\n4.3.1 The project-level README file\nAt a minimum, the project should have a README file in text or markdown format with the information listed below. This can be an evolving, living document. While it’s technically product documentation, it’s easiest to start it early in the project’s development.\n\nProject name and description\nProject PI and contact information\nList of staff responsible for data management and code development\nAssociated final product(s), date of release, and DOI (if applicable)\nLink to published data/code (if applicable)\nLicense associated with final product(s)\nNature of sensitive or proprietary data (if applicable)\nAny other important notes for navigating folder or using data/code\n\n\n\n4.3.2 Raw data\nBest practices:\n\nThe source of all downloaded raw datasets should be documented in a README file.\nCreate a README file associated with each raw data file, or each logical “cluster” of related raw data files, in the same folder as the data.\nIf there are multiple data files in a folder, name the README so that it is easily associated with the data file(s) it describes (e.g., README_PRISM_Daily_Temperature.txt).\nFormat README files consistently.\nWrite the README document in an plain text and open source file format, such as .txt or .md.\n\nBelow is a README template and example. Include in the README file the information shown in the example.\nFilename: README_PRISM_Daily_Temperature.txt\nDataset name & format: PRISM_Daily_Temperature_2024.csv\n\nData source: Downloaded from the PRISM Climate Group website: https://prism.oregonstate.edu/ on 2024-02-28.\n\nAcquired by: [Name of researcher who downloaded the data]\n\nData description: This dataset contains daily minimum and maximum temperature data for Washington State for the year 2024, with a spatial resolution of 4km.\n\nPreprocessing: No modifications were made to the raw dataset. The file is stored exactly as downloaded from the source.\n\nLicense & Usage Restrictions: This dataset is publicly available under the PRISM Climate Group's data use policy. Refer to https://prism.oregonstate.edu/documents/PRISM_terms_of_use.pdf for more details.\n\n\n\n\n\n\nNote\n\n\n\nAs described in the Organization section, all raw data should be retained in their raw form and not directly modified.\n\n\n\n\n\n\nMichener, W. K., J. W. Brunt, J. J. Helly, T. B. Kirchner, and S. G. Stafford. 1997. “Nongeospatial Metadata for the Ecological Sciences.” https://doi.org/10.1890/1051-0761.\n\n\nStoudt, S., V. N. Vásquez, and C. C. Martinez. 2021. “Principles for Data Analysis Workflows.” https://doi.org/10.1371/e1008770.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html",
    "href": "docs/data-management/file-formats.html",
    "title": "5  File & Data Types",
    "section": "",
    "text": "5.1 Data file formats\nIn general, data should be stored and/or archived in open formats. Open formats are non-proprietary, and therefore maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them (UC Santa Barbara’s Standard Operating Procedures section on Data Formats). The best file format to use will depend on the type and structure of data.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-file-formats",
    "href": "docs/data-management/file-formats.html#data-file-formats",
    "title": "5  File & Data Types",
    "section": "",
    "text": "5.1.1 Key characteristics of data file formats\n\nProprietary vs. non-proprietary: Non-proprietary software formats can be easily imported and accessed using open-source software. This enhances their interoperability, or how easily a file format can be used across different software platforms and systems. Formats that are widely supported and compatible with various tools are generally more versatile.\nTabular vs. hierarchical: Tabular data is organized into rows and columns, resembling a table, while hierarchical data is organized in a tree-like structure, with elements nested within others.\nStructured vs. unstructured: Structured data refers to data that is organized in a predefined format, typically in rows and columns, like databases or spreadsheets, which allows for easy search, analysis, and processing. Unstructured data, on the other hand, lacks a predefined format and is often textual or multimedia in nature, such as emails, social media posts, or video files.\nRetention of data types: Some file formats retain metadata about data types (e.g., whether a column is an integer or string), while others lose this information upon saving.\n\n\n\n5.1.2 Tabular formats\nIn general, it is best to use open-source plain text formats such as comma-separated values (.csv). In some cases, other file formats may be useful for optimizing file size or read/write speed, or for retaining data type information; however, alternative data types may have reduced interoperability. It is generally best to avoid Excel spreadsheets/workbooks (.xlsx) and Stata datasets (.dta) because they are proprietary, except in special cases where these required features offered by these file formats (e.g. variable labels in Stata .dta files) are not available in open-source file formats.\n\n\n\n\n\n\nNote\n\n\n\nIn cases where the native format of source data is in a proprietary software format, it is often necessary to use that software to view and edit data. For example, Stata dataset variables may have labels, a kind of embedded metadata that can only be accessed in Stata.\n\n\n\nCharacteristics of tabular formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nRecommended: comma-separated values\n.csv\nOpen-source\nNo\nStructured\n\n\nTab-separated values\n.tsv\nOpen-source\nNo\nStructured\n\n\nPlain text\n.txt\nOpen-source\nNo\nSemi-structured\n\n\nMicrosoft Excel spreadsheet/workbook\n.xls or .xlsx\nProprietary\nYes\nStructured\n\n\nDatabase File\n.dbf\nOpen-source\nYes\nStructured\n\n\nSAS dataset\n.sas7bdat\nProprietary\nYes\nStructured\n\n\nDTA\n.dta\nProprietary\nYes\nStructured\n\n\nFeather\n.feather\nOpen-source\nYes\nStructured\n\n\nSQLite\n.sqlite, .db\nOpen-source\nYes\nStructured\n\n\nRData\n.rdata or .rds\nOpen-source\nYes\nStructured\n\n\n\n\n\nDescriptions of tabular formats\n\nRecommended: comma-separated values (.csv), delimited text files widely used for data exchange and simple data storage. Each row contains the same number of values separated by commas.\nTab-separated values (.tsv), files similar to CSV files but with values separated by tabs.\nPlain text (.txt), files which can contain unformatted or formatted (schema) text. Not recommended for storing complex datasets.\nExcel spreadsheets/workbooks (.xls, .xlsx), files designed for use with Microsoft Excel software. XLS is a binary file format compatible only with Excel, both older and newer versions. XLSX was developed more recently. It is XML-based, making it compatible with open-source software such Google Sheets as well as versions of Excel released since 2007. Generally avoid relying on these files for data storage due to complex formatting, data formats, formulas, etc. They also complicate quality assurance. XLS is not version-control friendly and XLSX requires special version-control techniques because it is stored in a compressed state. Excel spreadsheets can easily be exported to CSV files.\nDatabase File (.dbf), files used by dBASE and other database systems to store tabular data. They support a fixed schema and metadata. DBF files cannot store full precision. Avoid creating this type of file.\nSAS Dataset (.sas7bdat), the proprietary file format used by SAS for storing datasets. It supports metadata and variable attributes. Datasets should be converted to open-source formats after processing.\nDTA (.dta), binary files created by the statistical analysis software Stata. Note that they sometimes include metadata (e.g., variable labels) that isn’t automatically loaded when importing into other software (e.g. R using the haven package).\nFeather (.feather), a fast, lightweight binary columnar data format used for data exchange between data analysis languages like R and Python. Optimized for performance and efficiency, especially when working with large tables of data.\nSQLite (.sqlite, .db), files used by the SQLite relational database engine, which supports SQL queries and transactions and is used for lightweight, portable databases.\nRData (.rds, .rdata), files used to store one R object (.rds) or an R environment with several objects (.rdata). Useful if working within an R project for efficiency and organization features, but providing limited interoperability.\n\n\n\n\n5.1.3 Hierarchical formats\nHierarchical data formats are best suited for storing and exchanging complex, nested data structures with parent-child relationships, such as configurations, scientific datasets, or web APIs, where flexibility and the ability to represent variable levels of detail are essential.\n\nCharacteristics of hierarchical formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nHierarchical Data Format version 5 (HDF5)\n.h5, .hdf5\nOpen-source\nYes\nStructured\n\n\nNetwork Common Data Form (NetCDF)\n.nc\nOpen-source\nYes\nStructured\n\n\nJavaScript Object Notation\n.json\nOpen-source\nNo\nSemi-structured\n\n\neXtensible Markup Language\n.xml\nOpen-source\nNo\nSemi-structured\n\n\nYAML\n.yml or .yaml\nOpen-source\nNo\nUnstructured\n\n\n\n\n\nDescriptions of hierarchical formats:\n\nHierarchical Data Format version 5 (.hdf5, .h5), commonly called HDF5, files for storing complex and hierarchical datasets, supporting large data volumes and complex data structures.\nNetwork Common Data Form (.nc), commonly called NetCDF, files designed for array-oriented scientific data. They work especially well for multi-dimensional data like time-series and spatial data.\nJavaScript Object Notation (.json), text-based files used for storing structured data. Often used to transfer data between a server and a web application, as well as when sending and receiving data via an API.\neXtensible Markup Language (.xml), files organizing data hierarchically with customizable tags, making them both machine-readable and human-readable. XML is widely used in web services, data exchange, and configuration files.\nYAML (.yaml or .yml), human-readable files using a data serialization format well suited for configuration files and data exchange. It uses indentation to define structure and supports key-value pairs, lists, and nested data, making it simpler and more concise compared to XML or JSON. “YAML” is a recursive acronym: YAML Ain’t Markup Language.\n\n\n\n\n5.1.4 Geospatial file formats\nGeospatial data are stored as either vector data or raster data. The format of input spatial data typically dictates which geospatial tools can be applied to it.\n\nVector data\n\nVector data is stored as pairs of coordinates. Points, lines, and polygons are all vector data.\nRecommended open-source vector file formats:\n\nGeopackage (.gpkg, recommended for its advantages over the shapefile format)\nKeyhole markup language (.kml, .kmz)\nGeoJSON (.json, .geojson)\nTables with coordinates (e.g., a CSV file)\n\nCommon proprietary vector file formats:\n\nShapefiles (.shp)\nFeature classes in geodatabases (.gdb)\n\n\n\n\n\n\n\nNote\n\n\n\nA shapefile is actually a collection of several files, including geometry (.shp), projection information (.prj), tabular data (.dbf), and more. Make sure to store all component files together within the same folder.\n\n\nAll vector data files should have three critical metadata components:\n\nCoordinate reference system\nExtent: the geographic area covered by the data, represented by four coordinate pairs\nObject type: whether the data consists of points, lines, or polygons\n\n\n\nRaster data\n \nRaster data formats store values across a regular grid containing cells of equal size, with each cell containing a value. A raster is similar to a pixelated image, except that it’s accompanied by information linking it to a particular geographic location. All cell values within a single raster variable are of the same scalar data type (integer, float, string, etc.). Common examples of raster data are elevation, land cover, and satellite imagery.\nThe recommended general purpose raster file format is GeoTIFF (.tif), as it supports multiple bands, retention of spatial reference metadata, large file sizes, high compression, and use in a variety of languages/software. Other formats may work better for specific use cases. All of the following common formats are open-source:\n\nGeoTIFF (.tif), the most widely used format for raster data\nASCII grid (.asc), plain-text-based files for elevation models and basic raster grids\nNetCDF (.nc) and HDF5 (.hdf5, .h5), both described in Section 5.1.3\n\nAvoid saving rasters as proprietary software file formats, including ESRI grid/tile and ERDAS Imagine (.img) files.\nAll raster files should have five critical metadata components:\n\nCoordinate reference system\nExtent: how large the raster is, often represented by the number of rows and columns\nOrigin point: a pair of coordinates pinpointing the bottom left corner of the image\nResolution: cell size\nNo data value: the value that represents when a cell’s data value is missing\n\nFor a more in-depth introduction to spatial data types, see Introduction to Geospatial Concepts: Summary and Setup (datacarpentry.org) and GIS Training (RFF intranet).\n\n\n\n5.1.5 Efficiency trade-offs\nIn addition to being more accessible, plain text-based formats are often more compatible with version control systems than many proprietary formats due to their human-readable structure. However, when working with large datasets, it’s important to consider efficiency in terms of input/output speed and file size. Data formats can vary significantly in these aspects—while binary formats are typically more efficient in terms of speed and storage, they are less suited for version control.\nFor example, in the R ecosystem, .Rds and .RData are binary file formats that allow for fast and space-efficient data storage. In comparison, large .shp (Shapefile) or .geojson files can take more than 100 times as long to load than an equivalent .Rds or .RData file. Other binary formats, such as .feather or .fst for tabular data, are both fast and lightweight, with the added benefit of being language-agnostic, meaning they can be used across different programming environments.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#figure-file-formats",
    "href": "docs/data-management/file-formats.html#figure-file-formats",
    "title": "5  File & Data Types",
    "section": "5.2 Figure file formats",
    "text": "5.2 Figure file formats\nIt is helpful to think ahead when generating and saving data visualizations and plots. Academic journals often accept TIFF and PNG formats, but they frequently have resolution and dimension requirements. Export figures with a minimum resolution of 300 dots per inch (DPI).\nFor RFF communications, however, the vector format .svg is best because it can be easily modified as needed. Academic journals often accept this format as well.\nConsider that you may want to be able to share the underlying data with the RFF Communications team so that they and their external design partners can create custom figures for presentation on the website, in the magazine, etc. This means clearly documenting the processing code that created the underlying data / figures, so that output data can be easily reproduced and shared as needed. If figure data is time-consuming to reproduce, you may want to save a copy of it to the L:/ drive or to your GitHub repository.\n\n“Sharing the underlying data of any maps and figures is always helpful for the Communications Team!”\n– Elizabeth Wason (Editorial Director, RFF)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-types",
    "href": "docs/data-management/file-formats.html#data-types",
    "title": "5  File & Data Types",
    "section": "5.3 Data types",
    "text": "5.3 Data types\nIndividual data values are stored in specific data types, or formats. It is important to identify the data types of important variables in raw datasets to understand their precision and to determine whether data type conversion is necessary for your analysis.\nEvery value, or object, has a type.\nTypes control what operations/methods can be performed on a given value.\nThe choice of data type affects storage requirements. Using larger types (e.g., float64 instead of float32, or int64 instead of int32) increases memory usage, which can be significant in large datasets. Conversely, choosing types that are too small can lead to data loss or overflow errors.\n\n\n\n\n\n\nWarning\n\n\n\nData types can be unwittingly changed, affecting precision and operations. For example, including a string in a numerical column of a CSV will likely cause all the column’s values to be read in as strings.\n\n\n\n5.3.1 Basic types\n\nExamples\n\n\n\n\n\n\n\n\n\nType\nAbbreviation\nDescription\nExamples\n\n\n\n\nInteger\nint\nWhole numbers\n0, 1, 42, -6,2e+30 (scientific notation)\n\n\nFloating point\nfloat\nReal numbers, with or without a fractional component\n3.1, 2.7182818285, -1.5, 0, 43\n\n\nCharacter string\nstr\nText, demarcated by quote marks on either side\n\"Hello, world!\", 'apple', \"#23\", \"'Why?', he said.\"\n\n\n\n\n\nDetails\nNumeric values are a key element of scientific computing:\n\nIntegers represent whole numbers, which can be positive, negative, or zero\n\nSubtypes of integers can be further categorized based on their size (bits) and whether they are signed (can represent negative numbers), for example,\n\nint8: 8-bit signed integer, ranges from -128 to 127\nuint16: 16-bit unsigned integer, ranges from 0 to 65,535\n\nUse cases include counting, indexing, and scenarios where whole numbers are needed (e.g., population counts, item quantities)\nSmaller integer types use less memory but have a limited range, so it is most efficient to use the smallest type with enough room for a given data value, vector, matrix, list, etc.\n\nFloating point numbers represent real numbers, which include all fractions in addition to all integers\n\nLike integers, they can be specified by size (bits), typically 32-bit or 64-bit\n\nfloat32 is “single precision”\nfloat64 is “double precision”\n\nUse cases include scientific calculations, financial modeling, and any scenario requiring precision for fractional values (e.g., temperature measurements, stock prices)\nThe float32 type uses less memory than float64 but with less precision, so it is best to use float32 for large datasets where memory is a constraint and float64 when precision is important\n\n\nData taking the form of letters, words, or other text are used just as widely as numbers:\n\nCharacter strings contain text written between quote marks (either single or double)\n\nUse cases include names, addresses, descriptive text, and categories (e.g., gender, region, brand)\nStrings longer than a few words often take up more memory than numbers, so it’s important to manage string data carefully, especially in large datasets.\n\n\n\n\n\n5.3.2 Special types\nCategorical data can be stored more efficiently using more specific types:\n\nLogical (boolean) data each take one of two possible values (e.g., 0 or 1, true or false)\n\nExamples of use cases include tests (pass/fail), survey responses (yes/no), and signals (on/off)\nVery memory-efficient, requiring little more than one bit per value\nProgramming note: In R, Python, and Julia, the logical or boolean type is considered a distinct type but is often compatible with or represented as integers for underlying storage or arithmetic operations. In Stata, logical values are represented directly as integers, with no separate boolean type.\n\n\nFactors represent data with a finite (usually relatively small) and usually labeled set of possible values, usually referred to as levels or categories\n\nUse cases include non-numeric data that falls into distinct categories, such as colors, months, quality survey responses, or Excel workbook cells with a dropdown list of values\nInternally stored as integers with an associated set of labels or levels\n\nDate/time values can be stored using data types designed for human- and machine-readability\n\nMost programming languages have modules that support specific date formats\nWe recommend using the conventional ISO 8601 format (YYYY-MM-DD)\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as “DD/MM/YYYY” but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect.\n\n\n\n\n\n5.3.3 Other resources\nSee these websites for additional information about data types:\n\nR-focused: R for Data Science: Transform\nPython-focused: Software Carpentry: Data Types and Type Conversion",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html",
    "href": "docs/data-management/sensitive-proprietary.html",
    "title": "6  Sensitive & Proprietary Data",
    "section": "",
    "text": "6.1 Identify and categorize sensitive data\nWhen using sensitive or proprietary data in your research project, it’s crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.\nDetermine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "href": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "title": "6  Sensitive & Proprietary Data",
    "section": "",
    "text": "To determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "href": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.2 Document data sensitivity and restrictions",
    "text": "6.2 Document data sensitivity and restrictions\n\nDocument data sensitivity class and details in both the project-level README and, if the data are secondary, the associated raw data README file. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "href": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.3 Determine appropriate security and privacy measures",
    "text": "6.3 Determine appropriate security and privacy measures\n\nContact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "href": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.4 Data derivatives: masking and aggregation",
    "text": "6.4 Data derivatives: masking and aggregation\n\nData derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "href": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.5 Review before publication or sharing",
    "text": "6.5 Review before publication or sharing\nRevisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html",
    "href": "docs/data-management/quality-preparation.html",
    "title": "7  Quality & Preparation",
    "section": "",
    "text": "7.1 Quality Assurance for Data Integrity\nAlways use and save a scripted program for data processing and analysis. Although it may seem more expeditious to take manual steps, writing code creates a documented and repeatable account of the processing steps taken and will save time and effort in the long-run.\nIf it is impossible to write code for processing steps, create a detailed record of the workflow in a document.\nQuality assurance (QA) is ensuring the accuracy, consistency, and reliability of data. Quality assurance measures should be implemented on both raw data from external sources and your project’s subsequent datasets; for example, after a major processing step such as data merging.\nEight basic quality assurance measures are listed below, some of which were adapted from (Hutchinson et al. 2015).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "href": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "title": "7  Quality & Preparation",
    "section": "",
    "text": "Read documentation / metadata that accompanies source datasets. It often comes in separate files (text, pdf, word, xml, etc.).\nAlways keep raw data in its original form. Do not modify raw datasets; save modified versions in the project’s “intermediate” data folder.\nVisually inspect data throughout processing. Visual checks can reveal issues with the data (e.g., repeated values or delimitation errors) that would affect analysis. This habit not only aids debugging processing code but also builds an understanding of the dataset.\nAssure data are delimited and line up in proper columns. Check that data is correctly delimited and parsed when imported into the processing program.\nCheck for missing values. Identify any missing or NA values in critical fields that could impact analysis. If there are missing values, identify the type of missingness and discuss solutions (applied example in R).\nIdentify impossible and anomalous values. Anomalies include values that are outside the expected range, logically impossible, outliers, or inconsistently formatted. In addition to checking for errors, identifying outliers can aid in data exploration by flagging rare events, errors, or interesting phenomena that require further investigation.\nPerform and review statistical summaries. Generate summary statistics to understand data distribution and identify inconsistencies or errors. Use these summaries to guide further cleaning, transformation, or data integrity checks.\nVisualize data through maps, boxplots, histograms, etc.\nFollow good software quality practices described in the software quality section of this guidance, such as pseudocoding, code review, and defensive programming.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#tidy-data",
    "href": "docs/data-management/quality-preparation.html#tidy-data",
    "title": "7  Quality & Preparation",
    "section": "7.2 Tidy Data",
    "text": "7.2 Tidy Data\nRaw data rarely comes in structures compatible with your team’s analysis needs. Once the raw data has been checked for quality, additional processing may be required to start exploration and analysis.\nTidy data is a framework for data organization that facilitates efficient exploration, wrangling, and analysis (Wickham 2014). The benefits of storing data in the tidy format are:\n\nEasier data exploration and analysis\nSimpler manipulation and transformation of data\nCompatibility with data analysis tools (e.g., R and Python)\nImproved reproducibility of analysis\n\nData in this format are easy to work with, analyze, and combine with other datasets. However, once analyses and data merges start taking place, the structure of newly generated datasets are likely to be more complex and dependent upon the modeling or analysis needs. Discuss the role of tidy data with your team, and if/when in the process project datasets should deviate from the tidy data structure.\n\n7.2.1 The Three Core Principles of Tidy Data\n\nEach variable forms a column: In a tidy dataset, each variable has its own dedicated column. This means all values associated with that variable are listed vertically within that single column. These are also often referred to as fields or attributes.\nEach observation forms a row: Define an observation and emphasize that each row should represent a single data point. In a tidy dataset, each observation occupies a single row in the table. All the information pertaining to that specific observation is listed horizontally across the columns. These are also often referred to as records.\nEach type of observational unit forms a table: In a tidy dataset, data pertaining to different types of observational units should be separated into distinct tables.\n\n\n\n\nimage from https://r4ds.had.co.nz/tidy-data.html#tidy-data-1\n\n\n\n\n7.2.2 Practical applications\n\nR: The R tidyverse is a set of R packages designed to work together within the tidy data framework. It includes dplyr, readr, ggplot2, and other packages useful for wrangling data.\nPython: The Python pandas library is useful for creating and working with tidy data, as it uses data frames and includes functions for cleaning, transforming, and manipulating data.\nJulia: The DataFrames.jl library is useful for working with tabular tidy data, and has many powerful tools for manipulating data. The Tidier.jl framework builds on DataFrames.jl and emulates the R tidyverse.\n\n\n\n7.2.3 Recommended reading and examples\n\nGeneral:\n\nIntroduction to Data Wrangling and Tidying | Codecademy\nA Gentle Introduction to Tidy Data in R | by Arimoro Olayinka | Medium\n\nR focus:\n\nTidy data | tidyr\nData tidying – R for Data Science\nHelpful libraries and functions:\n\ntidyr::separate\njanitor::clean_names\n\n\nPython focus:\n\nTidy Data in Python",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "href": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "title": "7  Quality & Preparation",
    "section": "7.3 Data Type Conversion and Standardization",
    "text": "7.3 Data Type Conversion and Standardization\nData types, which define how data is stored and interpreted, were presented in the previous section. This section introduces data type conversion and standardization in ensuring consistent and meaningful analysis.\nData type conversion, or typecasting, is the process of transforming a value from one data type to another. Converting data types ensures compatibility between different datasets and allows for proper analysis. For example, converting age values from string (“25 years”) to integer (25) enables mathematical operations.\nWarning: Data type conversion can sometimes lead to loss of information, so it’s crucial to understand the implications of conversion before applying it. Examples:\n\nWhen converting the age column from string (“25 years”) to integer (25), information about the unit (years) was lost.\nConverting a float (2.96) to an integer (3) truncates decimals.\nIf a date is formatted as “DD/MM/YYYY” (03/12/2015) but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect (2015-03-12, instead of the accurate 2015-12-03).\n\nProper type conversion ensures data is correctly interpreted and can prevent errors in calculations, data analysis, and visualization.\n\n7.3.1 Key Points for Type Conversion\n\nUnderstand the source and target types: Knowing the data types involved in conversion helps ensure accurate transformations.\nHandle missing or invalid data: Make sure to manage missing or improperly formatted data that could cause errors during conversion.\nTest conversions: Always verify that conversions produce the expected results to avoid downstream errors in your analysis.\nRemember to always use the ISO 8601 standard format for dates (YYYY-MM-DD).\nBe aware that importing, reimporting, or saving files in certain formats can lead to loss or changes in column data types. For instance, saving data in CSV format often results in date columns being interpreted as text upon re-import, or numeric columns losing precision. This issue arises because formats like CSV lack built-in metadata to store data types, meaning they rely on the importing program to infer types, which can cause inconsistencies and data integrity issues over time. In these cases, columns may need to be re-typecast.\n\n\n\n7.3.2 Resources\n\nPlotting and Programming in Python: Data Types and Type Conversion\nR for Data Science - Transform",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "href": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "title": "7  Quality & Preparation",
    "section": "7.4 Preparation for Analysis",
    "text": "7.4 Preparation for Analysis\nAdditional cleaning and transformation steps (often referred to as “data wrangling”) are often necessary, and are highly variable depending on project needs. Examples include:\n\nfiltering based on criteria\nrestructuring/reshaping/pivoting\nremoving duplicates\ncorrecting errors in the source data (e.g. misspellings)\nmerging\n\nThe approach depends on specific project and data needs. The following resources go into greater detail, with examples:\n\nR\n\nNCEAS Learning Hub’s coreR Course - 7 Cleaning & Wrangling Data (ucsb.edu)\nTransform – R for Data Science (2e) (hadley.nz)\nR for Reproducible Scientific Analysis: Subsetting Data (swcarpentry.github.io)\nR for Reproducible Scientific Analysis: Data Frame Manipulation with dplyr (swcarpentry.github.io)\n\nPython\n\nData Analysis and Visualization in Python for Ecologists: Indexing, Slicing and Subsetting DataFrames in Python (datacarpentry.org)\nData Analysis and Visualization in Python for Ecologists: Combining DataFrames with Pandas (datacarpentry.org)\n\n\n\n\n\n\nHutchinson, V. B., T. E. Burley, M. Y. Chang, T. A. Chatfield, and R. B. Cook. 2015. “USGS Data Management Training Modules—Best Practices for Preparing Science Data to Share: U.S. Geological Survey.” https://doi.org/10.5066/F7RJ4GGJ.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html",
    "href": "docs/data-management/archival.html",
    "title": "8  Archival & Disposal",
    "section": "",
    "text": "8.1 Archival\nArchiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Archival & Disposal</span>"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#archival",
    "href": "docs/data-management/archival.html#archival",
    "title": "8  Archival & Disposal",
    "section": "",
    "text": "ensures long-term and secure storage to projects for reproducibility and reuse,\nimproves organization, accessibility, and usability of both active and completed project files, and\nreleases computational resources for active projects, reducing energy consumption and storage costs.\n\n\n\n\n\n\n\nNote\n\n\n\nArchival takes place when projects are complete. To preserve the state of code and data at major milestones, such as journal article publication, see Publication.\n\n\n\n8.1.1 How to archive data at RFF\n\n\n\n\n\n\nNote\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to specified team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).\n\nStep 1: Finalize data organization\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code. However, use discretion: in some cases, intermediate data that are likely to be used again and are time-consuming to re-create should be retained.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see Publishing/Archiving data sourced from APIs.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\none project-level README\nall raw data README files\nany metadata files\n\n\n\nThis is generalized guidance. For additional guidance choosing which files to archive, see Decide what data to preserve.\n\n\nStep 2: Finalize documentation\nEnsure the project-level README file, raw data README files, and any metadata files are up to date.\n\n\nStep 3: Coordinate with IT to ensure long-term folder access\nContact IT at IThelp@rff.org to arrange and configure archival storage of the folder. Include the following with the email:\n\nProject-level README file\nList of researchers that should retain folder access\nApproximate folder size (e.g., 5 GB)\nThe nature of sensitive/proprietary datasets",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Archival & Disposal</span>"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#disposal",
    "href": "docs/data-management/archival.html#disposal",
    "title": "8  Archival & Disposal",
    "section": "8.2 Disposal",
    "text": "8.2 Disposal\nSome data may need to be deleted to protect sensitive information or comply with regulations, data agreements, or funder requirements. This is often referred to as data disposition. If any of these requirements applies to a project, follow these best practices when deleting data:\n\nVerify Requirements: Confirm funder agreements and legal obligations regarding data retention and deletion.\nSource Deletion: Confirm with IT that the files were fully deleted in accordance with requirements (e.g., backup files).\nDocumentation: Record when and how data was deleted.\n\nThese practices should apply to data stored the RFF network, OneDrive, or Microsoft Teams.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Archival & Disposal</span>"
    ]
  },
  {
    "objectID": "docs/software/index.html",
    "href": "docs/software/index.html",
    "title": "Software Quality",
    "section": "",
    "text": "Content coming soon.\nExisting style guide resources:\n\nR: Tidyverse Style Guide by Hadley Wickham\nPython: Google Python Style Guide\nStata: Suggestions on Stata programming style by Nicholas Fox\nOther languages: Google style guides for other languages",
    "crumbs": [
      "Software Quality"
    ]
  },
  {
    "objectID": "docs/version-control/index.html",
    "href": "docs/version-control/index.html",
    "title": "Version Control (Beta)",
    "section": "",
    "text": "What is version control and what can it do for you?\nVersion control is a system that records changes to a file or set of files over time so that you can recall specific versions later. You know how Microsoft Word will periodically save your file, and you can recover previous versions if you really mess up? That is a simple form of version control. Other forms offer far more functionality, including the ability to save and recover versions of an entire project, tools for collaboration, and more.\nSome commonly used version control systems are Git, Mercurial, and SVN. Here at RFF, we strongly advise using Git, which we will discuss for the remainder of the section. We also recommend using GitHub to host Git repositories online. Here are some of the benefits of using Git with GitHub for version control:\nThis section of the guidance will help you get started in making version control a standard part of the project life cycle.",
    "crumbs": [
      "Version Control (Beta)"
    ]
  },
  {
    "objectID": "docs/version-control/index.html#what-is-version-control-and-what-can-it-do-for-you",
    "href": "docs/version-control/index.html#what-is-version-control-and-what-can-it-do-for-you",
    "title": "Version Control (Beta)",
    "section": "",
    "text": "Grants peace of mind knowing work is stored safely and can be recalled with minimal effort, which gives greater liberty to test out new ideas (even when a computer breaks!)\nFacilitates collaboration by allowing multiple versions to coexist and efficiently borrow code from one another\nEnables efficient review and discussion of code changes before incorporating them into the main branch of the repository\nDocuments when, why, and by whom specific changes were made, which helps with debugging and troubleshooting\nConsolidates project code and documentation in the same, easily-accessible, backed-up location.\nPreserves easy web browser access to all versions of code.\nOrganizes documentation so it can be kept alongside the code itself\nProvides public access to datasets and code under the auspices of a software license\nEmpowers audiences to answer their own questions about the data and methods used in publications, reducing overhead for researchers who would otherwise be given that task",
    "crumbs": [
      "Version Control (Beta)"
    ]
  },
  {
    "objectID": "docs/version-control/index.html#version-control-basics",
    "href": "docs/version-control/index.html#version-control-basics",
    "title": "Version Control (Beta)",
    "section": "Version Control Basics",
    "text": "Version Control Basics\nBefore we get started in using Git for version control, let’s cover a few basic principles. Git is a popular version control system, which is software that we can download and use on our computer. Git works by monitoring the changes of the contents of an otherwise ordinary file folder, called a repository. In a Git repository, a user can tell Git which file changes to keep, which to discard, and can label those changes, go back to previous file versions, and much more!\nWhile it is possible to use Git only on your computer without posting it online, it is often advisable to host repositories online so that they are synced in the cloud, making it easy to share them and back them up. There are many websites that can host Git repositories, but the most popular is called GitHub, which is what we recommend using at RFF. (Some others you may come across GitLab and BitBucket) Not only does GitHub host these repositories, but it also provides convenient ways to host documentations, discuss code changes, and report bugs.",
    "crumbs": [
      "Version Control (Beta)"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html",
    "href": "docs/version-control/setup.html",
    "title": "9  Setup",
    "section": "",
    "text": "9.1 Make a GitHub Account\nBefore using GitHub, it is necessary to have a GitHub account. To make an account, go to the GitHub website and click Sign Up, following the prompts to create your account. As part of the process you will need to verify your email address. Note that GitHub accounts are associated with people, along the lines of a LinkedIn account, and it may be helpful to have a single personal GitHub account even if you plan to use it outside of your work at RFF. It is simple to associate multiple email addresses with your account later on, so it is unimportant which email address you create the account with.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#git-installation",
    "href": "docs/version-control/setup.html#git-installation",
    "title": "9  Setup",
    "section": "9.2 Git Installation",
    "text": "9.2 Git Installation\nIt is necessary to install Git onto any computer from which you wish to interact with local repositories. Normally, IT has installed Git on RFF computers. If not, please send a request to IThelp@rff.org to request a git install on an RFF machine. For personal computers, you may install manually following these instructions.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#choose-a-user-interface",
    "href": "docs/version-control/setup.html#choose-a-user-interface",
    "title": "9  Setup",
    "section": "9.3 Choose a User Interface",
    "text": "9.3 Choose a User Interface\nWhen you install Git, it generally will already install a couple of applications you can use to interface with git.\n\nGit Bash/CMD – (Installed with Git) These options allow you to enter text-based commands for Git in an interface similar to the command prompt window in Windows. Git Bash allows for git commands in addition to standard UNIX commands, whereas Git CMD allows for git commands in addition to standard windows CMD prompt commands. This type of interface is the industry standard, so it is what we suggest for new git users, and we will use Git Bash for tutorials on this site.\n\n\n\n\nGit GUI – (Installed with Git) This is a minimalist graphical user interface letting you use git without having to use a command line. It also allows for simplistic visualization of branches. While our tutorials will use Git Bash, it should be fairly straightforward to use the Git GUI instead.\n\n\n\n\nRStudio Git Integration - (Installed Separately) RStudio, a popular editor for R code, comes with a Git user interface built in. This may be a good option for R users once they have learned the fundamentals with Git Bash.\n\nIf you wish to learn a different user interface, check out the list here, and check with the IT team before installing. Most of the concepts will be fairly similar to what we share in the guide.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#optional-set-up-ssh-keys",
    "href": "docs/version-control/setup.html#optional-set-up-ssh-keys",
    "title": "9  Setup",
    "section": "9.4 Optional: Set up SSH Keys",
    "text": "9.4 Optional: Set up SSH Keys\nIf you will be using Git with private repositories, it is worth setting up a key on your PC so that you can freely pull and push from a remote repository without having to enter your password each time. Follow these instructions for setting up an SSH key, and these instructions for adding it to your GitHub account.\nThere is a small configuration change we need in order to enable the SSH protocol to work with RFF IT systems. From a git bash window, run the following lines:\n\ntouch ~/.ssh/config - This command creates a new empty file in your user home folder /.ssh directory.\nprintf \"Host github.com\\n  Hostname ssh.github.com\\n  Port 443\" &gt;&gt; ~/.ssh/config - This command writes 3 lines to the newly created config file.\n\nNote that these commands can be run from a Git Bash window open to any directory. The ~ sign represents your home directory. After running the commands, if you were to open the file in a text editor, it would look like:\nHost github.com\n  Hostname ssh.github.com\n  Port 443\nOnce you have that set up, when cloning a repository to your local folder, simply select the “SSH” option before copying the URL.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial.html",
    "href": "docs/version-control/tutorial.html",
    "title": "10  Tutorial",
    "section": "",
    "text": "10.1 First Steps\nBefore you begin this tutorial, make sure you have:",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tutorial</span>"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial.html#first-steps",
    "href": "docs/version-control/tutorial.html#first-steps",
    "title": "10  Tutorial",
    "section": "",
    "text": "10.1.1 Introduce Yourself to Git\nFirst, open Git Bash. You can hit the windows key and type “bash” and it should pop up for you to open.\nIn the Git Bash window, you can type all kinds of commands, including Git commands, which are always prependended with “git”, or bash commands that are commonly run in terminals.\nLet’s start with:\ngit --version\nIt should have printed out what version you are running, something like git version 1.47.0.windows.2. Now, let’s introduce ourselves to Git:\ngit config --global user.name 'Your Name Here'\ngit config --global user.email 'your-name-here@rff.org'\ngit config --global --list\nNow, Git will include your name and email when you publish changes.\n\n\n10.1.2 Creating a Repository in GitHub\nThis section is useful for learning, but you can also skip to the next section on cloning repositories if you already have a repository (i.e. for a project) that you’d like to clone.\n\nTo create a repository hosted on GitHub, first navigate to github.com and click the plus button in the top right corner, then click the “new repository” button.\n\n\n\n\nNext, you may select who you would like to own the repository. It could either by owned by your user account, or an organization you are a member of. This may be something to discuss with your project team. The repository owner will show up in the URL of the repository, in the structure of: https://github.com/&lt;owner-name&gt;/&lt;repository-name&gt;. For example, for the E4ST.jl repo created in the e4st-dev organization, the URL is: https://github.com/e4st-dev/E4ST.jl. If in doubt, you can specify yourself as the owner, and change ownership later on. See the Workflows section on repository ownership for more information.\n\n\n\n\nNow you must enter the repository name. We recommend all lowercase names with dashes separating words, like my-repo-name. The only exception is when a programming language’s best practices prescribe a specific repository naming convention. (i.e. julia package repositories are supposed to be camelcase as in MyRepoName.jl)\nNow you must choose whether the repository is to be public or private. Generally, choose private for repositories that will contain sensitive information, and public if the project requires it to be public. It is easy to change from private to public later on, so when in doubt choose private. See the public/private section in Workflows for more information. If you have chosen to make your project public, you will need to also select which license to use. See the section on software licenses to help make this decision.\nCheck the box to add a README file. This will create a file called README.md located in the repository’s root folder, where you can add basic documentation for the repository.\nOptional: Choose a .gitignore template from the dropdown menu. A .gitignore file specifies certain file types that Git will not track the changes of, by default. For example, it is best to ignore an auto-created config file made by R studio that is user-specific. Generally it is a good idea to select the .gitignore template for the programming language you will be using.\nClick the “Create Repository” button!! This should take you to the home page of your new repository.\n\n\n\n10.1.3 Cloning Your First Repository with Git\nNow that we’ve made a remote repository, let’s get it copied onto our computer. Copying a remote Git repository is called cloning. This process will work the same way for any existing repository, including the one that we made in the steps above.\nFirst, it’s important to choose a good file location to store the git repository. While it’s not required, many people like to have a single folder to store all of their Git repositories. An alternative would be to store the Git repository in an associated project folder.\nGit Bash has a working directory, which is the location that it is operating in. To see what the current working directory is, we can enter the pwd command, which stands for primary working directory.\npwd\nNow lets try changing the working directory with the cd command, to whatever directory you would like to store your Git repository in. For example:\ncd 'C:/Users/&lt;my-user-name&gt;/OneDrive - rff/repos'\n\n\n\n\n\n\nNote\n\n\n\nIt is only necessary to use quotes for filenames in bash if there is a space in the path, as in the example above.\n\n\nNow, in an internet browser, navigate to the home page of the repository you wish to clone.\nClick the button labeled &lt; &gt; Code, then copy the URL to clipboard by clicking the logo with intersecting squares.\nIn the Git bash window, type git clone then paste in the repository URL from your clipboard by right-clicking. Altogether this would look like:\ngit clone https://github.com/&lt;owner-name&gt;/&lt;repository-name&gt;\nIf the repository is private, Git Bash may prompt you for your GitHub credentials. After entering them, you should be left with the cloned repository located in the working directory!\n\n\n\n\n\n\nNote\n\n\n\nIf you have set up SSH keys as in the optional setup section, you can copy the SSH address for the repository. To do this, after clicking the &lt; &gt; Code button, select SSH before copying the URL to your clipboard. If properly set up, when cloning from an SSH URL, you should not be prompted for GitHub credentials, even for a private repository.\n\n\nFinally, you can navigate into that newly created repo using the cd command:\ncd &lt;repository-name&gt;\n\n\n10.1.4 Making and Publishing File Changes\nNow that we have a Git repository on our computer, let’s explore the process of making file changes and saving versions. To give a conceptual outline, there are four steps.\n\nPull changes from others. This is to ensure we are working on the latest version of the repository. git pull\nMake changes to files. You can do this with any text editor.\nStage changes. This tells Git which changes you would like to select for the next version. git add\nCommit staged changes. This publishes any staged changes as a new version. git commit\nPush commit(s) to the remote/online repository. git push\n\nLet’s walk through that process together. In Git Bash, type git status to show the status of the repository. It should give you a short report about which branch you are on (more about branches later), whether your local copy is up to date with the remote copy, and any file changes you’ve made.\nOn branch main                                 # this is the default branch\nYour branch is up to date with 'origin/main'.  # the local version matches the version in GitHub\n\nnothing to commit, working tree clean          # we haven't made file changes\nAt this point, it is ALWAYS a good idea to run the git pull command to make sure we are up to date with the latest version. This command first checks to see if the remote repository version (the version stored in GitHub) has had any commits since the last time we pushed or pulled. If there are any, it will download them incorporating those changes into your working version.\nNow, let’s open up the README file and make some changes.\nYou can either navigate to the file in Windows File Explorer or type notepad README.md to open the README file in the notepad text editor. Add a few lines to your README, then save and close the file. In the future, you can use whatever editor you prefer, such as RStudio, MATLAB, Visual Studio Code, etc. After making that change, let’s type our git status command into Git Bash again.\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nThis message is telling us that there are some file changes that have not yet been staged for commit. Let’s stage the changes from the README file with the command git add README.md.\n\nYou can use the command git add -p in order to interactively select which files, and even sections of files you would like to stage for commit.\n\nNow let’s check the status once more:\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   README.md\nNow, our README.md file is under the heading Changes to be committed, so it is ready to be published. Let’s publish with the command git commit. It should open up notepad and prompt you to enter a commit message. In the first line, enter a concise commit message describing the changes that you are publishing, more info on that here. Save and close the file, and you have successfully authored your first commit!\n\nIf you find typing your commit message in notepad arduous, we have the command for you!\ngit commit -m \"Simply type your messsage here\"\nOR if you want to be even more lazy, see the section for the commit alias\n\nThe last step is to push commits to the online repository. At this point, git status indicates that the local version of the repository is 1 commit ahead of the online version. To send the commits to the online repository, we will simply enter the command git push! Now, if you navigate to the online version of the repository in GitHub, you should be able to see the changes you made. Congratulations!!\n\n\n10.1.5 Handling Conflicts\nHave you ever been working on a Word document with another person, and maybe your computer gets disconnected, and you accidentally both edit the same paragraph? Usually Word will get smart and show both sets of changes and allow you to choose which one you would like. Sometimes, the same thing happens in Git/GitHub.\nSay, for example, my collaborator pushes a commit to our project repository without me realizing. In an ideal world, I would run the git pull command to make sure I have that commit incorporated into my local copy. However, maybe I forgot to pull, or my collaborator pushed that commit as I was already making file changes. Then, I commit my changes and push them and am greeted with the following ugly message:\n$ git push\nTo github.com:Ethan-Russell/MyTestRepo.jl.git\n ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'github.com:Ethan-Russell/MyTestRepo.jl.git'\nhint: Updates were rejected because the remote contains work that you do not\nhint: have locally. This is usually caused by another repository pushing to\nhint: the same ref. If you want to integrate the remote changes, use\nhint: 'git pull' before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nDon’t panic! Git is built for this. This message is Git telling us that we can’t push because of the other commit, and it also tells us that the solution is to git pull before pushing again. When we run git pull, Git will try to automatically merge the changes for us. Ideally, if changes are made in separate sections of the file (or different files altogether), Git will be able to merge everything, and you can simply git push again to push your commit(s). However, if both versions have changed the same file, Git will flag it as a conflict and it will enter “MERGING” mode (denoted in git bash to the right of the prompt). This is the type of message Git will print if there is a conflict:\n$ git pull\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nUnpacking objects: 100% (3/3), 947 bytes | 55.00 KiB/s, done.\nFrom github.com:Ethan-Russell/MyTestRepo.jl\n   6449e68..a3b9c32  main       -&gt; origin/main\nAuto-merging README.md\nCONFLICT (content): Merge conflict in README.md\nAutomatic merge failed; fix conflicts and then commit the result.\nFortunately, Git has identified which file(s) have conflicts: in this case, only the README.md file has a conflict. So let’s open the file where there is a conflict. Git identifies which lines have conflicts by adding some helpful syntax, which we can search for to find the conflict. You can do a search for “====” and that will take you to the conflict:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# My Test Repo\n=======\n# MyTestRepo\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; a3b9c323e36238db4a26a91addc5c699add04b6a\nGit will list the local version first, right after printing &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, followed by a line of =======, then the version that was pulled in from the remote repository, then &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a unique identifier for the commit in the remote repository. Translating the above chunk of text, it looks like someone else had added the title MyTestRepo to the README.md file, and I added the title My Test Repo. All we need to do here is:\n\nSelect which version of the changes we want (or we could edit it to be a combination of the two!)\ndelete the other version, as well as all of the notation that Git has added (&lt;&lt;&lt;, ===, &gt;&gt;&gt;)\nSave and commit the resulting file.\n\nIn the example above, I would change that portion of the file to be:\n# MyTestRepoFinalName\nAfter adding and committing that change, we can then safely run git push.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tutorial</span>"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial.html#branching",
    "href": "docs/version-control/tutorial.html#branching",
    "title": "10  Tutorial",
    "section": "10.2 Branching",
    "text": "10.2 Branching\nThis is one of the most amazing features of Git. Git allows us to create different working versions, or branches of our repository. This lets me take a snapshot of the repository and try out a new idea without affecting my main branch. Then, once I (and all my collaborators) are satisfied with the changes in my new branch, we can merge it back into the main branch. This is how the majority of modern software is developed. It allows easy review of the portions of the code that have changed, and gives me confidence that my changes are not messing up the main branch until they are fully developed.\n\n\n\n\n\n---\ntitle: Example Git Branching\n---\ngitGraph\n   commit id: \"initial commit\"\n   commit id: \"starting point for the new feature\"\n   branch my-feature\n   checkout main\n   commit id: \"ongoing development of main\"\n   checkout my-feature\n   commit id: \"trying new feature\"\n   commit id: \"perfecting new feature\"\n   checkout main\n   merge my-feature id: \"now it is merged!\"\n   commit id: \"continued development\"\n\n\n\n\n\n\nHere’s an overview of the process:\n\nPull any code updates! git pull\nCreate and check out a new branch with an appropriate branch name. git branch and git checkout\nMake and push commits to the new branch. git add, git commit, and git push.\nWhen ready to merge, create a Pull Request from GitHub.\nReview the Pull Request.\nMerge the Pull Request.\nCheck out and pull main branch in Git Bash. get checkout and git pull\n\n\n10.2.1 Creating a Branch\nYou may have realized that Git creates a branch by default, called main. You could think of the main branch as the trunk of a tree. (If you encounter any older repositories, the default was the master branch, but the default was changed in mid-2020). You may notice that Git Bash has the branch name main written to the right of the prompt, which designates that main is the active branch, or the branch that is currently “checked out”.\n\n\nTo create a new branch, simply enter the command git branch &lt;my-branch-name&gt;. Generally, branch names should be: * concise yet descriptive. plot-results &gt; plot-all-results-with-numpy * lowercase and hyphen-separated. plot-results rather than PlotResults or plot_results * no special characters (numbers, letters, and hyphens only) * See more info in the branch naming tip\nYou will notice that after running the git branch command, the main branch is still designated as the active branch. To work on the newly created branch, simply use the command git checkout &lt;new-branch-name&gt;.\nAfter running that command, your new branch should be active.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use the following command to create a new branch and check it out in one step:\ngit checkout -b &lt;new-branch-name&gt;\n\n\nNow that your new branch is active, you can safely make changes and commits to this branch without worrying about modifying the main branch. When you push the branch, Git will give you an error saying that there is no “upstream” branch for the branch you are pushing. You can simply run the command it gives you, i.e. git push --set-upstream origin &lt;new-branch-name&gt;, and it should push with no problems.\n\n\n10.2.2 Merging a Branch\n\nCreating a Pull Request\nOnce you have made some commits/changes and are ready to merge the new branch back into the main branch, now it is time to make a Pull Request in GitHub. To do that, navigate to the repository GitHub page, and click the “Pull requests” tab, shown below.\n\n\nNow select the “New pull request” button at the top of the page. We want to select the main branch as the “base” branch, and our new branch as the “compare” branch, as shown below:\n\n\nNow to finish creating the Pull Request, click the “Create pull request” button on the right side of the page. You may enter a title and description for the Pull Request, which is helpful for contributors to know what is contained in the Pull Request, and you could even include instructions for the contributors who you would like to review the Pull Request. Clicking the “Create pull request” button at the bottom will finish creating the request.\n\n\nReviewing and Merging a Pull Request\nPull requests provide a great opportunity for collaborators to review one anothers’ code, to ensure accuracy. Code review can be daunting, especially if there are only changes to specific parts of a larger codebase. GitHub makes this process very easy, by identifying lines of code that have changed, and allowing collaborators to comment on them.\nTo review a pull request, the reviewer would navigate to the Pull Request and click on the “files changed” tab. From there, they can see each of the lines added, changed, or deleted, and insert comments on particular lines of code. After reviewing all the files changed, they can complete their review and choose whether or not to approve the pull request. To learn more about reviewing Pull Requests, see the appropriate section of the GitHub documentation.\nIf the reviewer has requested changes to the code based on their review, you can simply make changes, commit, and push to the same branch, and the Pull Request will be updated with those changes. Once the Pull Request has been reviewed and approved, you can go back to the “Conversation” tab of the Pull Request and click the “Merge pull request” button at the bottom of the page, and your branch’s changes will be reflected in the main branch!\n\n\nConflicting Branches\nSometimes, when we try merging our new branch into the main branch, the Pull Request indicates that there have been changes made to the main branch that conflict with the changes in the new branch. When this happens, let’s go back to Git Bash and merge any new changes from the main branch into our branch, then resolve any potential conflicts. To do this, let’s pull updates to main and then merge main into our new branch by running the following commands:\ngit checkout main\ngit pull\ngit checkout &lt;new-branch-name&gt;\ngit merge main\nTo handle these conflicts, use the process described in the Handling Conflicts section to resolve them.\n\n\nMerging Branches without Pull Requests\nMerging with a Pull Request may, at times, feel a bit unwieldy, especially if you are in a hurry or working on a project by yourself. It is still possible to merge into the main branch without making and reviewing a Pull Request. We just advise caution with this method because it does not facilitate easy review, and so could easily introduce mistakes into the main branch. If you are confident that the branch is ready to merge without additional review, in Git Bash, simply enter the following commands:\ngit checkout main\ngit pull\ngit merge &lt;new-branch-name&gt;\nIf any conflicts come up, follow the instructions in the Handling Conflicts section. Once you have dealt with the conflicts (if any), you are done, and can push the main branch, which now has updates from your new branch.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Tutorial</span>"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html",
    "href": "docs/version-control/workflows.html",
    "title": "11  Workflows",
    "section": "",
    "text": "11.1 Key Principles\nNow that we know some basic Git principles, how do we make a workflow for a given project? First, let’s start by talking about some key principles for any workflow, then we can get into some of the specific considerations for a project team at RFF.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html#key-principles",
    "href": "docs/version-control/workflows.html#key-principles",
    "title": "11  Workflows",
    "section": "",
    "text": "Users should commit changes frequently, with descriptive messages.\n\nFrequent commits make it so that it is easier to find discrete check-points in the repository, rather than having everything committed at the same time.\nTry to have commits provide incremental improvements. For example, a commit might include the addition of a single function, or a 2-line fix to an existing script.\nIf you find yourself coding for hours between commits, consider trying to break up your task into discrete sub-tasks.\n\nPull often to ensure that you are incorporating the changes of others.\nA local clone of a repository should generally ONLY have a single user.\n\nThis is why in the Directory Structure section of the Data Management guidance, we recommend that each user works from a repository clone stored in their own individual subfolder within the L:/ drive project folder.\n\n\nTake care with encoding file paths within a repository. You want other people to be able to use the repository without having to move files around or change all the file paths in the repository. You could either:\n\nUse relative file paths to point to other files that are located in the repository.\nUse absolute file paths (i.e. on a shared network drive like the L drive) and consider making a variable for the path directory, especially if used multiple times. That way, if a collaborator has their data at a different path on their system, they only need to change a single path.\ndata_path = “L:/ProjectName/Data”\ndata_file_1 = joinpath(data_path, “data_file_1.csv”)\ndata_file_2 = joinpath(data_path, “data_file_2.csv”)\nIf you use relative file paths pointing to files outside the repository, specify in the project README how the external files and the repository should be positioned relative to each other.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html#workflow-planning",
    "href": "docs/version-control/workflows.html#workflow-planning",
    "title": "11  Workflows",
    "section": "11.2 Workflow Planning",
    "text": "11.2 Workflow Planning\nWhen a project team is deciding to use Git, it is important to think through the following considerations. We recommend answering these questions at the beginning of a version-controlled project.\n\n11.2.1 Where should each team member store their local repositories?\nHere are two examples of how a project / team might choose to store their repositories.\n\nGenerally Recommended: Each team member keeps their own working version of a repository on a shared drive, i.e. the L drive, which could (but probably shouldn’t) be accessed by other team members., i.e. inside of a larger project directory\n\nPros: individual users’ repositories are available on all lab computers; repos are stored within the project folder, allowing easy access to shared project files (e.g. raw or processed data files)\nCons: files could be accidentally changed by others if they are working in the incorrect version of the repository\n\nEach team member keeps their own working version of a repository somewhere private, either on the C:/ drive of their personal computer, or on the RFF servers, with the intention of no other team members having access to it. This is suitable for teams that have many repositories where it would get very confusing to store all the repositories in the same location for the whole team.\n\nPros: it is “clean”, with no accidental file changes from other users.\nCons: It would require care to be taken for access (file paths) to non-version-controlled data files.\n\n\n\n\n11.2.2 Should our repository be hosted by a GitHub Organization or User?\n\nDo you work on a team with their own GitHub Organization (i.e. E4ST)?\n\nIf so, verify with the other members of the team that it would be acceptable to use the organization to host the repository\nIf your team doesn’t already have one, does it make sense to create an organization for your team? Are there likely to be multiple repositories that would make sense to group under the same organization? If so, follow the steps in the GitHub Documentation for creating organizations.\n\nDoes this repository belong in the RFF Organization?\n\nGenerally, if the repository is associated with a publication, the code should be forked or transferred to the organization.\nWe suggest creating the repository as a personal repository and changing ownership or forking.\nSee the Data Management section on code publication for additional guidance.\n\n\n\n\n11.2.3 Should our repository be public or private?\nPublic repositories are visible to anyone, whereas private repositories allows for controlled access. For public repositories, it is still possible to control who is able to contribute to them, while private reporitories allow you to control who can even see them.\nGenerally, select for your repository to be private for repositories that will contain sensitive/proprietary information, and public if the project requires it to be public. Availability of data/code is important to ensure reproducability so making a repository public is a good thing in most cases. If you have sensitive data but would like for the repository to remain public, consider storing code and documentation in the repository, and sensitive data in a different way. See the Sensitive and Proprietary data section It is easy to change from private to public later on, so when in doubt choose private.\nIf you have chosen to make your project public, you will need to also select which license to use. See the section on software licenses to help make this decision.\n\n\n11.2.4 Should our team use branches?\nTo learn about branches, what they are for, and how to use them, see the Branching section of the Tutorial. There are a few aspects to consider with this question.\n\nHow many people will be making commits?\n\nIf only one person will be making commits, it may save time and effort to maintain a single branch, without creating additional branches.\nWith more people making commits, using multiple branches may help avoid unnecessary merge conflicts.\n\nIs it important to have a fully functional version at all times?\n\nBranches can be very good for preserving a functional version of the code, while allowing for risk-free experimental features to be developed.\n\nIs it important to have code review for this project?\n\nBranches facilitate streamlined code review, where reviewers only review sections that have changed (via Pull Requests).\n\n\nHere are some examples of when a branch might be useful:\n\nMajor Changes/Experimental Feature: Say you have an idea on how to improve your code, and it requires significantly re-vamping the code. It would make sense to create a branch for this experimental feature, so that you can develop the feature without worrying about making changes to the main branch. Then the whole team can use the Pull Request to evaluate whether or not they’d like to merge into the main branch.\nNew Team Member: You have an existing model that is already working well. A new researcher joins the project team and is tasked with a code improvement. They are new to the model, so you want to review their changes carefully before applying them to the main branch. It makes sense to have them create a branch for their development, and review it with the pull request.\nConcurrent Development: Your team has a couple of features that need to be developed in tandem. To prevent extensive conflicts and streamline the process of combining the new features, each feature gets its own branch.\nProject: If your team has a project that requires making project-specific changes, it could be a good idea to create a project branch, with no intention to merge the project branch into main. ⚠️However, for features that would be useful to have in the main branch, implementing those features in the project branch would make it difficult to merge them back into the main branch later on. In that case, it is best to implement those features in the main branch or in a separate branch (to be merged with main), and merge the main branch back into the project branch. It is difficult to selectively merge changes from the project branch into the main branch.\n\n\n\n11.2.5 If using branches, how do we want to handle code review?\nIt would be good to talk about the following questions with your team:\n\nShould code review be required for every change to the code?\n\nOne example of this would be requiring that every change be made in a branch and submitted via Pull Request, and that at least one person who did not author the Pull Request must approve it in order for the branch to be merged into the main branch.\nYour team may decide to only require a Pull Request and code review for major features.\n\nWho is allowed/required to approve Pull Requests?\n\nOne example would be to require every Pull Request created by a newer team member to be reviewed by a more experienced team member.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html",
    "href": "docs/version-control/faq.html",
    "title": "12  Tips and FAQ",
    "section": "",
    "text": "12.1 FAQ",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tips and FAQ</span>"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html#faq",
    "href": "docs/version-control/faq.html#faq",
    "title": "12  Tips and FAQ",
    "section": "",
    "text": "What types of files can I store in my repository?\nIn general, git works well with:\n\nSmall files. It is not advisable to store large data files in git, because it makes the repository large and subsequent operations get slow and unwieldy. For storing large files, see the Storage Options section\nHuman-readable files. One of the biggest perks of version control is the ability to see specific changes to files. That feature only works for files composed of human-readable characters, examples of which include comma-separated values (.csv) and plain text (.txt), as well as source code. However, there are some commonly-used filetypes that are not human-readable, and thus do not work very well with version control, including:\n\nAny Microsoft office format (.docx, .xlsx, .pptx, etc.)\nMATLAB notebooks (.mlx)\n\n\n\n\nWhat if there is a file in my local repository that I don’t want in the remote repository?\nGreat question! If I may want to add that file to the repository later, I could simply avoid staging it for current commit, and stage it when it is ready later. If that file or folder should never be tracked, you can also create a file called .gitignore in the repository. Inside the file, you can simply add the names of the file(s), folder(s), or file extension types (i.e. *.log) that you wish for Git to ignore.Generally, it is good to ignore files/folders that get automatically generated by existing scripts, are too big for git, or are not relevant to other users of the repository. Here are some examples of files that you may consider adding to a .gitignore file:\n\na folder containing intermediate data files\na folder containing dependencies that get downloaded using a script\na project file created by a code editor\n\n\n\nWhat if I don’t have anyone who can review my code or pull request?\nThe importance and benefits of code review was presented in the Software Quality section of the guidance &lt;TODO: LINK TO SUBSECTION&gt;. If you don’t have anyone working on a project with you who is available to review your code, feel free to reach out to &lt;TODO: figure out if this is something we can support&gt;",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tips and FAQ</span>"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html#tips",
    "href": "docs/version-control/faq.html#tips",
    "title": "12  Tips and FAQ",
    "section": "12.2 Tips",
    "text": "12.2 Tips\n\n12.2.1 Commit Messages\nIf you want to learn to write better commit messages, this is a great resource!\n\n\n12.2.2 Branch Naming\nHere is a great resource for learning more about branch naming conventions.\n\n\n12.2.3 Tags\nSometimes it is helpful to name a specific commit of a repository. For example, say you have a version of the repository that you used to generate analysis for a publication. You can easily use the git tag command to “name” that specific commit. For example,\ngit tag &lt;tag-name&gt;\ngit push --tags\n\n\n12.2.4 Aliases\nAn Alias is like creating a shortcut for a git command so that you don’t have to type out the whole command.\nFor example, we type git status fairly often and it would be nice to simply type git st. We can tell Git that we want to create an alias called git st that simply executes git status instead. Here is how we tell Git Bash to save that into its configuration file.\ngit config --global alias.st status\n\nCommit Message Alias\nThis alias makes it easier to type a commit message without opening notepad.\n\nSetup\ngit config --global alias.cm 'commit -m'\n\n\nUsage\ngit cm \"Commit message here\"\n\n\n\nTree Visualization\nGit Bash offers tools to visualize commits/branches. Usually these are based on the command git log, which has many different options for customization. Below is a customized git log command that is nicely color-coded, shows user, date, and commit message.\ngit log --graph --oneline --exclude=origin/gh-pages --branches --pretty=format:'%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s' --date=short\n\n\nAs you can see, it prints the commit hash, the date of the commit, the author, the name of the branch, and the commit message, as well as the branching structure at the far left.\n\nSetup\nTo make an alias for it with the name tree, enter the following command in Git Bash:\ngit config --global alias.tree \"log --graph --oneline --exclude=origin/gh-pages --branches --pretty=format:'%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s' --date=short\"\n\n\nUsage\nNow, to print the visualization, simply enter:\ngit tree\ngit tree -10     # this option makes it only print the latest 10 commits.",
    "crumbs": [
      "Version Control (Beta)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tips and FAQ</span>"
    ]
  }
]