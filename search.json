[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Guidance for Researchers",
    "section": "",
    "text": "Home",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-this-resource",
    "href": "index.html#about-this-resource",
    "title": "Data Guidance for Researchers",
    "section": "About this resource",
    "text": "About this resource\nThis guidance is designed to help RFF research teams:\n\nSave time through clear data practices, templates, and reusable workflows\nIncrease flexibility for collaboration, future reuse, and reproducibility\nReduce risk by supporting consistent and transparent workflows\n\nIt includes practical support to:\n\nBuild foundational skills and concepts\nPlan and manage data-driven research from start to finish\nNavigate RFF-specific systems like storage and access\nSet up new projects effectively\nImprove existing workflows",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-outline",
    "href": "index.html#site-outline",
    "title": "Data Guidance for Researchers",
    "section": "Site Outline",
    "text": "Site Outline\n\nFoundations\nData Management\nSoftware Quality\nVersion Control",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-and-feedback",
    "href": "index.html#questions-and-feedback",
    "title": "Data Guidance for Researchers",
    "section": "Questions and feedback",
    "text": "Questions and feedback\nThis is a living resource — it will continue to evolve as needs shift and feedback is incorporated.\nTo submit questions, bugs (e.g., broken hyperlinks), suggestions, or feedback on this guidance, click Report an issue on the right-hand side of this page and submit an issue to the repository. Note that your comment will be publicly visible.\nTo submit a question or comment over email, reach out to the Data Governance Working Group.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-authors",
    "href": "index.html#site-authors",
    "title": "Data Guidance for Researchers",
    "section": "Site authors",
    "text": "Site authors\nMembers of the RFF Data Governance Working Group\n\nAris Awang\nPenny Liao\nEthan Russell\nJohn Valdez\nMatthew Wibbenmeyer\nJordan Wingenroth\nAlexandra Thompson",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/foundations/index.html",
    "href": "docs/foundations/index.html",
    "title": "Foundations",
    "section": "",
    "text": "Introduction\nGood data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project. Good data practices provide (Langseth et al. (2015)):",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#introduction",
    "href": "docs/foundations/index.html#introduction",
    "title": "Foundations",
    "section": "",
    "text": "Short-term benefits\n\nAllow you to spend less time doing data management and more time doing research\nMake it easier to prepare and use data\nEnsure that collaborators can readily understand and use data files\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous\nAllow other researchers to find, understand, and use your data to address broad questions\nEnsure that you get credit for data products and for their use in other products",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#the-data-life-cycle",
    "href": "docs/foundations/index.html#the-data-life-cycle",
    "title": "Foundations",
    "section": "The data life cycle",
    "text": "The data life cycle\nA common axiom among data scientists is the application of the 80/20 rule to effort: 80% of time is spent wrangling (managing and preparing) data, while 20% is spent on analysis. Most activities in the data life cycle come before the analysis phase and are closely tied to data management. There are many different models of the data life cycle, and the relevant model for your individual project will vary. A general data life cycle is depicted below (see also Langseth et al. 2015).\n\n\n\n\n\ngraph LR\nA(Plan) --&gt; B(Collect)\nB --&gt; C(Process)\nC --&gt; D(Explore / Visualize)\nD --&gt; E(Analyze / Model)\nE --&gt; F(Archive, Publish, Share)\nE --&gt; C\nE --&gt; A\n\n\n\n\n\n\nThe data life cycle is often iterative and nonlinear, and does not always follow the order shown. Your actual analysis workflow may include dead ends or repeated steps. Regardless, it is helpful to plan and discuss your data-oriented research using these common components of the data life cycle:\n\nStep 1: Plan Identify data that will be collected and how it will be managed. Create a data management plan.\nStep 2: Collect Acquire and store raw data.\n\na. Acquire Retrieve data from the appropriate source.\nb. Describe Document the raw data source, format, variables, measurement units, coded values, and known problems.\nc. Quality assurance Inspect the raw data for quality and fit for analysis purpose.\nd. Store Store the raw data in the appropriate folder, as determined in the planning stage. Consider access, resilience (backing up), security, and, if relevant, data agreement stipulations. Make raw data files read-only so they cannot be accidentally modified.\n\nStep 3: Process Prepare the data for exploration and analysis.\n\na. Clean Preprocess the data to correct errors, standardize missing values, standardize formats, etc.\nb. Transform Convert data into appropriate format and spatiotemporal scale (e.g., convert daily values to annual statistics).\nc. Integrate Combine datasets.\n\nStep 4: Explore Describe, summarize, and visualize statistics and relationships.\nStep 5: Analyze / Model Develop, refine, and implement analysis and model specifications.\nStep 6: Archive, Publish, and Share Finalize documentation (project-level README and metadata). Dispose and/or archive data. Publish final data products and documentation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#sec-dmp",
    "href": "docs/foundations/index.html#sec-dmp",
    "title": "Foundations",
    "section": "The first step: Data management planning for reproducibility",
    "text": "The first step: Data management planning for reproducibility\nData management planning, a form of preliminary documentation, is the process of thinking ahead about how your team will access, use, create, modify, store, share, and describe data related to your research project. Data management plans (DMPs) enhance collaboration by establishing baseline expectations, make projects resilient to turnover, and save time in the long run. In addition, many funders require data management plans be submitted with grant proposals, so thinking about these issues early can facilitate the proposal process.\nThis guidance resource provides general instructions for data practices and addresses many of the core questions that are part of the DMP process. At a minimum, the questions below should be reviewed at the start of a project. Associated guidance is linked.\n\nWhere will data/code be stored and how will it be organized?\nHow will the team use Microsoft Teams for file storage and communication? What types of files will be stored in the Teams folder versus the project’s L:/ drive folder? What will be communicated over Teams chat versus email or GitHub?\nWho will be responsible for disposing / archiving data?\nWho will be responsible for publishing data/code and attaching appropriate documentation and use licenses?\nWhat will the version control / git / GitHub workflow be?\nWhat coding software and main libraries will be used?\nHow will code be reviewed for quality?\nWhat are expectations around data quality and code quality?\nHow will data sources, code, and major methodological decisions be documented?\nHas the appropriate budget been allocated to implement this DMP?\n\nCertain projects may require additional considerations. For development of a more thorough DMP, refer to the UCSB NCEAS data management planning section.\n\n\n\n\nLangseth, M. L., H. S. Henkel, V. B. Hutchison, C.J. Thibodeaux, and L. S. Zolly. 2015. “USGS Data Management Training Modules—Planning for Data Management Part II Using the DMPTool to Create Data Management Plans: U.S. Geological Survey.” https://doi.org/10.5066/F7RJ4GGJ.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "docs/data-management/index.html",
    "href": "docs/data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "Data management encompasses the methods used to collect, store, organize, and use data.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html",
    "href": "docs/data-management/storage-options.html",
    "title": "2  Storage Options",
    "section": "",
    "text": "2.1 Summary",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#summary",
    "href": "docs/data-management/storage-options.html#summary",
    "title": "2  Storage Options",
    "section": "",
    "text": "Store data and code for RFF projects in a project-specific L:/ drive folder.\n\nCreate and configure your new project folder\nOrganize your project folder to enable version control\n\nUse GitHub to share and version control code.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share and review code.\nFor small datasets, GitHub may be used for both data and code storage.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#internal-rff-projects",
    "href": "docs/data-management/storage-options.html#internal-rff-projects",
    "title": "2  Storage Options",
    "section": "2.2 Internal RFF Projects",
    "text": "2.2 Internal RFF Projects\n\n2.2.1 Data: L:/ drive\nThe L:/ drive is the primary location for storing project data and code. All internal projects should have a designated L:/ drive folder—even when working with external collaborators.\nThe L:/ drive is optimized for data-intensive workflows. It offers:\n\nHigh storage capacity\nRegular backups\nEnhanced security compared to personal drives\n\nAccess to the L:/ drive requires an RFF network account.\nSee instructions for setting up a project L:/ drive folder.\n\n\n2.2.2 Code: L:/ drive and GitHub\nProject code should be stored in the project’s L:/ drive folder alongside data. To support collaboration and reproducibility, also enable version control for directories containing scripts (e.g., .R, .py, .do files) and create a remote GitHub repository.\nGitHub’s distributed version control system allows team members to:\n\nWork on scripts independently without disrupting others\nTrack changes with clear commit messages\nReconcile and sync updates across folders\n\nFor example, you can revise a script locally and commit changes, with documentation, when they’re ready—without interfering with your colleague’s workflow.\nInstructions for setting up the version control system are forthcoming. In the meantime, see available external resources.\nIn order for your project to be enabled for version control in the future, each team member who will be viewing, running, or editing code should have their own script folder. Specific instructions are in the Organization section.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#collab",
    "href": "docs/data-management/storage-options.html#collab",
    "title": "2  Storage Options",
    "section": "2.3 Solutions for collaborative projects",
    "text": "2.3 Solutions for collaborative projects\n\n2.3.1 Sharing code\nThe GitHub repository method for storing and sharing scripts is ideal for collaborating with people who do not have access to the RFF network and L:/ drive. If they are made collaborators on the GitHub repository, they’re able to clone a copy of the repository codebase to their local folder directly from the web browser. (See external resources provided.)\n\n\n2.3.2 Sharing data\nIn addition to storing data and code on the L:/ drive project folder, it may be necessary to store and share files in other ways when collaborating with team members without RFF network access. In that case, there are a few options.\n\nOneDrive\nOneDrive can be used to share data with external collaborators. However, the L:/ drive should remain the primary storage location for project data, due to its advantages in access from RFF lab computers, computing capacity, storage space, and data security.\nStorage considerations\n\nFolders created on RFF’s institutional OneDrive accounts have an initial capacity of 5 TB and can be shared with non-RFF staff. If needed, storage capacity can be increased for RFF project folders (but not for individual external users’ accounts).\nIf external collaborators are “guests” in the OneDrive folder, they are also limited to your account’s storage limits.\nOneDrive is not suitable to support large data. There are synchronizations issues when working with single files bigger than 10GB.\nWhile it’s possible to load data from a OneDrive folder into a script, it is not recommended because a) the L:/ drive should be used as the primary storage location for authoritative datasets and b) loading from OneDrive can automatically trigger the OneDrive program to download/sync the data, causing hangups. Instead, copy new data from the OneDrive folder to the project L:/ drive folder.\nOneDrive files are only accessible from the web browser on RFF lab computers. The ability to sync OneDrive folders to the local drive on lab computers is unavailable at this time. The L:/ drive should be used to accessed data from lab computers.\n\nWhen using OneDrive for sharing:\n\nConsider the security implications of storing sensitive data on the cloud and specify access accordingly.\nCopy only necessary data files to the OneDrive folder (e.g., inputs and outputs, not intermediate processing files).\nExternal collaborators can either work directly from the shared OneDrive folder or download files to their own preferred storage.\nExternal collaborators can be given OneDrive folder access as Editors (ability to create, edit, and delete files) or Viewers (read-only access). How to set up collaboration in OneDrive.\nMirror (replicate) the directory structure between the L:/ and OneDrive folders to maintain clarity and consistency. This helps collaborators understand where each file fits within the overall project structure. For example: If a file is stored at raw_data/fires/data.shp on the L:/ drive, it should appear in the OneDrive folder under the same path: OneDrive/.../raw_data/fires/data.shp.\nBe mindful of who is making changes and consider using version control or clear file-naming conventions to track edits to data files. When files exist in both locations (L:/ drive and OneDrive), edits made separately on the L:/ drive and OneDrive can lead to version conflicts.\n\n\n\n\n2.3.3 GitHub\nWhile we recommend only using GitHub to version control script files, experienced users can also use it to share and version control small data files (well below 100 MB). This is also discussed in the Organization section.\nConsider security, sensitivity, and license restrictions when hosting data on GitHub.\nFiles larger than 100 MB should be shared using other tools.\n\n\n2.3.4 Accessing raw data via Application Program Interfaces (APIs)\nSome data sources allow access to data via APIs, such as the US Census and the USDA National Agricultural Statistics Service. APIs enable users or programs to download relevant subsets of data directly into memory, eliminating the need to download and store entire datasets on disk. This approach not only reduces storage requirements but also improves efficiency by allowing applications to process relevant data immediately without managing large raw files.\nWriting code that uses an API is especially advantageous for working with large datasets, because it often allows for subsets of the data to be loaded (e.g., part of a state map covering one city), which can reduce the need for local disk space. If an API is not available, having code download data from remote servers via other methods (e.g., curl) can also be a good option.\nUsing APIs is ideal for a variety of reasons:\n\nStorage size: An API allows the user/program to access specific data of relevance without needing to find room to store an entire, broader dataset (which is instead hosted on the data provider’s server)\nReproducibility advantage: Using public APIs or other ways of accessing public data on the internet allows for code produced using other best practices to run “out of the box” on any computer/VM with internet access\nFresh data: Code that queries an API can automatically retrieve the most current data each time it runs, eliminating the need to manually update local files.\n\nAPIs do require some special considerations:\n\nReproducibility risk: Because API data can change over time, it’s important to save a local copy of the queried data at key stages (e.g., before analysis or publication). This helps ensure results can be reproduced later, even if the live API data has changed.\nLongevity: APIs may stop being accessible/maintained. It is not always a safe assumption that data stored remotely will continue to be accessible via an API. API packages and query formats may also change, so project code may need to be debugged occasionally to maintain compatibility with an API.\nAccessibility: Not all online data sources have convenient API functions in a programmer’s language of choice. Some have URL formats that allow data to be accessed regardless. The amount of documentation and the level of technical skill required to understand and use an API can vary.\n\n\nPublishing/Archiving data sourced from APIs\nIf you use an API to access source data, the best course for publishing or archiving source data will vary based on project needs, dataset size, and nature of the data. Some options are:\n\nDownload to folder: During the archival phase, download the source data in its current state and save it to the project folder to be archived.\nDocument: Document the dataset version and access date in lieu of providing source data. While not ideal for reproducibility, this is suitable in cases where the source data is large, reliable, and not likely to be modified.\nDownload to cloud: Use a repository service, such as Zenodo, to store source data as it existed when archiving the project. Source data accessed through an API can be downloaded directly to a Zenodo repository, without having to save files locally. This can be done through R (zen4R) or python.\n\n\n\n\n2.3.5 Other cloud storage options for large data\nAlternatives to OneDrive, such as Azure, Google Bucket, AWS S3, or Dropbox may be well suited to your project, especially for short-term storage and file transfer. However, note that these storage options may incur additional costs, depending on data size (even the “free tiers” of these services may incur pay-as-you-go costs). For Azure setups, contact IT at IThelp@rff.org.\n\n\n2.3.6 ArcGIS Online for sharing and exploring spatial data\nArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular). In order to access the data and exploratory mapping interface, users need an ArcGIS Online account. Online accounts cost $100 per user and data storage costs vary by data size. Contact RFF’s GIS Coordinator at Thompson@rff.org for more information.\n\n\n2.3.7 Enabling external collaborator access to the L:/ drive\nWhile not recommended, it is possible to enable access to the L:/ drive for non-RFF staff. Temporary accounts can be requested by contacting IT at IThelp@rff.org.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Storage Options</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html",
    "href": "docs/data-management/organization.html",
    "title": "3  Organization",
    "section": "",
    "text": "3.1 Directory structure\nStandardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs. A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions. Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.\nRegardless of the specific method deployed, your data project organization should have the following qualities:\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here.\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L:/ drive. This version illustrates a personal repository folder model of code version control, which operates best on the L:/ drive.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#directory-structure",
    "href": "docs/data-management/organization.html#directory-structure",
    "title": "3  Organization",
    "section": "",
    "text": "Raw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\n\n\nproject_name/\n├── data/\n│   ├── raw/\n│   ├── intermediate/\n│   ├── clean/ (optional)\n├── results*/\n├── repos/\n│   ├── edgar/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n│   ├── caudle/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n\n3.1.1 Data folder\n\nThe data folder contains all project data sets.\nRaw data is preserved in its own subfolder.\nThe intermediate folder contains datasets created during data cleaning and processing.\nIf practical, a clean folder can contain cleaned output datasets, but note that it’s often unclear when datasets are truly “clean” until late project stages.\n\n\n\n\n\n\n\nNote\n\n\n\nFor projects with small datasets, this folder can be version controlled, with larger files ignored using the .gitignore file. In this case, this folder would be a subfolder of individuals’ repository folders.\n\n\n\n\n3.1.2 Results folder\n\nThe results folder contains analysis results, and model outputs. For example, it can be used to store tables, figures, and model estimates.\n*For internal RFF projects, the results folder can be stored in the main directory and not within repositories.\n**For projects with external collaborators, it may be useful to store the results folder within repositories. These files are generally smaller than 100 MB and can be stored in a main repository using GitHub; however, some formats (such as SVG) can be quite large. The .gitignore file can be configured to ignore certain file types (such as SVG files, when both PNG and SVG file formats are generated).\n\n\n\n3.1.3 Repos folder\n\nThis directory structure is based on personal repositories. The repos, or repositories, directory contains version-controlled files. To allow individuals to work on version controlled files without interfering with others’ versions, each researcher should have their own folder that’s linked to the GitHub remote repository. Team members can then clone the project’s Git repository into their respective folders and work exclusively within their own copies. Changes can be synced and reconciled using GitHub, preventing simultaneous edits of the same file and ensuring effective version control.\nIndividual folders (e.g. Edgar, Caudle) should have mirrored (the same) directory structures.\n\n\n\n3.1.4 Scripts folders\n\nThe scripts folder contains all code files (e.g. .R, .py, .do) for the project. These scripts provide explicit instructions for processing data and performing analyses. It should be possible to reproduce the entirety of the project’s processed data sets and results using only these scripts and the raw data.\nThe scripts folder can be parsed into subfolders containing scripts that process raw data (processing), analyze processed data (analysis), and tools. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software.\n\n\n\n3.1.5 Docs folder\nThe documents folder should contain any version-controlled shared documents (e.g. LaTeX, Markdown, Overleaf).\n\n\n3.1.6 Other project files\nThis template does not include specific folders for meeting notes, literature reviews, presentations, project management, etc., because those types of files are not the focus of this guidance.\nWe recommend storing these types of files in the project-level Microsoft Teams folder. See the RFF Communication Norms and Guidance for more information.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#other-organization-practices",
    "href": "docs/data-management/organization.html#other-organization-practices",
    "title": "3  Organization",
    "section": "3.2 Other organization practices",
    "text": "3.2 Other organization practices\n\n3.2.1 Subfolders\nOrganizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (figures or model_results).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "href": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "title": "3  Organization",
    "section": "3.3 Naming folders, files and scripts",
    "text": "3.3 Naming folders, files and scripts\nWhen creating folders:\n\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders.\n\nWhen creating data or script files, make them:\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content. For example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you must create different versions of files, make sure to document the distinction in a README file.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a project-level README file.\n\nMachine readable:\n\nUse only ASCII characters (letters, numbers, and underscores).\nDo not include spaces or special characters (/  : * ? &lt;&gt; &).\nUse hyphens or underscores instead of spaces (the “snake_case” method).\nFiles and folders should be easy to search and filter based on name using structured file names. The ability to sort and read files by name is useful and helps organization but requires specific conventions. See examples in the table below, keeping in mind that:\n\nIt is not recommended to use dates in script file names to denote when a script was created or modified. Instead, leverage version control to save and document different script versions.\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the file name prefixes enter the double digits.\n\n\n\n\n\n\n\n\n\n\n\nPractice for structured file names\nDescription\nExample\n\n\n\n\nID-based names\nStructure file paths and names in a way that makes them easy to access programmatically—e.g., enabling batch loading or iteration across identifiers like years/dates, counties, or scenarios. For example, storing county-level data as shown would allow data to be read into memory by simply looping over FIPS codes as they appear in the directory file names.\n53019_data.csv53033_data.csv53061_data.csv\n\n\nChronological order\nUse ISO 8601 format for date-based files: YYYY-MM-DD. This ensures dates sort correctly by default.\n2021_01_01_precipitation_mm.csv2021_01_02_precipitation_mm.csv2021_01_01_temperature_statistics_f.csv2021_01_02_temperature_statistics_f.csv\n\n\nLogical processing order\nFor scripts or folders that must run in sequence, use numeric prefixes to indicate the intended order of execution.\n01_clean_raw_data.R02_merge_clean_data.R03_descriptive_statistics.R04_regressions.R",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html",
    "href": "docs/data-management/documentation.html",
    "title": "4  Documentation",
    "section": "",
    "text": "4.1 Introduction\n“If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point” (Stoudt, Vásquez, and Martinez (2021)).\nQuality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts.\nWithout documentation, projects lose their usefulness over time, as illustrated below (see also Michener et al. 1997).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#introduction",
    "href": "docs/data-management/documentation.html#introduction",
    "title": "4  Documentation",
    "section": "",
    "text": "Image from Michener et al. 1997",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#types-of-documentation",
    "href": "docs/data-management/documentation.html#types-of-documentation",
    "title": "4  Documentation",
    "section": "4.2 Types of documentation",
    "text": "4.2 Types of documentation\nThis summary can help guide team conversations around documentation strategies.\n\n4.2.1 Preliminary\nPreliminary documentation refers to early-stage descriptions created during the planning phase. This often includes a data management plan (DMP), which outlines data collection, organization, storage, and sharing strategies.\n\n\n4.2.2 Process\nProcess documentation involves capturing step-by-step procedures and workflows used to collect, process, analyze, and model data, including:\n\nDocumenting raw data sources\nDocumenting methods with code comments\nDocumenting methods with version control tools\n\n\n\n4.2.3 Product\nProduct documentation provides information to accompany code and data of completed projects, ensuring usability and transparency. This documentation can be a part of or accompanying written products.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#core-documentation",
    "href": "docs/data-management/documentation.html#core-documentation",
    "title": "4  Documentation",
    "section": "4.3 Core documentation",
    "text": "4.3 Core documentation\n\n4.3.1 The project-level README file\nAt a minimum, the project should have a README file in text or markdown format with the information listed below. This can be an evolving, living document. While it’s technically product documentation, it’s easiest to start it early in the project’s development.\n\nProject name and description\nProject PI and contact information\nList of staff responsible for data management and code development\nAssociated final product(s), date of release, and DOI (if applicable)\nLink to published data/code (if applicable)\nLicense associated with final product(s)\nNature of sensitive or proprietary data (if applicable)\nAny other important notes for navigating folder or using data/code\n\n\n\n4.3.2 Raw data\nBest practices:\n\nThe source of all downloaded raw datasets should be documented in a README file.\nCreate a README file associated with each raw data file, or each logical “cluster” of related raw data files, in the same folder as the data.\nIf there are multiple data files in a folder, name the README so that it is easily associated with the data file(s) it describes (e.g., README_PRISM_Daily_Temperature.txt).\nFormat README files consistently.\nWrite the README document in an plain text and open source file format, such as .txt or .md.\n\nBelow is a README template and example. Include in the README file the information shown in the example.\nFilename: README_PRISM_Daily_Temperature.txt\nDataset name & format: PRISM_Daily_Temperature_2024.csv\n\nData source: Downloaded from the PRISM Climate Group website: https://prism.oregonstate.edu/ on 2024-02-28.\n\nAcquired by: [Name of researcher who downloaded the data]\n\nData description: This dataset contains daily minimum and maximum temperature data for Washington State for the year 2024, with a spatial resolution of 4km.\n\nPreprocessing: No modifications were made to the raw dataset. The file is stored exactly as downloaded from the source.\n\nLicense & Usage Restrictions: This dataset is publicly available under the PRISM Climate Group's data use policy. Refer to https://prism.oregonstate.edu/documents/PRISM_terms_of_use.pdf for more details.\n\n\n\n\n\n\nNote\n\n\n\nAs described in the Organization section, all raw data should be retained in their raw form and not directly modified.\n\n\n\n\n\n\nMichener, W. K., J. W. Brunt, J. J. Helly, T. B. Kirchner, and S. G. Stafford. 1997. “Nongeospatial Metadata for the Ecological Sciences.” https://doi.org/10.1890/1051-0761.\n\n\nStoudt, S., V. N. Vásquez, and C. C. Martinez. 2021. “Principles for Data Analysis Workflows.” https://doi.org/10.1371/e1008770.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html",
    "href": "docs/data-management/file-formats.html",
    "title": "5  File & Data Types",
    "section": "",
    "text": "5.1 Data file formats\nIn general, data should be stored and/or archived in open formats. Open formats are non-proprietary, and therefore maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them (UC Santa Barbara’s Standard Operating Procedures section on Data Formats). The best file format to use will depend on the type and structure of data.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-file-formats",
    "href": "docs/data-management/file-formats.html#data-file-formats",
    "title": "5  File & Data Types",
    "section": "",
    "text": "5.1.1 Key characteristics of data file formats\n\nProprietary vs. non-proprietary: Non-proprietary software formats can be easily imported and accessed using open-source software. This enhances their interoperability, or how easily a file format can be used across different software platforms and systems. Formats that are widely supported and compatible with various tools are generally more versatile.\nTabular vs. hierarchical: Tabular data is organized into rows and columns, resembling a table, while hierarchical data is organized in a tree-like structure, with elements nested within others.\nStructured vs. unstructured: Structured data refers to data that is organized in a predefined format, typically in rows and columns, like databases or spreadsheets, which allows for easy search, analysis, and processing. Unstructured data, on the other hand, lacks a predefined format and is often textual or multimedia in nature, such as emails, social media posts, or video files.\nRetention of data types: Some file formats retain metadata about data types (e.g., whether a column is an integer or string), while others lose this information upon saving.\n\n\n\n5.1.2 Tabular formats\nIn general, it is best to use open-source plain text formats such as comma-separated values (.csv). In some cases, other file formats may be useful for optimizing file size or read/write speed, or for retaining data type information; however, alternative data types may have reduced interoperability. It is generally best to avoid Excel spreadsheets/workbooks (.xlsx) and Stata datasets (.dta) because they are proprietary, except in special cases where these required features offered by these file formats (e.g. variable labels in Stata .dta files) are not available in open-source file formats.\n\n\n\n\n\n\nNote\n\n\n\nIn cases where the native format of source data is in a proprietary software format, it is often necessary to use that software to view and edit data. For example, Stata dataset variables may have labels, a kind of embedded metadata that can only be accessed in Stata.\n\n\n\nCharacteristics of tabular formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nRecommended: comma-separated values\n.csv\nOpen-source\nNo\nStructured\n\n\nTab-separated values\n.tsv\nOpen-source\nNo\nStructured\n\n\nPlain text\n.txt\nOpen-source\nNo\nSemi-structured\n\n\nMicrosoft Excel spreadsheet/workbook\n.xls or .xlsx\nProprietary\nYes\nStructured\n\n\nDatabase File\n.dbf\nOpen-source\nYes\nStructured\n\n\nSAS dataset\n.sas7bdat\nProprietary\nYes\nStructured\n\n\nDTA\n.dta\nProprietary\nYes\nStructured\n\n\nFeather\n.feather\nOpen-source\nYes\nStructured\n\n\nSQLite\n.sqlite, .db\nOpen-source\nYes\nStructured\n\n\nRData\n.rdata or .rds\nOpen-source\nYes\nStructured\n\n\n\n\n\nDescriptions of tabular formats\n\nRecommended: comma-separated values (.csv), delimited text files widely used for data exchange and simple data storage. Each row contains the same number of values separated by commas.\nTab-separated values (.tsv), files similar to CSV files but with values separated by tabs.\nPlain text (.txt), files which can contain unformatted or formatted (schema) text. Not recommended for storing complex datasets.\nExcel spreadsheets/workbooks (.xls, .xlsx), files designed for use with Microsoft Excel software. XLS is a binary file format compatible only with Excel, both older and newer versions. XLSX was developed more recently. It is XML-based, making it compatible with open-source software such Google Sheets as well as versions of Excel released since 2007. Generally avoid relying on these files for data storage due to complex formatting, data formats, formulas, etc. They also complicate quality assurance. XLS is not version-control friendly and XLSX requires special version-control techniques because it is stored in a compressed state. Excel spreadsheets can easily be exported to CSV files.\nDatabase File (.dbf), files used by dBASE and other database systems to store tabular data. They support a fixed schema and metadata. DBF files cannot store full precision. Avoid creating this type of file.\nSAS Dataset (.sas7bdat), the proprietary file format used by SAS for storing datasets. It supports metadata and variable attributes. Datasets should be converted to open-source formats after processing.\nDTA (.dta), binary files created by the statistical analysis software Stata. Note that they sometimes include metadata (e.g., variable labels) that isn’t automatically loaded when importing into other software (e.g. R using the haven package).\nFeather (.feather), a fast, lightweight binary columnar data format used for data exchange between data analysis languages like R and Python. Optimized for performance and efficiency, especially when working with large tables of data.\nSQLite (.sqlite, .db), files used by the SQLite relational database engine, which supports SQL queries and transactions and is used for lightweight, portable databases.\nRData (.rds, .rdata), files used to store one R object (.rds) or an R environment with several objects (.rdata). Useful if working within an R project for efficiency and organization features, but providing limited interoperability.\n\n\n\n\n5.1.3 Hierarchical formats\nHierarchical data formats are best suited for storing and exchanging complex, nested data structures with parent-child relationships, such as configurations, scientific datasets, or web APIs, where flexibility and the ability to represent variable levels of detail are essential.\n\nCharacteristics of hierarchical formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nHierarchical Data Format version 5 (HDF5)\n.h5, .hdf5\nOpen-source\nYes\nStructured\n\n\nNetwork Common Data Form (NetCDF)\n.nc\nOpen-source\nYes\nStructured\n\n\nJavaScript Object Notation\n.json\nOpen-source\nNo\nSemi-structured\n\n\neXtensible Markup Language\n.xml\nOpen-source\nNo\nSemi-structured\n\n\nYAML\n.yml or .yaml\nOpen-source\nNo\nUnstructured\n\n\n\n\n\nDescriptions of hierarchical formats:\n\nHierarchical Data Format version 5 (.hdf5, .h5), commonly called HDF5, files for storing complex and hierarchical datasets, supporting large data volumes and complex data structures.\nNetwork Common Data Form (.nc), commonly called NetCDF, files designed for array-oriented scientific data. They work especially well for multi-dimensional data like time-series and spatial data.\nJavaScript Object Notation (.json), text-based files used for storing structured data. Often used to transfer data between a server and a web application, as well as when sending and receiving data via an API.\neXtensible Markup Language (.xml), files organizing data hierarchically with customizable tags, making them both machine-readable and human-readable. XML is widely used in web services, data exchange, and configuration files.\nYAML (.yaml or .yml), human-readable files using a data serialization format well suited for configuration files and data exchange. It uses indentation to define structure and supports key-value pairs, lists, and nested data, making it simpler and more concise compared to XML or JSON. “YAML” is a recursive acronym: YAML Ain’t Markup Language.\n\n\n\n\n5.1.4 Geospatial file formats\nGeospatial data are stored as either vector data or raster data. The format of input spatial data typically dictates which geospatial tools can be applied to it.\n\nVector data\n\nVector data is stored as pairs of coordinates. Points, lines, and polygons are all vector data.\nRecommended open-source vector file formats:\n\nGeopackage (.gpkg, recommended for its advantages over the shapefile format)\nKeyhole markup language (.kml, .kmz)\nGeoJSON (.json, .geojson)\nTables with coordinates (e.g., a CSV file)\n\nCommon proprietary vector file formats:\n\nShapefiles (.shp)\nFeature classes in geodatabases (.gdb)\n\n\n\n\n\n\n\nNote\n\n\n\nA shapefile is actually a collection of several files, including geometry (.shp), projection information (.prj), tabular data (.dbf), and more. Make sure to store all component files together within the same folder.\n\n\nAll vector data files should have three critical metadata components:\n\nCoordinate reference system\nExtent: the geographic area covered by the data, represented by four coordinate pairs\nObject type: whether the data consists of points, lines, or polygons\n\n\n\nRaster data\n \nRaster data formats store values across a regular grid containing cells of equal size, with each cell containing a value. A raster is similar to a pixelated image, except that it’s accompanied by information linking it to a particular geographic location. All cell values within a single raster variable are of the same scalar data type (integer, float, string, etc.). Common examples of raster data are elevation, land cover, and satellite imagery.\nThe recommended general purpose raster file format is GeoTIFF (.tif), as it supports multiple bands, retention of spatial reference metadata, large file sizes, high compression, and use in a variety of languages/software. Other formats may work better for specific use cases. All of the following common formats are open-source:\n\nGeoTIFF (.tif), the most widely used format for raster data\nASCII grid (.asc), plain-text-based files for elevation models and basic raster grids\nNetCDF (.nc) and HDF5 (.hdf5, .h5), both described in Section 5.1.3\n\nAvoid saving rasters as proprietary software file formats, including ESRI grid/tile and ERDAS Imagine (.img) files.\nAll raster files should have five critical metadata components:\n\nCoordinate reference system\nExtent: how large the raster is, often represented by the number of rows and columns\nOrigin point: a pair of coordinates pinpointing the bottom left corner of the image\nResolution: cell size\nNo data value: the value that represents when a cell’s data value is missing\n\nFor a more in-depth introduction to spatial data types, see Introduction to Geospatial Concepts: Summary and Setup (datacarpentry.org) and GIS Training (RFF intranet).\n\n\n\n5.1.5 Efficiency trade-offs\nIn addition to being more accessible, plain text-based formats are often more compatible with version control systems than many proprietary formats due to their human-readable structure. However, when working with large datasets, it’s important to consider efficiency in terms of input/output speed and file size. Data formats can vary significantly in these aspects—while binary formats are typically more efficient in terms of speed and storage, they are less suited for version control.\nFor example, in the R ecosystem, .Rds and .RData are binary file formats that allow for fast and space-efficient data storage. In comparison, large .shp (Shapefile) or .geojson files can take more than 100 times as long to load than an equivalent .Rds or .RData file. Other binary formats, such as .feather or .fst for tabular data, are both fast and lightweight, with the added benefit of being language-agnostic, meaning they can be used across different programming environments.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#figure-file-formats",
    "href": "docs/data-management/file-formats.html#figure-file-formats",
    "title": "5  File & Data Types",
    "section": "5.2 Figure file formats",
    "text": "5.2 Figure file formats\nIt is helpful to think ahead when generating and saving data visualizations and plots. Academic journals often accept TIFF and PNG formats, but they frequently have resolution and dimension requirements. Export figures with a minimum resolution of 300 dots per inch (DPI).\nFor RFF communications, however, the vector format .svg is best because it can be easily modified as needed. Academic journals often accept this format as well.\nConsider that you may want to be able to share the underlying data with the RFF Communications team so that they and their external design partners can create custom figures for presentation on the website, in the magazine, etc. This means clearly documenting the processing code that created the underlying data / figures, so that output data can be easily reproduced and shared as needed. If figure data is time-consuming to reproduce, you may want to save a copy of it to the L:/ drive or to your GitHub repository.\n\n“Sharing the underlying data of any maps and figures is always helpful for the Communications Team!”\n– Elizabeth Wason (Editorial Director, RFF)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-types",
    "href": "docs/data-management/file-formats.html#data-types",
    "title": "5  File & Data Types",
    "section": "5.3 Data types",
    "text": "5.3 Data types\nIndividual data values are stored in specific data types, or formats. It is important to identify the data types of important variables in raw datasets to understand their precision and to determine whether data type conversion is necessary for your analysis.\nEvery value, or object, has a type.\nTypes control what operations/methods can be performed on a given value.\nThe choice of data type affects storage requirements. Using larger types (e.g., float64 instead of float32, or int64 instead of int32) increases memory usage, which can be significant in large datasets. Conversely, choosing types that are too small can lead to data loss or overflow errors.\n\n\n\n\n\n\nWarning\n\n\n\nData types can be unwittingly changed, affecting precision and operations. For example, including a string in a numerical column of a CSV will likely cause all the column’s values to be read in as strings.\n\n\n\n5.3.1 Basic types\n\nExamples\n\n\n\n\n\n\n\n\n\nType\nAbbreviation\nDescription\nExamples\n\n\n\n\nInteger\nint\nWhole numbers\n0, 1, 42, -6,2e+30 (scientific notation)\n\n\nFloating point\nfloat\nReal numbers, with or without a fractional component\n3.1, 2.7182818285, -1.5, 0, 43\n\n\nCharacter string\nstr\nText, demarcated by quote marks on either side\n\"Hello, world!\", 'apple', \"#23\", \"'Why?', he said.\"\n\n\n\n\n\nDetails\nNumeric values are a key element of scientific computing:\n\nIntegers represent whole numbers, which can be positive, negative, or zero\n\nSubtypes of integers can be further categorized based on their size (bits) and whether they are signed (can represent negative numbers), for example,\n\nint8: 8-bit signed integer, ranges from -128 to 127\nuint16: 16-bit unsigned integer, ranges from 0 to 65,535\n\nUse cases include counting, indexing, and scenarios where whole numbers are needed (e.g., population counts, item quantities)\nSmaller integer types use less memory but have a limited range, so it is most efficient to use the smallest type with enough room for a given data value, vector, matrix, list, etc.\n\nFloating point numbers represent real numbers, which include all fractions in addition to all integers\n\nLike integers, they can be specified by size (bits), typically 32-bit or 64-bit\n\nfloat32 is “single precision”\nfloat64 is “double precision”\n\nUse cases include scientific calculations, financial modeling, and any scenario requiring precision for fractional values (e.g., temperature measurements, stock prices)\nThe float32 type uses less memory than float64 but with less precision, so it is best to use float32 for large datasets where memory is a constraint and float64 when precision is important\n\n\nData taking the form of letters, words, or other text are used just as widely as numbers:\n\nCharacter strings contain text written between quote marks (either single or double)\n\nUse cases include names, addresses, descriptive text, and categories (e.g., gender, region, brand)\nStrings longer than a few words often take up more memory than numbers, so it’s important to manage string data carefully, especially in large datasets.\n\n\n\n\n\n5.3.2 Special types\nCategorical data can be stored more efficiently using more specific types:\n\nLogical (boolean) data each take one of two possible values (e.g., 0 or 1, true or false)\n\nExamples of use cases include tests (pass/fail), survey responses (yes/no), and signals (on/off)\nVery memory-efficient, requiring little more than one bit per value\nProgramming note: In R, Python, and Julia, the logical or boolean type is considered a distinct type but is often compatible with or represented as integers for underlying storage or arithmetic operations. In Stata, logical values are represented directly as integers, with no separate boolean type.\n\n\nFactors represent data with a finite (usually relatively small) and usually labeled set of possible values, usually referred to as levels or categories\n\nUse cases include non-numeric data that falls into distinct categories, such as colors, months, quality survey responses, or Excel workbook cells with a dropdown list of values\nInternally stored as integers with an associated set of labels or levels\n\nDate/time values can be stored using data types designed for human- and machine-readability\n\nMost programming languages have modules that support specific date formats\nWe recommend using the conventional ISO 8601 format (YYYY-MM-DD)\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as “DD/MM/YYYY” but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect.\n\n\n\n\n\n5.3.3 Other resources\nSee these websites for additional information about data types:\n\nR-focused: R for Data Science: Transform\nPython-focused: Software Carpentry: Data Types and Type Conversion",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File & Data Types</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html",
    "href": "docs/data-management/sensitive-proprietary.html",
    "title": "6  Sensitive & Proprietary Data",
    "section": "",
    "text": "6.1 Identify and categorize sensitive data\nWhen using sensitive or proprietary data in your research project, it’s crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.\nDetermine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "href": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "title": "6  Sensitive & Proprietary Data",
    "section": "",
    "text": "To determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "href": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.2 Document data sensitivity and restrictions",
    "text": "6.2 Document data sensitivity and restrictions\n\nDocument data sensitivity class and details in both the project-level README and, if the data are secondary, the associated raw data README file. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "href": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.3 Determine appropriate security and privacy measures",
    "text": "6.3 Determine appropriate security and privacy measures\n\nContact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "href": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.4 Data derivatives: masking and aggregation",
    "text": "6.4 Data derivatives: masking and aggregation\n\nData derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "href": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "title": "6  Sensitive & Proprietary Data",
    "section": "6.5 Review before publication or sharing",
    "text": "6.5 Review before publication or sharing\nRevisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sensitive & Proprietary Data</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html",
    "href": "docs/data-management/quality-preparation.html",
    "title": "7  Quality & Preparation",
    "section": "",
    "text": "7.1 Quality Assurance for Data Integrity\nAlways use and save a scripted program for data processing and analysis. Although it may seem more expeditious to take manual steps, writing code creates a documented and repeatable account of the processing steps taken and will save time and effort in the long-run.\nIf it is impossible to write code for processing steps, create a detailed record of the workflow in a document.\nQuality assurance (QA) is ensuring the accuracy, consistency, and reliability of data. Quality assurance measures should be implemented on both raw data from external sources and your project’s subsequent datasets; for example, after a major processing step such as data merging.\nEight basic quality assurance measures are listed below, some of which were adapted from (Hutchinson et al. 2015).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "href": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "title": "7  Quality & Preparation",
    "section": "",
    "text": "Read documentation / metadata that accompanies source datasets. It often comes in separate files (text, pdf, word, xml, etc.).\nAlways keep raw data in its original form. Do not modify raw datasets; save modified versions in the project’s “intermediate” data folder.\nVisually inspect data throughout processing. Visual checks can reveal issues with the data (e.g., repeated values or delimitation errors) that would affect analysis. This habit not only aids debugging processing code but also builds an understanding of the dataset.\nAssure data are delimited and line up in proper columns. Check that data is correctly delimited and parsed when imported into the processing program.\nCheck for missing values. Identify any missing or NA values in critical fields that could impact analysis. If there are missing values, identify the type of missingness and discuss solutions (applied example in R).\nIdentify impossible and anomalous values. Anomalies include values that are outside the expected range, logically impossible, outliers, or inconsistently formatted. In addition to checking for errors, identifying outliers can aid in data exploration by flagging rare events, errors, or interesting phenomena that require further investigation.\nPerform and review statistical summaries. Generate summary statistics to understand data distribution and identify inconsistencies or errors. Use these summaries to guide further cleaning, transformation, or data integrity checks.\nVisualize data through maps, boxplots, histograms, etc.\nFollow good software quality practices described in the software quality section of this guidance, such as pseudocoding, code review, and defensive programming.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#tidy-data",
    "href": "docs/data-management/quality-preparation.html#tidy-data",
    "title": "7  Quality & Preparation",
    "section": "7.2 Tidy Data",
    "text": "7.2 Tidy Data\nRaw data rarely comes in structures compatible with your team’s analysis needs. Once the raw data has been checked for quality, additional processing may be required to start exploration and analysis.\nTidy data is a framework for data organization that facilitates efficient exploration, wrangling, and analysis (Wickham 2014). The benefits of storing data in the tidy format are:\n\nEasier data exploration and analysis\nSimpler manipulation and transformation of data\nCompatibility with data analysis tools (e.g., R and Python)\nImproved reproducibility of analysis\n\nData in this format are easy to work with, analyze, and combine with other datasets. However, once analyses and data merges start taking place, the structure of newly generated datasets are likely to be more complex and dependent upon the modeling or analysis needs. Discuss the role of tidy data with your team, and if/when in the process project datasets should deviate from the tidy data structure.\n\n7.2.1 The Three Core Principles of Tidy Data\n\nEach variable forms a column: In a tidy dataset, each variable has its own dedicated column. This means all values associated with that variable are listed vertically within that single column. These are also often referred to as fields or attributes.\nEach observation forms a row: Define an observation and emphasize that each row should represent a single data point. In a tidy dataset, each observation occupies a single row in the table. All the information pertaining to that specific observation is listed horizontally across the columns. These are also often referred to as records.\nEach type of observational unit forms a table: In a tidy dataset, data pertaining to different types of observational units should be separated into distinct tables.\n\n\n\n\nimage from https://r4ds.had.co.nz/tidy-data.html#tidy-data-1\n\n\n\n\n7.2.2 Practical applications\n\nR: The R tidyverse is a set of R packages designed to work together within the tidy data framework. It includes dplyr, readr, ggplot2, and other packages useful for wrangling data.\nPython: The Python pandas library is useful for creating and working with tidy data, as it uses data frames and includes functions for cleaning, transforming, and manipulating data.\nJulia: The DataFrames.jl library is useful for working with tabular tidy data, and has many powerful tools for manipulating data. The Tidier.jl framework builds on DataFrames.jl and emulates the R tidyverse.\n\n\n\n7.2.3 Recommended reading and examples\n\nGeneral:\n\nIntroduction to Data Wrangling and Tidying | Codecademy\nA Gentle Introduction to Tidy Data in R | by Arimoro Olayinka | Medium\n\nR focus:\n\nTidy data | tidyr\nData tidying – R for Data Science\nHelpful libraries and functions:\n\ntidyr::separate\njanitor::clean_names\n\n\nPython focus:\n\nTidy Data in Python",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "href": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "title": "7  Quality & Preparation",
    "section": "7.3 Data Type Conversion and Standardization",
    "text": "7.3 Data Type Conversion and Standardization\nData types, which define how data is stored and interpreted, were presented in the previous section. This section introduces data type conversion and standardization in ensuring consistent and meaningful analysis.\nData type conversion, or typecasting, is the process of transforming a value from one data type to another. Converting data types ensures compatibility between different datasets and allows for proper analysis. For example, converting age values from string (“25 years”) to integer (25) enables mathematical operations.\nWarning: Data type conversion can sometimes lead to loss of information, so it’s crucial to understand the implications of conversion before applying it. Examples:\n\nWhen converting the age column from string (“25 years”) to integer (25), information about the unit (years) was lost.\nConverting a float (2.96) to an integer (3) truncates decimals.\nIf a date is formatted as “DD/MM/YYYY” (03/12/2015) but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect (2015-03-12, instead of the accurate 2015-12-03).\n\nProper type conversion ensures data is correctly interpreted and can prevent errors in calculations, data analysis, and visualization.\n\n7.3.1 Key Points for Type Conversion\n\nUnderstand the source and target types: Knowing the data types involved in conversion helps ensure accurate transformations.\nHandle missing or invalid data: Make sure to manage missing or improperly formatted data that could cause errors during conversion.\nTest conversions: Always verify that conversions produce the expected results to avoid downstream errors in your analysis.\nRemember to always use the ISO 8601 standard format for dates (YYYY-MM-DD).\nBe aware that importing, reimporting, or saving files in certain formats can lead to loss or changes in column data types. For instance, saving data in CSV format often results in date columns being interpreted as text upon re-import, or numeric columns losing precision. This issue arises because formats like CSV lack built-in metadata to store data types, meaning they rely on the importing program to infer types, which can cause inconsistencies and data integrity issues over time. In these cases, columns may need to be re-typecast.\n\n\n\n7.3.2 Resources\n\nPlotting and Programming in Python: Data Types and Type Conversion\nR for Data Science - Transform",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "href": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "title": "7  Quality & Preparation",
    "section": "7.4 Preparation for Analysis",
    "text": "7.4 Preparation for Analysis\nAdditional cleaning and transformation steps (often referred to as “data wrangling”) are often necessary, and are highly variable depending on project needs. Examples include:\n\nfiltering based on criteria\nrestructuring/reshaping/pivoting\nremoving duplicates\ncorrecting errors in the source data (e.g. misspellings)\nmerging\n\nThe approach depends on specific project and data needs. The following resources go into greater detail, with examples:\n\nR\n\nNCEAS Learning Hub’s coreR Course - 7 Cleaning & Wrangling Data (ucsb.edu)\nTransform – R for Data Science (2e) (hadley.nz)\nR for Reproducible Scientific Analysis: Subsetting Data (swcarpentry.github.io)\nR for Reproducible Scientific Analysis: Data Frame Manipulation with dplyr (swcarpentry.github.io)\n\nPython\n\nData Analysis and Visualization in Python for Ecologists: Indexing, Slicing and Subsetting DataFrames in Python (datacarpentry.org)\nData Analysis and Visualization in Python for Ecologists: Combining DataFrames with Pandas (datacarpentry.org)\n\n\n\n\n\n\nHutchinson, V. B., T. E. Burley, M. Y. Chang, T. A. Chatfield, and R. B. Cook. 2015. “USGS Data Management Training Modules—Best Practices for Preparing Science Data to Share: U.S. Geological Survey.” https://doi.org/10.5066/F7RJ4GGJ.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality & Preparation</span>"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html",
    "href": "docs/data-management/archival.html",
    "title": "8  Archival & Disposal",
    "section": "",
    "text": "8.1 Archival\nArchiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Archival & Disposal</span>"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#archival",
    "href": "docs/data-management/archival.html#archival",
    "title": "8  Archival & Disposal",
    "section": "",
    "text": "ensures long-term and secure storage to projects for reproducibility and reuse,\nimproves organization, accessibility, and usability of both active and completed project files, and\nreleases computational resources for active projects, reducing energy consumption and storage costs.\n\n\n\n\n\n\n\nNote\n\n\n\nArchival takes place when projects are complete. To preserve the state of code and data at major milestones, such as journal article publication, see Publication.\n\n\n\n8.1.1 How to archive data at RFF\n\n\n\n\n\n\nNote\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to specified team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).\n\nStep 1: Finalize data organization\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code. However, use discretion: in some cases, intermediate data that are likely to be used again and are time-consuming to re-create should be retained.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see Publishing/Archiving data sourced from APIs.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\none project-level README\nall raw data README files\nany metadata files\n\n\n\nThis is generalized guidance. For additional guidance choosing which files to archive, see Decide what data to preserve.\n\n\nStep 2: Finalize documentation\nEnsure the project-level README file, raw data README files, and any metadata files are up to date.\n\n\nStep 3: Coordinate with IT to ensure long-term folder access\nContact IT at IThelp@rff.org to arrange and configure archival storage of the folder. Include the following with the email:\n\nProject-level README file\nList of researchers that should retain folder access\nApproximate folder size (e.g., 5 GB)\nThe nature of sensitive/proprietary datasets",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Archival & Disposal</span>"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#disposal",
    "href": "docs/data-management/archival.html#disposal",
    "title": "8  Archival & Disposal",
    "section": "8.2 Disposal",
    "text": "8.2 Disposal\nSome data may need to be deleted to protect sensitive information or comply with regulations, data agreements, or funder requirements. This is often referred to as data disposition. If any of these requirements applies to a project, follow these best practices when deleting data:\n\nVerify Requirements: Confirm funder agreements and legal obligations regarding data retention and deletion.\nSource Deletion: Confirm with IT that the files were fully deleted in accordance with requirements (e.g., backup files).\nDocumentation: Record when and how data was deleted.\n\nThese practices should apply to data stored the RFF network, OneDrive, or Microsoft Teams.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Archival & Disposal</span>"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html",
    "href": "docs/data-management/publication.html",
    "title": "9  Publication",
    "section": "",
    "text": "9.1 Licensing\nPublication is making code/data available to the broader community, often through formal dissemination channels such as data repositories, journal articles, or public databases. Publication ensures that data is discoverable and can be accessed by other researchers, stakeholders, or the public. Documentation and metadata are included to facilitate understanding and reuse. Publication may also involve adherence to specific standards and best practices to enhance the visibility and impact of the data.\nWhile RFF does not mandate the publication of code and data, it is highly encouraged.\nIncreasingly, journals require code and data to be submitted along with the article. In addition, many funders and stakeholders value open source software and data availability. Planning for this in the early stages of a project facilitates reproducibility, access, and the ability of others to use and cite your work.\nA well-chosen license clarifies permissions, prevents misunderstandings, and encourages responsible use. The next section provides guidance on selecting and attaching appropriate licenses to ensure your data and code remain accessible and properly credited.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publication</span>"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html#licensing",
    "href": "docs/data-management/publication.html#licensing",
    "title": "9  Publication",
    "section": "",
    "text": "Note\n\n\n\nData and code products are licensed separately from RFF publication products. All work on the RFF.org and Resources.org websites (working papers, reports, issue briefs, explainers, Common Resources blog posts, Resources magazine articles, Resources Radio podcast episodes, graphs, charts, photographs, audio, and video) are listed under the Deed - Attribution-NonCommercial-NoDerivatives 4.0 International - Creative Commons license. This Creative Commons license is not suitable for either software or data.\n\n\n\n9.1.1 Verify Data and Code Rights and Ownership\nBefore proceeding with choosing licenses, confirm that your team owns IP rights to the work produced and has full discretion over licensing the project’s data and code—without restrictions from funders, institutional policies, or legal/data agreements.\nIn some cases the research team may have joint IP ownership with partners or funders. In this case, licensing options must be agreed upon by both parties.\n\n\n9.1.2 Choose Appropriate Licenses\n\nCommon Licenses\nFor software and data, there are three main license suites common in the academic space: MIT, GNU GPU, and ODC. MIT and GNU GPU are separate software licenses, while ODC has two commonly-used data licenses. All four are described below.\n\nSoftware licenses\n\nMIT: The more permissive and flexible. Users, including commercial entities, can view, use, modify, and distribute the work freely. Allows for commercial use and enclosure. Attribution is required.\n\n(Enclosure is the process of restricting access, usage, or modification of software or its source code through the imposition of licensing terms. It involves the application of intellectual property rights (such as copyright, patents, or trade secrets) to create boundaries around software, typically to protect the developer’s or organization’s control over its distribution and use.)\n(Attribution refers to crediting to the original creator, author, or source as specified in the licensing terms. Attribution ensures recognition of the intellectual property and efforts of the creators while allowing others to use, modify, or distribute the licensed material.)\n\nGNU General Public Use (GPU): Users, including commercial entities, can view, use, modify, and distribute the work freely. However, it carries copyleft, so all distributions must be released under the same GNU GPU license, ensuring open access (viral). Attribution is required.\n\n(Copyleft is a concept in software (code/script) licensing that ensures any derivative works (modifications or adaptations) of a particular work remain subject to the same licensing terms as the original work. For example, if a developer modifies a program released under the GNU General Public License (GPL), they are required to distribute their modified version under the same GPL license. This means they must also make the source code available and allow others to freely use, modify, and redistribute it under the same terms. Copyleft aims to preserve software freedom by preventing proprietary restrictions from being reintroduced into derivative works.)\n(Viral: Licenses like the GNU General Public License (GPL) and the Open Database License (ODbL) are often described as “viral” because they require derivative works to comply with the same licensing terms as the original work. This characteristic ensures that the freedoms granted by the license (such as the ability to use, modify, and share) are preserved in all subsequent versions or derivatives, but it also imposes specific obligations on those who modify or build upon the original.)\n\n\nData licenses by Open Data Commons (ODC)\n\nODC-By: The more permissive and flexible. Users, including commercial entities, can view, use, modify, and distribute the work freely. Allows for commercial use and enclosure. Attribution is required.\nODCbL: Users, including commercial entities, can view, use, modify, and distribute the work freely. However, it carries share-alike, so all distributions must be released under the same ODCbL license, ensuring open access (viral). Attribution is required.\n\n(Share-alike, similar to copyleft but applied to datasets, requires that any derivative works of content (adaptations or modifications) are licensed under the same or a compatible license. This ensures that future users of the adapted work can use it under similar terms.)\n\n\n\nRecommendations for selecting licenses\n\nAdhere to any product licensing requirements from the funding agreement and the source data/software.\nConsider whether the common academic licenses that are more permissive and flexible (MIT for software, ODC-By for data) are appropriate. If so, use those. These licenses are suitable for most applications.\nIf project authors/partners and RFF would prefer that all derivative works remain open source and accessible, use GNU GPLv3 plus citation request for software and ODbL for data. Caution that these licenses, because they are more restrictive, are more complex, so review license terms carefully.\nIf there are other requirements or the team would like to review a broader range of software licenses and their specifications, visit ChooseALicense.\n\n\n\n\n9.1.3 Create and customize license files\n\nOnce the license has been selected, download or create a .txt license file from the license website. Save the file as LICENSE.txt in the project folder and/or repository.\nReview the license terms and modify where necessary (most open-source and open-data licenses are designed to be used as-is, but others may require you to fill in specific details, such as name or organization).\nIf adding additional terms, include them in a separate README or license appendix to avoid conflicting with the main license.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publication</span>"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html#publishing",
    "href": "docs/data-management/publication.html#publishing",
    "title": "9  Publication",
    "section": "9.2 Publishing",
    "text": "9.2 Publishing\nFor publishing both code and data, ensure that the project-level README is up to date.\n\n9.2.1 Code\nWhen your project is ready to publish code—whether alongside a journal article, report, or other research output—there are a few options for how to share it, especially if your code is hosted on GitHub.\n\nNote on journal requirements\nSome journals require authors to:\n\nMake the codebase publicly accessible\nInclude a link to the GitHub code repository in the manuscript\nReference the codebase DOI in the manuscript\n\n\n\nSharing code via the RFF GitHub Organization\nYou can link your GitHub repository to the RFF GitHub organization in one of two ways:\n\nOption 1: Forking into the RFF GitHub Organization\nBest for: Capturing a snapshot of a repository at a specific point in time (e.g., associated with a publication).\n\nA fork creates a copy of your repository under the RFF organization account.\nThis snapshot can be tagged to align with a publication version (e.g., v1.0-paper-release).\nThe fork can be updated to stay in sync with the original repo, or kept as a fixed version.\nInclude a note in the README linking the snapshot to the relevant publication.\n\n\n\nOption 2: Transferring Ownership to the RFF Organization\nBest for: Ongoing or institutional projects requiring a permanent home and clean URL.\n\nThe repository is moved under the RFF GitHub organization.\nAll existing collaborators retain their permissions.\nThe repository URL will change, but redirects will be preserved by GitHub.\nOrganization owners (e.g., staff managing RFF’s GitHub) gain administrative control.\n\n\n\n\nRecommended Publishing Workflow\nIf your repository is already in the RFF GitHub organization:\n\nMake the repository public once you’re ready to publish.\nTag the version associated with the publication (e.g., v1.0-publication-name) (how to create GitHub tags).\nArchive it on Zenodo if a DOI is needed.\n\nIf your repository is under a personal GitHub account:\n\nDecide whether you plan to continue developing the code:\n\n\nFor ongoing or reusable projects, consider transferring the repository to the RFF organizational GitHub.\nFor static, published versions, request that the RFF GitHub account fork your repository.\n\n\nContact the RFF GitHub organization admins via DGWG@rff.org so that they can fork or migrate the repository and add a tag associated with the publication (tags do not automatically transfer with the fork).\nOptionally, deposit the tagged version to Zenodo and link the DOI in your README.\n\n\n\n\n9.2.2 Data\n\nData-level documentation (metadata)\nIt is recommended to attach metadata files to published datasets. Metadata (“data about data”) documents the “who, what, when, where, how, and why” of a data resource. Metadata not only allows users (your future self included) to understand and use datasets, but also facilitates search and retrieval of the data when deposited in a data repository.\nBelow are the key components of metadata. They can be stored in a simple text or markdown file.\n\nTitle: Descriptive name of the dataset.\nDOI number: Associated with the final publication, dataset, or both\nAbstract: Summary of the dataset’s content\nKeywords: Relevant terms for search and discovery\nTemporal Extent: Time period covered by the data\nData Format: File format\nData Source(s): Origin of the data\nAccuracy and Precision: Information about data quality\nAccess Constraints: Restrictions on data use\nAttribute / field definitions: Define all abbreviations and coded entries\nAdditional geospatial metadata components, if applicable\nSpatial Extent: Geographic coverage (bounding coordinates)\nProjection Information: Coordinate system details\n\nIn some contexts, generating machine-readable metadata that adheres to certain disciplinary standards is useful. There are various metadata formats and standards for specific disciplines. Additional guidance and resources for generating machine-readable metadata are here: Metadata and describing data – Cornell Data Services.\n\n\nUploading data to Zenodo\nZenodo is an online repository for sharing research data, software, and other scientific outputs. It has a broad disciplinary focus and is safe, citeable (every upload is assigned a DOI), compatible with GitHub, and free for up to 50GB of storage.\n\nStep 1: Prepare the research data\nBefore uploading, ensure that:\n\nThe data is well-organized (e.g., structured folders, clear file names).\nMetadata file are prepared for each data file or sets of data files, including dataset title, description, author names, and relevant keywords.\nA license is attached.\nAny sensitive or restricted data is removed or anonymized (if applicable).\nThe project-level README is up to date.\n\nFor guidance on choosing which files to publish and how to handle API-accessed data, see Finalize data organization.\n\n\nStep 2: Create a Zenodo account & access the upload dashboard\n\nGo to Zenodo and sign in (or create an account). Note that you can create an account using your GitHub profile.\nClick the “New Upload” button on the Zenodo dashboard.\n\n\n\nStep 3: Upload the data files, fill in metadata, set access\n\nUpload data, metadata, and the project-level README file.\nEnter metadata information in applicable fields (contributors, associated journal article or conference presentation, etc.)\nInclude a link to the GitHub code repository.\nChoose an Access Level\nOpen Access: Publicly available for anyone.\nEmbargoed: Set a release date if the data must remain private for a certain period.\nRestricted Access: Requires users to request access.\n\n\n\nStep 4: Publish & Get a DOI\n\nReview all details and make any necessary edits.\nClick “Publish” to finalize the upload.\nZenodo will generate a DOI — use this when citing the dataset in publications.\n\n\n\nVersioning & Updates\nIf the dataset needs to be udpated:\n\nUse the “New Version” option in Zenodo instead of creating a separate upload.\nZenodo will link versions together and maintain persistent DOIs.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Publication</span>"
    ]
  },
  {
    "objectID": "docs/software/index.html",
    "href": "docs/software/index.html",
    "title": "10  Software Quality",
    "section": "",
    "text": "Content coming soon.\nExisting style guide resources:\n\nR: Tidyverse Style Guide by Hadley Wickham\nPython: Google Python Style Guide\nStata: Suggestions on Stata programming style by Nicholas Fox\nOther languages: Google style guides for other languages",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Software Quality</span>"
    ]
  },
  {
    "objectID": "docs/version-control/index.html",
    "href": "docs/version-control/index.html",
    "title": "11  Version Control",
    "section": "",
    "text": "Content coming soon.\nGet started with GitHub documentation.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Version Control</span>"
    ]
  }
]