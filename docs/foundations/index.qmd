---
title: Foundations
references:
- type: article
  id: DMPTool
  author:
  - family: Langseth
    given: M. L.
  - family: Henkel
    given: H. S.
  - family: Hutchison
    given: V. B.
  - family: Thibodeaux
    given: C.J.
  - family: Zolly
    given: L. S.
  issued:
    date-parts:
    - - 2015
  title: 'USGS Data Management Training Modules—Planning for Data Management Part II Using the DMPTool to Create Data Management Plans: U.S. Geological Survey.'
  DOI: 10.5066/F7RJ4GGJ
  URL: https://doi.org/10.5066/F7RJ4GGJ
- type: article
  id: Stoudt
  author:
  - family: Stoudt
    given: S.
  - family: Vásquez
    given: V. N.
  - family: Martinez
    given: C. C.
  issued:
    date-parts:
    - - 2021
  title: 'Principles for Data Analysis Workflows.'
  DOI: 10.1371/e1008770
  URL: https://doi.org/10.1371/journal.pcbi.1008770
- type: article
  id: Michener
  author:
  - family: Michener
    given: W. K.
  - family: Brunt
    given: J. W.
  - family: Helly
    given: J. J.
  - family: Kirchner
    given: T. B.
  - family: Stafford
    given: S. G.
  issued:
    date-parts:
    - - 1997
  title: 'Nongeospatial Metadata for the Ecological Sciences.'
  DOI: 10.1890/1051-0761
  URL: https://doi.org/10.1890/1051-0761(1997)007%5b0330:NMFTES%5d2.0.CO;2
---

## Foundations

Good data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project.
Good data practices provide (Langseth et al. 2015):

* Short-term benefits
* Spend less time doing data management and more time doing research.
* Easier to prepare and use data.
* Collaborators can readily understand and use data files.
* Long-term benefits
* Make your work more transparent, reproducible, and rigorous.
* Other researchers can find, understand, and use your data to address broad questions.
* You get credit for preserving data products and for their use in other products.


## The Data Life Cycle

A common axiom among data scientists is the application of the 80/20 rule to effort: 80% of time is spent wrangling (managing and preparing) data, while 20% is spent on analysis. 
Most activities in the data life cycle come before the analysis phase and are closely tied to data management. 
There are many different models of the data life cycle, and the relevant model for your individual project will vary. 
A general data life cycle is depicted below [see also @DMPTool].


```{mermaid}
graph TB
A(Plan) --> B(Collect)
B --> C(Process)
C --> D(Explore / Visualize)
D --> E(Analyze / Model)
E --> F(Archive, Publish, Share)
E --> C
E --> A
```

The data life cycle is often iterative and nonlinear, and does not always follow the order shown. Your actual analysis workflow may include dead ends or repeated steps. Regardless, it is helpful to plan and discuss your data-oriented research using these common components of the data life cycle:

1. **Plan**: Identify data that will be collected and how it will be managed.
Create a data management plan.
2. **Collect**: Acquire and store raw data.
a. **Acquire**: Retrieve data from the appropriate source.
b. **Describe**: Document the raw data source, format, variables, measurement units, coded values, and known problems. 
Create metadata for primary data. Cite secondary data. 
c. **Quality assurance**: Inspect the raw data for quality and fit for analysis purpose.
d. **Store**: Store the raw data in the appropriate folder, as determined in the planning stage. 
Consider access, resilience (backing up), security, and, if relevant, data agreement stipulations. 
Make raw data files read-only so they cannot be accidentally modified.
3. **Process**: Prepare the data for exploration and analysis. 
a. **Clean**: Preprocess the data to correct errors, standardize missing values, standardize formats, etc.
b. **Transform**: Convert data into appropriate format and spatiotemporal scale (e.g., convert daily values to annual statistics).
c. **Integrate**: Combine datasets.
4. **Explore**: Describe, summarize, and visualize statistics and relationships.
5. **Analyze / Model**: Develop, refine, and implement analysis and model specifications.
6. **Archive, publish, and share**: Save and publish final data products and documentation.

## Documentation

“*__If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point__*” (@Stoudt).

Quality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts. 

Without documentation, projects lose their usefulness over time, as illustrated below [see also @Michener].

![Image from [Michener et al. 1997](https://doi.org/10.1890/1051-0761(1997)007%5b0330:NMFTES%5d2.0.CO;2)](../../images/information_loss.png)