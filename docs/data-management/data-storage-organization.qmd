---
title: Data Storage and Organization
---

Standardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs.
A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions.
Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.

This section provides storage options and suggested practices for organization and preservation. 

# Storage Options at RFF

## Summary

* Always create an L: drive folder for your project. Store all data and code here.
* If working with external collaborators:
  * Use OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.
  * Use GitHub to share code. More about setting up version control is in the next section.
  * For small datasets, GitHub may be used for both data and code storage.
* For both L: drive and OneDrive project folder setup, contact [IThelp@rff.org](mailto:ithelp@rff.org) with specifications.

## L: Drive

The primary place to store project data and code is the L: drive.
All projects should have an L: drive folder, even when working with external collaborators.
It is optimized for data-intensive work, has large storage capacity, is regularly backed up, and is more secure than personal drives.
An RFF account is required to access the network, which can be accessed from work computers or remotely.
Git repositories can be setup within L drive project folders, as described in the Organization section below.

### L: Drive Storage Request
For both L: drive and OneDrive storage, coordinate with IT to ensure appropriate resources are available when you need them.
At the start of a project (or upon major changes to specifications, such as timeline or disk space), email [IThelp@rff.org](mailto:ithelp@rff.org) with the following information. Example answers are provided.

| Field | Response |
| --- | --- |
| Project name | SLR-Septic |
| Storage location | L drive |
| Folder name | L:/Project-SLR-Septic |
| Short description | This project analyzes problems with septic systems in areas subject to sea level rise and potential solutions |
| Principal Investigator(s) | Margaret Walls and Penny Liao |
| RFF collaborators  | Sophie Pesek |
| External collaborators | Several University of Maryland collaborators but they will not need access to the project folder |
| Data types | R, Stata, GIS datasets |
| Size requested | 80 GB |
| Estimated max. size | 150GB |
| Archival date | December 2026 |
| Data agreement or sensitive data security considerations | Proprietary data will be in raw data folder and will need to be made read-only |

## Working with External Collaborators

Because an RFF account is required for access to the L: drive, it may be necessary to incorporate additional storage options into the project.
In that case, there are a few options.

### OneDrive and GitHub
The recommended method for project data and code organization when working with external collaborators is to use a combination of OneDrive and GitHub.
If access to data is the main consideration, OneDrive may be ideal. If external collaborators will be contributing to the codebase, using GitHub for code and sufficiently small data (well below 100 MB) can work well.

#### For Data

Folders created on RFF's OneDrive account have an initial capacity of 5 TB and can be shared with non-RFF staff.
This folder can be accessed from any computer logged into a OneDrive account.
However, OneDrive is not accessible from RFF lab computers.
To accommodate this, use the L: drive project folder for all data storage, and copy only necessary data files to the OneDrive folder (for example, do not transfer over intermediate data files, only inputs and outputs).
Because lab computers don't have access to OneDrive, it might be necessary to copy files using the web browser.
If necessary, OneDrive storage capacity can be increased on a project basis (but not the OneDrive accounts of external collaborators).

External collaborators can choose to work directly from the OneDrive folder or use it for file transfer to their own preferred storage.
Directory structures should be mirrored across the L: and OneDrive folders to maintain consistency and code functionality.
For example, if a file is saved as `raw_data/fires/data.shp` in the L: drive, it should be shared with external collaborators in a OneDrive folder under the same directory structure.

One consideration when using this strategy is that if different copies of data are edited on both the L: drive and OneDrive, it can sometimes be difficult to determine who edited them and reconcile different versions.
One way to avoid this problem is to store data on GitHub and use version control when possible. 
Git version control is most applicable for data that is small (well below 100 MB) and not restricted by licenses or other sensitivities.

::: {.callout-note}
When a function to load data from a OneDrive folder is included in code, if the data is not already synchronized, it can sometimes automatically trigger the OneDrive program to download/sync the data. That may "freeze" the console where the code is running, without an informative message. This can be avoided by manually syncing the OneDrive folder before running code that depends on the data.
:::

#### Collaborative Coding

Use GitHub as the primary storage solution.
GitHub repositories can be "cloned" to a local computer.
This method leverages GitHub's distributed version control features to streamline collaboration, organization, and resilience by allowing users to "clone" GitHub repositories to a local computer (such as an RFF server folder or personal OneDrive folder), make changes independently, and then upload those changes to a shared workspace that can be accessed through any web browser or any computer with git installed
[LINK TO VC SECTION]

As with data organization, use the same folder structure for code across shared folders
In addition, it is particularly important to allow for easily configurable paths.
One way to do this would be to use relative paths [LINK TO SOFTWARE QUALITY SECTION ON RELATIVE FILE PATHS] so that other code users don't need to modify the absolute paths to be custom to their computer setup, and code can be run "out-of-the-box".

Considerations:

* __Storage limitations__: OneDrive is not suitable to support large data. There are synchronizations issues when working with single files bigger than 10GB.

  ::: {.callout-note}
  If external collaborators are "guests" in the OneDrive folder, they are also limited to your account's storage limits.
  :::

* __Security__: Consider the security implications of storing sensitive data on the cloud and specify access accordingly.

### Other Options for Collaborating with External Partners

#### Other Cloud Storage Options for large data

Alternatives to OneDrive, such as Azure, Google Bucket, or AWS S3, may be well suited to your project. Especially for short-term storage and file transfer.
Dropbox is also an option, but note that it can incur additional costs.
For Azure setups, contact IT at [IThelp@rff.org](mailto:ithelp@rff.org). 

#### ArcGIS Online for Sharing and Exploring Spatial Data

ArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular).
In order to access the data and exploratory mapping interface, users need an ArcGIS Online account.
Online accounts cost $100 per user and data storage costs vary by data size.
Contact RFF's GIS Coordinator at [Thompson@rff.org](mailto:thompson@rff.org) for more information.

#### Enabling External Collaborator Access to the L: drive

While not recommended, it is possible to enable access to the L: drive for non-RFF staff.
Temporary accounts can be requested by contacting IT at [IThelp@rff.org](mailto:ithelp@rff.org).

#### Accessing raw data via Application Program Interfaces (APIs)

For large datasets, it is best to include lines in your code for downloading raw data from the web via an API, where available, rather than downloading data to disk and importing it.
If an API is not available, having code download data from remote servers via other methods (e.g., curl) is still preferable.

Using APIs is ideal for a variety of reasons:

* __Storage size__: An API allows the user/program to access specific data of relevance without needing to find room to store an entire, broader dataset (which is instead hosted on the data provider's server)
* __Reproducibility__: Using public APIs or other ways of accessing public data on the internet allows for code produced using other best practices to run "out of the box" on any computer/VM with internet access
* __Updates__: Code written using API queries can be programmed to retrieve new data periodically without needing to manually update a local dataset

APIs do require some special considerations:

* __Consistency__: Remote data may change. It is important to use version control and smaller intermediate datasets within a version control framework to ensure that past results can be reproduced consistently
* __Longevity__: APIs may stop being accessible/maintained. It is not always a safe assumption that data stored remotely will continue to be accessible via an API. However, public government datasets tend to be more reliable due to laws enforcing their accessibility. Even so, API packages and query formats may change, so project code may need to be debugged occasionally to maintain compatibility with an API.
* __Accessibility__: Not all online data sources have convenient API functions in a programmer's language of choice. Some have URL formats that allow data to be accessed regardless. The amount of documentation and the level of technical skill required to understand it can vary.


# Data Organization

## Directory Structure

Regardless of the specific method deployed, your data project organization should have the following qualities:

* __Raw data are kept in a distinct folder and never modified or overwritten.__ Always keep an unaltered version of original data files, "warts and all." Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets "read-only" so they cannot be accidentally modified.
* __Simple__: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.
* __Flexible__: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.

These qualities also facilitate version control practices. There is additional guidance on organization for version control here 

<!-- [TODO: Insert LINK TO RELEVANT SUBSECTION]. -->
Below is an example of a directory structure that would be compatible with version control implementation on RFF's L: drive. It has four main folders: data, results, code, and docs. This version illustrates a personal repository folder model of code version control, which operates best on the L: drive. Similar directories with slight changes to the code folder can be employed in other cases.

```
project_name/
├── data/
│   ├── raw/
│   ├── intermediate or int/
│   ├── clean/ (optional)
├── results/
├── docs/
├── repos/
│   ├── smith/
│   │   ├── scripts/
│   │   │   ├── processing/
│   │   │   ├── analysis/
│   │   ├── tools/
│   ├── pesek/
│   │   ├── scripts/
│   │   │   ├── processing/
│   │   │   ├── analysis/
│   │   ├── tools/
```

### `Data` folder

In the data folder, raw data is preserved in its own subfolder. The intermediate or int folder contains datasets created during data cleaning and processing. If practical, a clean folder can contain cleaned output datasets and associated READMEs [LINK TO SECTION], but note that it's often unclear when datasets are truly "clean" until late project stages.

### `Results` folder

The results folder contains analysis results, model outputs, and all figures.

### `Docs` folder

The documents folder should contain the data management plan, a link to the GitHub site, and any other version-controlled shared documents (such as LaTeX or Markdown).

### `Code` folder

For integration with version control, the code folder has one copy of the project git repository for each of the project collaborators, so that each team member can make changes without affecting the working version of the rest of the team.  Repositories are synced manually, so that changes can be made independently and then merged to the shared, remote version of the codebase stored on Github.
Each individual's repository folder has subfolders separating types of code. The scripts folder code is the main project workflow. 
The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be "called" (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software [LINK TO SQ SECTION ABOUT MODULAR PROGRAMMING].

### Other project files

Note that this template does not include specific folders for notes, literature reviews, presentations, products, project management, etc., because those types of files are not the focus of this guidance.
Folders for these documents should exist either with the data folders or in another project folder, such as the shared Teams folder.
If other departments or external collaborators should have access to a folder or file, we recommend storing them in Teams. See the RFF Communication Norms guidance for more information [LINK].

## Other Organization Practices

### Subfolders

Organizing files into subfolders can help manage complexity and improve workflow. 
Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. 
By creating logical groupings you can keep related files together and streamline collaboration. 
Examples of logical groupings for subfolder names are by 

* data source (e.g., `usda`), 
* variable (`precipitation`), 
* processing step (`merge`), or
* results category (`figures` or `model_results`).

However, it's important to strike a balance.
Too many subfolders can complicate navigation and make the project harder to understand.
Aim to create subfolders only when they help categorize files meaningfully—like separating raw data from processed data or utility functions from analysis scripts—without over-complicating the structure.

### Miscellaneous Practices

* When creating folders, follow the naming conventions outlined in the following section.
* Avoid ambiguous/overlapping categories and generic catch-all folders (e.g. "temp" or "new").
* Avoid creating or storing copies of the same file in different folders

## Naming Folders, Files and Scripts

Folder (including subfolder), file, and script names should be consistent and descriptive. 
Specifically, they should be human readable, machine readable, and compatible with default ordering. 

* __Human readable__: Create brief but meaningful names that can be interpreted by colleagues.
  * Make names descriptive: they should convey information about file/folder content.
    * For example, if you're generating output visualizations of the same metric, instead of `county_means_a` and `county_means_b`, use `county_means_map` and `county_means_boxplot`.
  * Avoid storing separate versions of files (e.g. `county_means_map_v2`), and instead rely on version control tools to save and document changes.
  * If you use abbreviations or acronyms, make sure they are defined in documentation such as a README [LINK TO SECTION]
* __Machine readable__: Files and folders are easy to search for and filter based on name.
  * Use lowercase characters (avoid "camelCase" method)
  * Contain only ASCII characters (letters, numbers, and underscores)
    * Do not include spaces or special characters (/ \ : * ? <> &)
    * Use hyphens or underscores instead of spaces (the "snake_case" method)
* Compatible with default ordering: the ability to sort files by name is useful and helps organization, as shown below.
  * __Chronological order__: If there are temporal measurements, use the ISO 8601 standard for dates `(YYYY-MM-DD)`.
  Here is an example of temporal data files named for compatibility with default ordering:
    * `2021_01_01_precipitation_mm.csv`
    * `2021_01_02_precipitation_mm.csv`
    * `2021_01_01_temperature_statistics_f.csv`
    * `2021_01_02_temperature_statistics_f.csv`
    
    ::: {.callout-note}
    It is not recommended to use dates in script file names, except in cases of individual temporal measurements. Instead, leverage version control to save different script versions.
    :::

  * __Logical order__: one way to organize scripts or folders is by using numbered prefixes signaling the processing order. 
  Example of source code files named for compatibility with default ordering:
    * `01_clean_raw_data.R`
    * `02_merge_clean_data.R`
    * `03_descriptive_statistics.R`
    * `04_regressions.R`

    ::: {.callout-note}
    Make sure to "left pad" numbers with zeros. For example, use `01` instead of `1`.  This is to allow default sorting to still apply if and when the filename prefixes enter the double digits.
    :::


# File Formats

Guidance coming soon!!!

# Sensitive and Proprietary Data, Data Agreements

When using sensitive or proprietary data in your research project, it's crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.

## Identify and categorize sensitive data.

Determine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary. 

* To determine whether any of your project data is sensitive, consider the following:
  * Was the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?
  * Does the data include:
    * Identifying information about individuals (e.g., names, addresses, personal records)?
    * Sensitive environmental information (e.g., locations of endangered species, private property soil samples)?
    * Sensitive infrastructure information (e.g., detailed electricity grid data)?
    * Sensitive economic information (e.g. trade data)
    * Information concerning sovereign tribal governments or vulnerable communities?
  * Did accessing the data require Institutional Review Board (IRB) approval or human subjects research training?
  * Did accessing the data require special security training?
  * Was the data collected via surveys, interviews, or focus groups?
* If the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:
  * __Proprietary Data__ has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.
  * __Regulated Data__ is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.
  * __Confidential Data__ is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.

## Document data sensitivity and restrictions.

* Document data sensitivity class and details in both the data management plan and a README file in the data folder. Include details about the data's source, use restrictions, and sensitivity.
* Keep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.
* Check with data providers or experts for recommended security measures

## Determine appropriate security and privacy measures.

* Contact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.
* Ensure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators. 
* Do not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.

## Data Derivatives: Masking and Aggregation

* Data derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.
* Example techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.
  * __Statistical Disclosure Control__: Ensure that summary statistics or figures can't be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.
  * __Generalization and Anonymization__: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.
  * __Storage Considerations__: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets. 

## Review before publication or sharing.

Revisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.

