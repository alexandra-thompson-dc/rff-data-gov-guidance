---
title: Storage Options
---

## Summary

* Store data and code for RFF projects in a project-specific L:/ drive folder, and use GitHub to share and version control code.
* If working with external collaborators:
  * Use OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.
  * Use GitHub to share and review code.
  * For small datasets, GitHub may be used for both data and code storage.
* For both L:/ drive and OneDrive project folder setup, contact [IThelp@rff.org](mailto:ithelp@rff.org) with specifications.

## L:/ drive

The primary place to store project data and code is the L:/ drive.
All projects should have an L:/ drive folder, even when working with external collaborators.
It is optimized for data-intensive work, has large storage capacity, is regularly backed up, and is more secure than personal drives.
An RFF account is required to access the network, which can be accessed from work computers or remotely.
Git repositories can be setup within L drive project folders, as described in the Organization section below.

### L:/ drive storage request
For both L:/ drive and OneDrive storage, coordinate with IT to ensure appropriate resources are available when you need them.
At the start of a project (or upon major changes to specifications, such as timeline or disk space), email [IThelp@rff.org](mailto:ithelp@rff.org) with the following information. Example answers are provided.

| Field | Response |
| --- | --- |
| Project name | SLR-Septic |
| Storage location | L drive |
| Folder name | L:/Project-SLR-Septic |
| Short description | This project analyzes problems with septic systems in areas subject to sea level rise and potential solutions |
| Principal Investigator(s) | Margaret Walls and Penny Liao |
| RFF collaborators  | Sophie Pesek |
| External collaborators | Several University of Maryland collaborators but they will not need access to the project folder |
| Data types | R, Stata, GIS datasets |
| Size requested | 80 GB |
| Estimated max. size | 150GB |
| Archival date | December 2026 |
| Data agreement or sensitive data security considerations | Proprietary data will be in raw data folder and will need to be made read-only |

## Working with external collaborators

An RFF account is required for access to the L:/ drive. Therefore, when working with external collaborators, it may be necessary to store and share data in other ways (in addition to storing data on the L:/ drive).
In that case, there are a few options.

### GitHub and OneDrive
For projects with external collaborators, we recommend using a combination of GitHub and OneDrive. Github can be used for sharing code and sufficiently small data (well below 100 MB), and OneDrive can be used from sharing raw and processed data files between RFF analysts working on RFF servers and external collaborators working elsewhere.

#### Sharing code

GitHub's distributed version control system enables users to collaboratively develop, organize, and share code across multiple computers. Unlike personal cloud storage services such as OneDrive, sharing code via GitHub allows users to maintain independent local versions of a project's codebase, using version control to track edits, reconcile differences, and seamlessly synchronize changes across devices. On the other hand, a disadvantage of using GitHub for sharing is that it does not support sharing files larger than 100MB; files above the size limit need to be shared in other ways.

#### Sharing data

Folders created on RFF's OneDrive account have an initial capacity of 5 TB and can be shared with non-RFF staff.
However, OneDrive is not accessible from RFF lab computers.
To accommodate this, use the L:/ drive project folder for all data storage, and copy only necessary data files to the OneDrive folder (for example, do not transfer over intermediate data files, only inputs and outputs).
Because lab computers don't have access to OneDrive, it might be necessary to copy files using the web browser.
If necessary, OneDrive storage capacity can be increased on a project basis (but not the OneDrive accounts of external collaborators).

External collaborators can choose to work directly from the OneDrive folder or use it for file transfer to their own preferred storage.
Directory structures should be mirrored across the L:/ and OneDrive folders to instruct other users to where files should be placed if they are copying the file to their own directory.
For example, if a file is saved as `raw_data/fires/data.shp` in the L:/ drive, it should be shared with external collaborators in a OneDrive folder under the same directory structure.

One consideration when using this strategy is that if different copies of data are edited on both the L:/ drive and OneDrive, it can sometimes be difficult to determine who edited them and reconcile different versions.
One way to avoid this problem is to store data on GitHub and use version control when possible. 
Git version control is most applicable for data that is small (well below 100 MB) and not restricted by licenses or other sensitivities.

::: {.callout-note}
When a function to load data from a OneDrive folder is included in code, if the data is not already synchronized, it can sometimes automatically trigger the OneDrive program to download/sync the data. That may "freeze" the console where the code is running, without an informative message. This can be avoided by manually syncing the OneDrive folder before running code that depends on the data.
:::

Considerations:

* __Storage limitations__: OneDrive is not suitable to support large data. There are synchronizations issues when working with single files bigger than 10GB.

  ::: {.callout-note}
  If external collaborators are "guests" in the OneDrive folder, they are also limited to your account's storage limits.
  :::

* __Security__: Consider the security implications of storing sensitive data on the cloud and specify access accordingly.

### Other options for collaborating with external partners

#### Accessing raw data via Application Program Interfaces (APIs)

APIs enable users or programs to download relevant subsets of data directly into memory, eliminating the need to download and store entire datasets on disk. This approach not only reduces storage requirements but also improves efficiency by allowing applications to process relevant data immediately without managing large raw files.

Writing code that uses an API is especially advantageous for working with large datasets, because it often allows for subsets of the data to be loaded (e.g., part of a state map covering one city), which can reduce the need for local disk space.
If an API is not available, having code download data from remote servers via other methods (e.g., `curl`) is still preferable.

Using APIs is ideal for a variety of reasons:

* __Storage size__: An API allows the user/program to access specific data of relevance without needing to find room to store an entire, broader dataset (which is instead hosted on the data provider's server)
* __Reproducibility__: Using public APIs or other ways of accessing public data on the internet allows for code produced using other best practices to run "out of the box" on any computer/VM with internet access
* __Updates__: Code that uses API queries retrieves the most current data available each time it is executed, removing the need to manually update a local dataset.

APIs do require some special considerations:

* __Consistency__: Remote data may change. It is important to use version control and smaller intermediate datasets within a version control framework to ensure that past results can be reproduced consistently
* __Longevity__: APIs may stop being accessible/maintained. It is not always a safe assumption that data stored remotely will continue to be accessible via an API. API packages and query formats may also change, so project code may need to be debugged occasionally to maintain compatibility with an API.
* __Accessibility__: Not all online data sources have convenient API functions in a programmer's language of choice. Some have URL formats that allow data to be accessed regardless. The amount of documentation and the level of technical skill required to understand it can vary.

##### Publishing/Archiving data sourced from APIs {#publish-archive-apis}

If you used an API to access source data, the best course for publishing or archiving source data will vary based on project needs, dataset size, and nature of the data. Some options are:

* __Download to folder__: During the archival phase, download the source data in its current state and save it to the project folder to be archived.
* __Document__: Document the dataset version and access date in lieu of providing source data. While not ideal for reproducibility, this is suitable in cases where the source data is large, reliable, and not likely to be modified. 
* __Download to cloud__: Use a repository service, such as Zenodo, to store source data as it existed when archiving the project. Source data accessed through an API can be [downloaded directly to a Zenodo repository](https://developers.zenodo.org/), without having to save files locally. This can be done through R ([zen4R](https://cran.r-project.org/web/packages/zen4R/zen4R.pdf)) or [python](https://pypi.org/project/zenodo-client/).

#### Other cloud storage options for large data

Alternatives to OneDrive, such as Azure, Google Bucket, AWS S3, or Dropbox may be well suited to your project, especially for short-term storage and file transfer. However, note that these storage options may incur additional costs, depending on data size (even the "free tiers" of these services may incur pay-as-you-go costs).
For Azure setups, contact IT at [IThelp@rff.org](mailto:ithelp@rff.org). 

#### ArcGIS Online for sharing and exploring spatial data

ArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular).
In order to access the data and exploratory mapping interface, users need an ArcGIS Online account.
Online accounts cost $100 per user and data storage costs vary by data size.
Contact RFF's GIS Coordinator at [Thompson@rff.org](mailto:thompson@rff.org) for more information.

#### Enabling external collaborator access to the L:/ drive

While not recommended, it is possible to enable access to the L:/ drive for non-RFF staff.
Temporary accounts can be requested by contacting IT at [IThelp@rff.org](mailto:ithelp@rff.org).